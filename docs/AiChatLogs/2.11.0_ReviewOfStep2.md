# Review implementation of step 2
_Exported on 17/09/2025 at 11:41:26 BST from Cursor (1.4.5)_

---

**User**

You are a research engineer, working as part of a team to implement a research proposal. Please take the time to read the ResearchPlan.md. Your colleagues have already implemented steps 1-2 of the plan. Step 1 has been well-checked and are deemed completed, so you do not need to worry about this. You can read the ProgressNotes left by your colleagues, and read the code they have written, in order to understand what you are working with.

Your task is to review the implementation of step 2, comparing it to the specification and goals laid out in the ResearchPlan.md. It is unlikely that the plan has been implemented correctly, since this attempt was just a first pass, and you must provide feedback on what need to be changed in order to satisfy the specificaiton laid out in the plan. You should also feel free to think about the plan itself, and let me know if you think there are any issues with step 2 of the plan.

You are not to change any code, but instead to review the work and give a list of feedback, ordered roughly from highest importance to lowest. Be as critical as you can - it's really important that we get this step up to standard to set us up in a way which makes the rest of the research plan implementation as easy and smooth as possible. 

---

**Cursor**

I'll start by locating and reading the research plan and the key files changed for step 2 so I can compare the implementation to the spec. Then I'll skim relevant progress notes for context.

I'll read the research plan and the step 2 progress notes, plus the curriculum config and training/evaluation scripts to compare the implementation against the spec.

I'll skim the model definitions and the training/evaluation scripts to compare architecture and training flow against Step 2 requirements before drafting the feedback.

I'll skim the step 2 progress notes to see intended design decisions and compare them to the plan before writing feedback.

- I read the research plan (esp. Step 2) and the code/notes for the current Step 2 implementation. Now I’ll give a prioritized review of gaps and concrete fixes.

### High-priority issues

- Recurrent DRC training is not wired correctly to SB3’s rollout/optimization
  - What I see:
    - `RecurrentActorCriticPolicy` extends `BasePolicy` and manages hidden state internally, but SB3’s vanilla `PPO` does not handle recurrent sequence masks or pass `episode_starts` into the policy. There is no custom rollout buffer nor any callback that calls `set_episode_starts`. The comment “remove ad‑hoc recurrent callback” implies this was deliberately left out.
    - `_init_hidden_states` returns `None`, and the extractor is given `hidden_states`, but without proper reset/masking across episodes/minibatches, hidden state will leak between trajectories and batches.
  - Why it matters:
    - This makes training “recurrent” models effectively feed-forward or, worse, incorrect due to state leakage—contradicting Step 2’s aim to train a ConvLSTM/DRC agent properly.
  - What to change:
    - Either:
      - Use `sb3-contrib RecurrentPPO` end-to-end with a policy that conforms to its API (i.e., implement a `CnnLstmPolicy`-style custom policy whose feature extractor is DRC) so sequence handling and masks are correct; or
      - Implement a custom recurrent rollout buffer, teach the collector to track `episode_starts`, pass sequence lengths, and ensure masking/reset in both rollout and minibatch slicing. That’s substantial work—reusing `RecurrentPPO` is likely faster and safer.

- Online curriculum uses solver gating at reset, likely violating Step 2’s “no solver in sampling” guidance
  - What I see:
    - `OptimalLengthFilteredEnv.reset` loops generating puzzles and checks `info['optimal_length']` up to 100 attempts, only if `ensure_solvable` is True.
  - Why it matters:
    - Running the solver at episode generation during training is expensive and can throttle throughput, contrary to Step 2 recommendations which push solver use to offline/bank curation or evaluation.
  - What to change:
    - Prefer bank-based curriculum for Step 2 training runs. If an “online” curriculum must exist, gate difficulty using cheap proxies (board size, wall counts, robots) without invoking optimal-length checks, or cache pre-solved seeds offline and only sample from them online.

- DRC policy/extractor integration deviates from SB3’s expected signatures
  - What I see:
    - `ConvLSTMFeaturesExtractor.forward` returns `(features, new_hidden_states)` and accepts `hidden_states`, which is non-standard for `BaseFeaturesExtractor`. It only works because the custom policy calls the extractor directly. But SB3 expects `forward(observations) -> features` and uses it in other code paths (e.g., value prediction), increasing maintenance risk and corner-case breakage.
  - Why it matters:
    - This tight coupling to a bespoke policy is brittle and doesn’t integrate with SB3 tooling (e.g., evaluation callbacks, policies that expect standard extractors).
  - What to change:
    - If you move to `sb3-contrib RecurrentPPO`, port DRC logic into a policy class that matches its recurrent policy API (with `latent` recurrence handled in the policy, not via extractor). Alternatively, keep extractor pure (features only), and manage hidden state exclusively in the policy.

- Hidden-state resets and batching safety are incomplete
  - What I see:
    - `set_episode_starts` exists but is never called by training. `_apply_pending_episode_starts` zeroes `h/c` with a mask but falls back to dropping all states if mismatched—this will silently degrade to non-recurrent behavior in common vec-env + minibatch scenarios.
  - Why it matters:
    - Even if recurrence “sort of works,” masks must be correct per env index; otherwise training signals are noisy or wrong.
  - What to change:
    - Ensure episode start masks are plumbed from the rollout collector to the policy; or switch to `RecurrentPPO`, which already handles this.

### Medium-priority issues

- Curriculum mechanism split (online vs bank) lacks a clean, unified trainer flow
  - What I see:
    - There are two parallel systems: online `CurriculumWrapper` with solver-based “max_optimal_length” gating, and a bank-based `BankCurriculumWrapper`. The train script chooses either via flags, and eval logic has divergence between them.
  - Why it matters:
    - Step 2 asks for a coherent training framework with curriculum. The bank-based path aligns with speed/scale guidance; the online path harms throughput and adds complexity.
  - What to change:
    - Make bank curriculum the default path for Step 2; keep online curriculum behind an “experimental” flag with warnings. Consolidate eval so both paths share the same success-rate/level logging and frozen-level eval.

- Observation shape stability under bank curriculum is risky
  - What I see:
    - Bank levels’ `SpecKey` may vary grid sizes. Vectorized envs require fixed observation shapes. The script prints a note recommending `--obs-mode rgb_image` but does not enforce fixed-size rendering or resizing.
  - Why it matters:
    - If levels change H×W, the vec-env will break at level transitions or sampler boundaries unless `rgb_image` outputs a consistent image size (not guaranteed from the env code shown).
  - What to change:
    - Enforce `obs_mode=rgb_image` and implement consistent output size (fixed H×W via renderer) across all levels in bank curriculum; or constrain bank levels to same board size for training. Validate with a test that observation shapes never change along a run.

- Missing ResNet baseline called out in Step 2
  - What I see:
    - Baselines are MLP and SmallCNN. ResNet baseline (as in the referenced paper) is absent.
  - Why it matters:
    - Step 2 suggests multiple architectures for comparison. ResNet is a useful non-recurrent baseline for interpretability comparisons and sanity checks.
  - What to change:
    - Add a light ResNet feature extractor option and CLI flag (e.g., `--resnet-small`), with parity in logging and evaluation.

- No-op “thinking” steps exist but are under-specified and under-tested
  - What I see:
    - `include_noop` and `noop_penalty` exist; milestone env factories don’t pass `noop_penalty`, and there’s no curriculum phase toggling no-op availability as suggested in the plan.
  - Why it matters:
    - Step 2 mentions experimenting with explicit planning via no-op steps. If this is a research target, we need a measured setup and logging (usage rates, effect on success/length).
  - What to change:
    - Add tests and logging for no-op usage. Optionally stage a curriculum phase that first disallows no-op then enables it, measuring changes in episode length and success.

- Default curriculum levels have a slight non-monotonicity in max optimal length
  - What I see:
    - Level 2 `max_optimal_length=6`, Level 3 is 5, then Level 4 is 10. Your validator permits a 0.8x dip, but semantically Level 3 being “harder” with a lower max optimal length is odd.
  - Why it matters:
    - Off-by-one/semantics can cause counterintuitive gating and agent plateau.
  - What to change:
    - Revisit level specs so difficulty signals (robots, walls, max optimal length) align monotonically.

### Lower-priority issues and polish

- Policy normalization toggles are inconsistent across modes
  - What I see:
    - For `CnnPolicy`, `normalize_images` is set True only when `obs_mode=="rgb_image"`. For other modes, dtype/scale assumptions differ between custom policy and SB3 built-ins.
  - What to change:
    - Standardize: ensure uint8 inputs are normalized to [0,1] in all image modes, or explicitly convert env observations to float32 and document it.

- Documentation mismatch with implementation
  - What I see:
    - README advertises ConvLSTM as “DRC-lite” and recurrent planning, but in practice the only safe recurrent path is `--convlstm` using `sb3-contrib CnnLstmPolicy`. The `--drc` path likely trains incorrectly as explained above.
  - What to change:
    - Clarify docs: mark `--drc` experimental until it’s integrated with `RecurrentPPO`. Provide recommended commands for recurrent training that are known-good.

- Evaluation vs optimality integration is fragmented
  - What I see:
    - Monitoring hub hooks `optimality_gap` when available, but only bank env attaches it reliably. Step 2’s evaluation section expects solver-computed optimality on a fixed set.
  - What to change:
    - Make the fixed-set + solver optimality evaluation path first-class and documented, with a simple CLI example. Ensure it runs regardless of bank/online curriculum choice.

- Test coverage gaps for recurrent path and curriculum transitions
  - What to change:
    - Add tests that: (a) recurrent policy resets hidden state on episode boundaries across VecEnvs; (b) observation shapes remain constant across a bank curriculum level advance; (c) curriculum advancement occurs when success-rate threshold is met in a controlled mock env.

### Potential concerns with Step 2 plan itself

- Insisting on online, optimal-length-filtered curriculum during training is impractical
  - The plan suggests using solver-gated optimal-length during generation; for single-GPU training this is a major bottleneck. The implemented bank curriculum is the right direction. I’d recommend updating Step 2 to codify “offline curation (bank) only during training; solver reserved for evaluation and bank construction.”

- Recurrent training should standardize on `RecurrentPPO` or equivalent library support
  - Rolling your own recurrent PPO collector/minibatching is non-trivial and error-prone. The plan should explicitly prefer `sb3-contrib RecurrentPPO` for Step 2 to ensure correct sequence handling, and scope any custom recurrent training loop as future work.

- Observation consistency across curricula needs to be explicit
  - The plan should require a fixed observation tensor shape (e.g., RGB 64×64) irrespective of board size to avoid vec-env incompatibilities.

### Concrete next steps (implementation guidance)

- Replace `--drc` training path with a `RecurrentPPO`-compatible custom policy that embeds the DRC core, or temporarily disable `--drc` and recommend `--convlstm` until the policy is correctly integrated.
- Make bank curriculum the default when `--curriculum` is on; emit a warning when `--curriculum` is used without `--use_bank_curriculum`.
- Enforce fixed-size `rgb_image` observations (resize if needed) for bank curriculum; add a guard that fails fast if shapes vary.
- Add a small ResNet extractor and CLI flag.
- Add a fixed-set solver-backed evaluation command example and ensure it logs optimality metrics.
- Expand tests to cover recurrent resets, curriculum advancement, and obs-shape invariance.

- I’ve finished the review. Next, if you want, I can draft a minimal plan to refactor the recurrent training path onto `RecurrentPPO` while keeping your DRC core, and outline the tests needed to validate it.

- Key gaps to fix first:
  - Recurrent training correctness (switch to `RecurrentPPO` integration for DRC).
  - Remove solver from training-time sampling; default to bank curriculum.
  - Enforce stable observation shape under curriculum.

---

**User**

Thanks for the great work. Please can you make the edits to update the step 2 research plan according to your 'potential concerns' - your suggestions all seem great.

