# Review feedback for research proposal step 1.1
_Exported on 11/09/2025 at 18:25:59 BST from Cursor (1.4.5)_

---

**User**

You are a language model, working as part of a team to implement a research proposal. Please take the time to read the ResearchPlan.md.

You have been tasked with reviewing the work of a colleague who has been tasked with implementing 'step 1.1' of the plan detailed in ResearchPlan.md. You are not to change any code, but instead to review the work and give a list of feedback, ordered roughly from highest importance to lowest. Please go ahead and get started. Be as critical as you like.

---

**Cursor**

I'll scan the research plan and the relevant implementation files to assess how well step 1.1 was met and prepare prioritized feedback.

I reviewed the plan’s Step 1.1 requirements against the current environment and core implementation. Here’s prioritized feedback.

- Bold items are the highest-impact gaps or risks first.

- References use file/function names like `env/ricochet_env.py::RicochetRobotsEnv.step`.

- I’m not changing any code; this is review-only.

- If you want me to propose edits later, I can.

- **Target robot not exposed in observation**
  - The observation doesn’t reveal which robot is the target. The agent sees per-robot one-hot channels but nothing distinguishes the target robot from others. This prevents optimal behavior learning for the actual objective.
  - Where: `ricochet_env._board_to_obs` builds channels: walls (0), goal (1), then one channel per robot; no target indicator.
  - Fix suggestion: add a “target robot mask” channel or encode target ID in the observation.

- **Wall representation diverges from Ricochet Robots rules**
  - Official walls are on cell edges; current implementation uses walls as blocked cells. This changes reachability and legal stopping mechanics (e.g., inner-corner behaviors and classic “bumpers” differ).
  - Where: `ricochet_core.Board.walls` is a cell-occupancy grid; movement blocks if next cell is occupied by a wall.
  - Risk: Learned policies may not transfer to canonical puzzles or literature comparisons.

- **No formal guarantee of solvable reset layout when `ensure_solvable=True` fails or times out**
  - The code falls back to unsolved boards after 20 attempts or if solver import fails, silently breaking the Step 1.1 “ensure solvable” guidance.
  - Where: `ricochet_env.reset` loop sets `_board` and breaks even if no solution found after attempts or on ImportError.
  - Suggest at least flag in `info`, or raise/soft-fail to avoid training on unsolvable boards when the flag is set.

- **No unit tests for multi-robot interactions (blocking and stopping)**
  - Current tests check sliding and a specific goal path, but don’t cover stopping next to another robot, robots as “bumpers,” or collisions across multiple robots.
  - Where: `tests/test_env.py` and `tests/test_solver.py`.
  - Recommend tests: “moving robot A stops at robot B,” “robot B movement unaffected by A after move,” “chain pushes are impossible (correct).”

- **Action decoding behavior for no-op is leaky and surprising**
  - `_decode_action` maps out-of-range to `(robot_id=0, direction=UP)` which could mask bugs. No-op is handled elsewhere; decoding should reject invalid actions.
  - Where: `ricochet_env._decode_action`.

- **Observation layout may be incompatible with common CNN policy defaults**
  - Channel-last `(H, W, C)` is fine, but many RL libs expect channel-first `(C, H, W)`. This can cause friction or silent transposes later.
  - Where: `ricochet_env.__init__` sets `spaces.Box` with `(H, W, C)`.

- **Reward shaping is okay but lacks explicit no-op cost differentiation**
  - Step penalty applies equally to no-op and movement, which is defensible, but Step 1.1 suggests possibly a distinct smaller penalty for no-op “thinking” to study pacing behavior.
  - Where: `ricochet_env.step` adds `step_penalty` regardless.

- **Lack of symbolic observation mode mentioned in plan**
  - Step 1.1 proposes supporting an “image” vs “symbolic” observation mode for debugging; only image mode is implemented.
  - Where: `ricochet_env` constructors and `_board_to_obs`.

- **Random generator seeding semantics**
  - Passing `seed` to `reset` replaces the RNG rather than re-seeding deterministically from a base seed with episode counter. It’s fine, but can lead to surprises (e.g., same seed reused across resets yields identical board each time, which is sometimes desirable, sometimes not).
  - Where: `ricochet_env.reset` sets `self._rng = np.random.default_rng(seed)`.

- **`observation_space` and `action_space` when `gymnasium` missing**
  - Dummy `spaces.Box`/`spaces.Discrete` placeholders are present; but `Discrete.n` is set to 0 in the dummy and not updated from env, which could mislead external tooling relying on `n`.
  - Where: `ricochet_env` fallback `_SpacesDummy`.

- **Solver state serialization excludes non-robot aspects by assumption**
  - `serialize` tracks only robot positions, assuming walls/goal/target are constant. This is valid within a single search but brittle if ever used across boards. Add a clarifying comment or guard.
  - Where: `env/solver.py::serialize`.

- **No explicit tests for `ensure_solvable=True` path**
  - There’s no test that the env actually returns solvable instances when requested.
  - Where: `tests`.

- **`apply_action` updates position even if no movement**
  - The docstring implies “unchanged,” but it still writes the same coordinates. Harmless but inconsistent with the docstring’s spirit.
  - Where: `ricochet_core.apply_action`.

- **Basic ASCII rendering is sufficient; but no image render**
  - For debugging CNNs, a quick matplotlib renderer often helps visually validate inputs vs channels.
  - Where: `ricochet_env.render`.

- **FixedLayout tests are purpose-built; consider broader fixtures**
  - With multiple robots and more walls, fixtures can validate corner cases (edges, corners, tight corridors).
  - Where: `tests/test_env.py`.

- **`max_steps` truncation test missing**
  - No test to assert truncated=True when step limit exceeded.
  - Where: `tests/test_env.py`.

- **Info dict lacks diagnostics**
  - Consider `info={"success":..., "steps":..., "target_robot":...}` to support logging and curriculum later.
  - Where: `ricochet_env.step`.

- **Performance considerations**
  - Recomputing `occupied` per slide is okay for small boards; if scaling up, caching per step might be needed. Not urgent for Step 1.1.

- **Demo CLI good, but single-robot only**
  - The CLI only drives robot 0; a quick mapping to switch robots would help manual QA of multi-robot behavior.

In brief: the biggest blockers for Step 1.1 are the missing target-robot signal in observations and the wall model deviating from canonical rules. The rest are quality-of-life, robustness, and testing gaps that are straightforward to address.

I found and reviewed the environment, core movement logic, solver, tests, and the demo CLI. Now I’ll stop here unless you want proposed edits.

- Implemented rules and state are solid for a basic version: sliding, stopping, discrete action encoding, rewards and terminations align with Step 1.1.

- Two high-priority issues: observation lacks target-robot identity; wall model differs from the canonical Ricochet Robots which may affect downstream validity.

- Test coverage is good on basics but thin on multi-robot and solvability guarantees.

