# Review implementation and document concerns
_Exported on 18/09/2025 at 15:13:07 BST from Cursor (1.4.5)_

---

**User**

Please think about the current task. It has been implemented, but I don't trust it. Please go through the implementation, reading any code you think you need to, and then make edits to the WorkInProgress.md file to lay out concerns you have, any questions or uncertainties you have, make suggestions for important improvements or fixes to the implementation, and feel free to make any other notes you think are useful or important - make these edits under a 'Review #1' heading in the markdown file. Do not write any code yet.

---

**Cursor**

I'll first create a short set of todos to track the review, then scan the core modules (env, bank, curriculum, solver, training) and the tests in parallel to surface issues. After that, I'll add a "Review #1" section to `WorkInProgress.md` with concerns, questions, and suggested fixes.

I'll scan docs and WorkInProgress first, then in parallel review env/solver, bank/curriculum, training/models, and tests to gather concrete issues before drafting the review.

I finished scanning the docs and current WIP context. Next I’ll audit the env and solver implementations, bank/curriculum, precompute controller, and training entrypoints in parallel to surface concrete concerns before drafting the Review #1 notes.

I'll quickly scan the core environment and solver modules to ground the review, then update the todo statuses and add the "Review #1" section to WorkInProgress.md.

I'll quickly scan the test files to align expectations with implementation, then I'll update the todo list and add the "Review #1" section to `WorkInProgress.md`.

```markdown
# Work in progress
Contains a description, detailed specifications, discussion, notes, questions and answers etc. all related to the current task

## Current task
Redesign the curriculum environment generation and storage to improve generation speed, reduce wasted compute and future-proof for relevant later changes.

## Plan 
### Scope and intent (quick summary)
- **Goal**: Make the bank-based curriculum the primary path; keep online generation only for smoke tests. Achieve faster precompute, zero online solving during training, robust sampling, and schema/versioning for future changes.

### Current state (from code scan)
- Two curriculum paths exist:
  - **Online filtered env**: `OptimalLengthFilteredEnv` within `CurriculumWrapper` regenerates until constraints met (risk: slow, loops, online solver calls).
  - **Bank-based**: `PuzzleBank` + `BankSampler` + `CriteriaFilteredEnv` via `BankCurriculumManager` (preferred; cached DataFrames, Parquet partitions by `SpecKey`).
- Precompute scripts: `generate_curriculum_bank.py` uses `PuzzleGenerator` from `env/precompute_pipeline.py` and shared specs in `env/curriculum_config.py`.
- Gaps/risks noticed:
  - `layout_blob` deserialization is not implemented; recovery relies on regen-by-seed + hash match.
  - Potential generator drift vs stored `board_hash`/`layout_hash` without explicit `generator_rules_version` discipline.
  - Criteria naming: `criteria['max_depth']` is passed to solver as max search depth; ensure not confused with optimal_length fields.
  - Bank sampling cache growth and memory usage; no eviction policy.

### Proposed phases
1) Requirements & targets
   - Lock target curriculum levels and bounds (optimal length, robots moved) for v1.1.
   - Decide minimum viable bank size per level and overall generation time budget.
   - Make bank-based curriculum default in training scripts and README.

2) Generation speedups (precompute)
   - Add parallel/multiprocess batching to `PuzzleGenerator.generate_puzzles_for_spec` with process-safe appends to Parquet (chunk files then merge).
   - Early rejection: light-weight heuristic before full solve (e.g., bounding-box distance lower bound) to skip hopeless boards.
   - Deduplicate by `layout_hash` during generation to avoid repeated layouts with different seeds.
   - Track and store solver nodes expanded; expose to criteria later.

3) Storage/schema hardening
   - Manifest v1.1: add schema version, generator hash, solver config fingerprint, counts per difficulty band.
   - Enforce `pyarrow` Parquet path; add basic indices (via sorting + metadata) on `optimal_length`, `robots_moved`.
   - Optional: split large partitions into shards (e.g., `...parquet/part-*.parquet`) for append-friendly writes.

4) Sampling and curriculum
   - Use `BankSampler` everywhere in `CriteriaFilteredEnv`; add graceful fallback if a level is underfilled (nearest-band backoff, with logging).
   - Add iterator eviction policy (LRU by criteria key, cap memory).
   - Curriculum advancement remains success-rate based; ensure `BankCurriculumManager.get_stats()` is surfaced in logs.

5) Validation and tests
   - Determinism: regen-from-seed -> `board_hash` and `layout_hash` equality tests.
   - Query filters: unit tests for `PuzzleBank.query_puzzles` and `BankSampler.sample_puzzles` across bounds.
   - End-to-end: precompute a tiny bank; train for N episodes; ensure zero online solve calls; curriculum advances.

6) Migration & tooling
   - Write a migration script to re-tag existing partitions with v1.1 manifest fields; verify hashes; compute missing stats.
   - Backup current bank directories and provide verification report.

7) Monitoring & ergonomics
   - Add concise CLI progress and final summaries; optionally wandb/TensorBoard counters for sampled difficulty bands.
   - Docs: update `README.md` quickstart and `docs/ProgressNotes` once implemented.

### Open questions / decisions needed
- Target bank size per level and acceptable precompute duration on current machine?
  - Answer: target 1000 per level for now - but note that this means 1000 actually solving this criteria; currently we track how many we try and then only a fraciton of this satisfy criteria. Ideally precompute takes less than 10 mins per level, but this may not be achievable.
- Keep both BFS and A* variants in bank or standardize on one? If both, how to encode in `SolverKey` for sampling?
  - Keep option to switch if we want, but default to whichever is faster. They should be producing the same minimum  solves (note that the actual solve found might not be the same, since different solves of same length can exist).
- Minimum/maximum `robots_moved` constraints we actually want during early curriculum?
  - In early curriculum, we will want max 1 robot moved, switch to allowing 2+ after a couple of levels.
- Do we require `layout_blob` serialization now, or is seed+hash regeneration sufficient for the next milestones?
  - I'm unsure on this, but I'm expecting seed+hash is good enough. Feel free to investigate this yourself if you think it's likely to be important for what you choose to do next.
- Memory cap for sampler cache (MB) and eviction policy preferences?
  - We have 16GB ram on CPU to work with, but note that some of this will be taken up by other processes.
- Any near-term changes to generator rules that require bumping `generator_rules_version`?
  - No

### Notes and risks
- Generator drift can invalidate stored hashes; strict versioning and manifest fingerprints mitigate this.
- Parallel Parquet writes can corrupt files if not sharded; prefer shard-per-worker then consolidated manifest.
- Overly strict criteria can starve levels; implement backoff and report.
- Large pandas DataFrames in cache can cause RAM spikes; cap and monitor.

### Next actions (initial)
- Align on level spec targets and bank size per level.
- Prototype small parallel generation on 1–2 specs; measure puzzles/sec and success rate.
- Add starvation backoff in `CriteriaFilteredEnv`/`BankSampler` and log warnings when underfilled.
- Draft manifest v1.1 fields and plan migration script.

## User Questions
- I think at the moment we generate N puzzles for each level and try to solve them, and only store puzzles of interest. I think this leads to a lot of wasted compute. I am wondering if it's somehow better to do something like:
  1) Just solve a load of puzzles, storing the metadata about each one, inculding optimal solve length, number of robots moved in optimal solve, hashes, all the rest of the meta data... They could be stored in whatever format is most useful, but I am wondering if storing by (optimal solve length, num robots moved) pair as a key is useful
  2) Keep track of (or maybe it's easy enough just to count, periodically) how many puzzles exist for each (optimal solve length, num robots moved) pair
  3) Periodically check, for each level in our curricula, whether there are sufficient puzzles which satisfy the criteria, by summing over the permissable (optimal solve length, num robots moved) pairs.
  4) Stop once *either* each level has sufficient puzzles satisfying the criteria *or* a maximum puzzle generation count is met.
  5) Ideally have the option to continue adding to an (already generated) puzzle bank, by skipping seeds which have already been calculated for
Please think about this, and let me know if you think it's possible/works well, or if you have improvements to suggest or think that an alternative method (including just continuing with your suggested implementation) is better. Feel free to push back against me on this, I want to come up with the best strategy so I would appreciate your honest input on how best to move forward.

### Answer / proposed approach
- Short answer: Yes—this “band-first” strategy is good and should reduce waste. We should solve-and-record broadly, then allocate to curriculum levels from an indexed pool rather than pre-filtering per level.

- Concretely:
  - Store all solved puzzles with full metadata (already supported by `PuzzleMetadata`), plus a per-partition histogram keyed by `(optimal_length, robots_moved)` and optionally by coarse wall density bins. Persist histogram into the manifest for O(1) availability.
  - Maintain a `seen_seeds` (or `seen_layout_hashes`) set per `SpecKey` in the manifest to support incremental runs and dedup across sessions. Prefer dedup by `layout_hash` to avoid trivial target-robot variations.
  - Generation stopping rule: run in chunks, after each chunk recompute coverage per curriculum level by summing across admissible bands; stop when all levels ≥ target counts or a global max budget is hit.
  - Selection for training: implement a quota-based sampler per level that draws proportionally from its admissible bands to prevent mode collapse (stratified within level constraints).

- Improvements to make it robust:
  - Sharded Parquet writes (per worker) and post-run consolidation (or leave sharded) to avoid writer contention.
  - Lightweight pre-solve filters (e.g., Manhattan lower bound to goal) to skip obviously-too-long boards and lift effective hit-rate.
  - Track solver nodes expanded and time to allow future cost-aware generation (prioritize specs/bands with best hit-rate per second).
  - Starvation backoff: if a level is underfilled, temporarily widen its admissible bands within a guardrail and record that deviation.

- Why this beats level-by-level pre-filtering:
  - Converts many hard rejections into useful inventory for other levels; reuses compute broadly.
  - Enables global stopping criteria and incremental topping-up without re-solving duplicates.

### Implementation notes (minimal changes needed)
- Add manifest fields: per-partition band histogram, `seen_layout_hashes` bloom or rolling shard, generator/solver fingerprints.
- Extend `PuzzleGenerator` to periodically flush histograms and seen-sets; add an outer controller loop to evaluate stop criteria.
- Extend `BankSampler` to accept per-level band quotas and to consult histograms for availability.

### Trade-offs / risks
- Histograms and seen-sets can grow; use compact structures (counts, approximate bloom) and shard by partition.
- Stratification slightly complicates the sampler but yields smoother training.
- If generator rules change, histograms become incomparable—guard with version pins already proposed.

### Additional questions (for alignment)
- Are we okay with approximate dedup (bloom on `layout_hash`) to bound memory, or do we prefer exact sets with periodic compaction?
  - I think approximation is okay
- Should we include a coarse wall-complexity bin in histograms now, or defer until we see coverage gaps?
  - I think include it for future-proofing but for now we'll be using 2-per quadrant for both the edge-T and central-L squares by default.
- For starvation backoff, what max widening factor of `(optimal_length, robots_moved)` bands is acceptable before we’d rather keep generating?
  - Actually I think initially we'll set this to 'none' (ie, always prefer generating rather than widening)

## Progress
- 2025-09-18: Wrote initial plan, answered alignment questions, and designed band-first generation strategy. Preparing manifest v1.1 changes (histograms, dedup fields) and sampler adjustments.
- 2025-09-18: Implemented manifest v1.1 upgrade path and per-partition histograms `(optimal_length, robots_moved)` in `env/puzzle_bank.py`; backfilled defaults on load and incremented counts in `add_puzzles`. Lint check passed.
- 2025-09-18: Added controller outline `run_band_first_precompute_controller` and `compute_level_coverage_from_histograms` in `env/precompute_pipeline.py` to drive chunked generation using manifest histograms. Lint check passed.
- 2025-09-18: Exposed controller via CLI in `generate_curriculum_bank.py` with flags `--use_controller`, `--target_per_level`, `--max_puzzles_global`, and `--chunk_per_spec`. Lint check passed.
- 2025-09-18: Added stratified sampling API `sample_puzzles_stratified` with monitoring in `BankSampler` (env/puzzle_bank.py) to support per-band quotas and capture outcomes.
- 2025-09-18: Integrated stratified quotas into `CriteriaFilteredEnv` and `BankCurriculumManager` (auto-banding per optimal length) and attached sampling plan to `info` for monitoring.
- 2025-09-18: Fixed None handling for `info` from `env.reset()` in `PuzzleGenerator` to prevent `'NoneType' object has no attribute 'get'` errors during precompute runs.
- 2025-09-18: Ensured the controller respects custom curriculum configs by passing parsed levels through; default `train_agent.py` supports bank curriculum with `--curriculum --use_bank_curriculum` and a precomputed bank.

## Next steps
- Implement manifest v1.1 structure in `env/puzzle_bank.py` (histograms, fingerprints, dedup scaffolding).
- Wire `add_puzzles` to increment histograms and persist.
- Sketch precompute controller loop for chunked generation + stop criteria (no parallel yet).
- Add logging of coverage per curriculum level using histograms.
- Expose controller via CLI (optional new script entrypoint or extend `generate_curriculum_bank.py`).
- Add sampler quotas/backoff logic (stratified within level constraints) and monitoring hooks.
- Integrate stratified quotas into `CriteriaFilteredEnv`/`BankCurriculumManager` and surface monitoring via logs.
- Add unit tests for new sampler method and controller coverage calculation; update README with CLI usage for controller.

## Review #1

- Concerns
  - Bank reproducibility depends on regeneration-from-seed; `layout_blob` deserialization is unimplemented (`create_fixed_layout_from_metadata` raises NotImplementedError). Any generator drift breaks replay despite stored hashes. Suggest prioritizing a compact, versioned layout serialization and round-trip tests.
  - Hash discipline: `SpecKey.generator_rules_version` defaults to "1.0" but there’s no enforcement that code changes bump it. Mismatch between stored `board_hash`/`layout_hash` and regenerated boards will silently fall back to exceptions only at runtime. Add an explicit generator fingerprint in manifest and assert on load.
  - Solver metadata gaps: `nodes_expanded` and node-limit hits are not tracked in `PuzzleGenerator` (always 0/False). This weakens difficulty diagnostics and bank QA. The solvers already support metadata variants; wire them in.
  - Criteria confusion: `precompute_pipeline.PuzzleGenerator.generate_puzzles_for_spec` treats `criteria['max_depth']` as a solver cutoff while curriculum criteria elsewhere refer to optimal_length and robots_moved. Risk of conflation in future changes; rename to `solver_max_depth_override` and keep types distinct.
  - Sampler cache growth: `BankSampler._criteria_cache` is unbounded and never evicted; long runs with varying bands/criteria can grow memory. Add LRU with max entries and metrics.
  - Manifest histograms: counts updated only via `add_puzzles`; no validation against file contents. If external edits occur or duplicates are appended, histograms can drift. Consider recompute/verify utility and duplicate prevention (e.g., unique `(spec_key, board_hash)` index or dedup pass).
  - Stratified sampling coverage: allocation by weights doesn’t check availability before planning, so many bands may underfill silently. Expose deficit and optionally reallocate leftover to other bands to keep `total_sampled` close to `total_count`.
  - Fallback path in `CriteriaFilteredEnv`: on failed sample, it creates a fresh random solvable env, which violates “no online solving during training”. At minimum, log and count separately; ideally, never fallback in training mode and instead surface a clear error/metric to fix bank coverage.
  - RGB obs consistency: `RicochetRobotsEnv` sets fixed pixel size 96 for rgb_image but comments reference 128 in places; ensure all constants are consistent and documented. Also verify channels_first path for rgb returns the exact Box shape.
  - Reward/termination semantics: step uses `step_penalty`, optional `noop_penalty`, and goal reward, but no per-move optimality shaping. That’s fine, but ensure tests cover truncation (`TimeLimit.truncated`) and that success is only set when not truncated (some wrappers rely on this, and `CriteriaFilteredEnv` assumes it).

- Questions/uncertainties
  - Should bank usage in training hard-fail when criteria underfill rather than fallback to online env? If not, what’s the desired policy for deficit reallocation among admissible bands?
  - Target coverage per level: README suggests 1000; do we want adaptive targets based on observed hit-rates per spec to balance total compute?
  - Multiple solvers: keep BFS default, but do we anticipate changing heuristics (e.g., `admissible_one`) during generation? If yes, we need solver fingerprints in manifest and possibly separate partitions or fields.
  - Curriculum advancement: current `BankCurriculumManager` uses windowed success rate. Do we also want an optimality-gap threshold to avoid advancing on sloppy solves when metadata is available?
  - Dedup granularity: prefer `layout_hash` or full `board_hash`? If we allow multi-round use of layouts with different target robots, layout-level dedup seems preferable; confirm.

- Suggested improvements/fixes (high priority)
  - Implement layout serialization/deserialization: define a minimal binary schema (dims, walls, robots, goal, target) with version header; store in `layout_blob` and validate round-trip against hashes. Add tests.
  - Wire metadata-rich solving in precompute: use `solve_bfs_with_metadata`/`solve_astar_with_metadata` to populate `nodes_expanded`, and set `hit_node_limit`/`hit_depth_limit` correctly.
  - Rename/clarify `criteria` usage in precompute controller: use explicit `solver_max_depth_override`; keep curriculum filters separate and enforced only at allocation/coverage.
  - Add LRU cache and limits to `BankSampler` iterators; expose cache size metrics and an API to clear per spec.
  - Enhance stratified sampler: compute availability per band from cached filtered views; reallocate unfilled quota to bands with surplus; record deficits in `plan` for monitoring.
  - Add a `verify_manifest` CLI to recompute histograms from Parquet, detect duplicates, and optionally repair manifest.
  - Make `CriteriaFilteredEnv`’s fallback optional and default to disabled in training; when disabled, raise with a helpful message including coverage stats from manifest.
  - Standardize RGB constants and document in `README.md`; add a unit test that asserts observation space shape and dtype for both orders.

- Nice-to-have / future work
  - Store per-partition Bloom filter for `layout_hash` to avoid regenerating duplicates during long runs; rotate shards to bound size.
  - Record solver time and nodes in manifest aggregates to guide spec prioritization in the controller (puzzles/sec, hit-rate per spec/band).
  - Add quota-based curriculum level sampler that draws proportionally from admissible bands and respects per-band caps to avoid mode collapse.
  - Provide a tiny demo bank in repo for tests/integration to decouple CI from heavy precompute.

- Evidence from tests vs code
  - Tests cover stratified weight allocation and histogram updates; current sampler meets allocation but doesn’t reallocate deficits (acceptable but noted).
  - Precompute controller coverage test only asserts non-negative coverage; expand tests to check exact counts and spec matching with mixed entries.
  - Env solvability tests validate `optimal_length` and solver limits; consistent with implementation.

```

I reviewed env/solver, bank/curriculum, precompute controller, tests, and README/WIP, then added a Review #1 section capturing concrete issues and suggestions. All todos are checked off.

- Added ‘Review #1’ to `WorkInProgress.md` with:
  - Concerns: seed-only reproducibility and missing `layout_blob` round-trip; manifest/versioning discipline; solver metadata not wired; criteria naming conflation; unbounded sampler cache; histogram drift/duplicates; stratified underfill handling; fallback to online env; RGB obs constant mismatch; reward/termination semantics.
  - Questions: strict bank-only policy vs fallback; coverage targets; multiple solver fingerprints; advancement using optimality gap; dedup granularity.
  - High-priority fixes: implement layout serialization; wire solver metadata; clarify controller overrides; LRU cache in `BankSampler`; stratified reallocation; manifest verify CLI; make fallback optional/disabled for training; standardize RGB constants/tests.
  - Nice-to-haves: bloom dedup; controller guided by cost; quota-based level sampler; tiny demo bank for CI.
  - Test alignment notes and suggested expansions.

Would you like me to prioritize implementing layout serialization and solver metadata wiring next?

---

**User**

Before we make any changes, please could you write a quick script in the style of initial_testing_cells.py (ie, using jupyter cell tags) which reads in and displays (using matplotlib/rgb_image mode) a few sample puzzles from each level in curriculum_config_example3.json

---

**Cursor**

I’m going to add a new Jupyter-style script with #%% cells that loads `curriculum_config_example3.json`, samples a few puzzles per level from the bank, and displays them using Matplotlib in `rgb_image` mode.

```python
#%% Imports and setup
import json
from pathlib import Path
import matplotlib.pyplot as plt

from env.puzzle_bank import PuzzleBank, BankSampler, PuzzleMetadata
from env.curriculum_config import load_curriculum_config_from_json
from env.puzzle_bank import create_fixed_layout_from_seed
from env.ricochet_env import RicochetRobotsEnv


#%% Configuration
# Path to the curriculum config and bank directory
PROJECT_ROOT = Path(__file__).parent
CONFIG_PATH = PROJECT_ROOT / "curriculum_config_example3.json"
BANK_DIR = PROJECT_ROOT / "puzzle_bank"

# How many samples to preview per level
SAMPLES_PER_LEVEL = 3


#%% Load curriculum levels
levels = load_curriculum_config_from_json(str(CONFIG_PATH))
print(f"Loaded {len(levels)} levels from {CONFIG_PATH.name}")
for lvl in levels:
    print(f"Level {lvl.level}: {lvl.name} | spec: {lvl.spec_key} | OL:[{lvl.min_optimal_length},{lvl.max_optimal_length}] RM:[{lvl.min_robots_moved},{lvl.max_robots_moved}]")


#%% Initialize bank and sampler
bank = PuzzleBank(str(BANK_DIR))
sampler = BankSampler(bank)
print(f"Bank dir: {BANK_DIR}")
print(f"Total puzzles in bank: {bank.get_puzzle_count()}")


#%% Helper: render one puzzle as rgb image using a fixed layout env
def render_puzzle_rgb(metadata: PuzzleMetadata):
    fixed_layout = create_fixed_layout_from_seed(metadata)
    env = RicochetRobotsEnv(
        fixed_layout=fixed_layout,
        obs_mode="rgb_image",
        channels_first=False,  # HWC for matplotlib
        include_noop=False,
        render_mode=None,
    )
    obs, info = env.reset()
    env.close()
    return obs, info


#%% Preview: sample and display puzzles for each level
for lvl in levels:
    print(f"\n=== Level {lvl.level}: {lvl.name} ===")
    samples = sampler.sample_puzzles(
        spec_key=lvl.spec_key,
        count=SAMPLES_PER_LEVEL,
        min_optimal_length=lvl.min_optimal_length,
        max_optimal_length=lvl.max_optimal_length,
        min_robots_moved=lvl.min_robots_moved,
        max_robots_moved=lvl.max_robots_moved,
    )

    if len(samples) == 0:
        print("No matching puzzles found in bank for this level.")
        continue

    cols = len(samples)
    fig, axes = plt.subplots(1, cols, figsize=(4 * cols, 4))
    if cols == 1:
        axes = [axes]

    for idx, (ax, md) in enumerate(zip(axes, samples)):
        try:
            rgb, info = render_puzzle_rgb(md)
            ax.imshow(rgb)
            title = f"seed={md.seed}\nOL={md.optimal_length}, RM={md.robots_moved}"
            ax.set_title(title)
            ax.axis('off')
        except Exception as e:
            ax.set_title(f"Error rendering: {e}")
            ax.axis('off')

    fig.suptitle(f"Level {lvl.level}: {lvl.name}")
    plt.tight_layout()
    plt.show()


```

I added a Jupyter-style preview script with #%% cells that loads `curriculum_config_example3.json`, samples a few puzzles per level from the bank, and renders them via Matplotlib using `rgb_image`.

- File: `initial_bank_preview_cells.py`
- It:
  - Loads levels with `load_curriculum_config_from_json`.
  - Uses `PuzzleBank` and `BankSampler` to fetch SAMPLES_PER_LEVEL per level.
  - Reconstructs boards via `create_fixed_layout_from_seed`.
  - Creates a `RicochetRobotsEnv` in `rgb_image` mode and displays frames.

Run tips:
- Open `initial_bank_preview_cells.py` in VS Code/Cursor or any editor that supports Python “cells” and run each `#%%` block top-to-bottom.
- Or run as a normal script: `python initial_bank_preview_cells.py` (it will print and show Matplotlib windows).

- Config defaults:
  - Bank: `puzzle_bank/`
  - Config: `curriculum_config_example3.json`
  - Samples per level: 3

Would you like me to point it at a different bank directory or adjust the sample count?

---

**User**

Hmmm this is very strange. When I render these images, I don't see any internal walls (ie edge-T or central-L structures). I am pretty sure I generated them with 2 of each per quadrant. Can you work out if it's just a rendering issue or if it's a real thing? 

---

**Cursor**

Quick check: it’s a rendering bug, not missing walls.

- Root cause: internal walls overlay is composited with np.maximum against a white background, so black wall pixels never show up.
  - In `env/ricochet_env.py` both render paths do this:
```686:711:/Users/sidbaines/Documents/Code/20250911_RLNew/env/ricochet_env.py
        with profile("env_rgb_draw_internal_walls", track_memory=True):
            if self._rgb_internal_walls_overlay is None:
                self._build_rgb_internal_walls_overlay(board)
            if self._rgb_internal_walls_overlay is not None:
                img = np.maximum(img, self._rgb_internal_walls_overlay)
```
```726:734:/Users/sidbaines/Documents/Code/20250911_RLNew/env/ricochet_env.py
        bg = self._get_rgb_background(H, W, cell_size, grid_color, grid_th, wall_color, wall_th)
        img = bg.copy()
        if self._rgb_internal_walls_overlay is None or self._rgb_internal_walls_overlay.shape[:2] != (H_px, W_px):
            self._build_rgb_internal_walls_overlay(board, cell_size_override=cell_size)
        if self._rgb_internal_walls_overlay is not None:
            img = np.maximum(img, self._rgb_internal_walls_overlay)
```
- Because the overlay draws walls as black (0) on a zero background, np.maximum(background, overlay) keeps the background’s white value. You want np.minimum (black over lighter) or a mask-based overwrite.

How to verify it’s not generation:
- In your preview cell, after creating `fixed_layout`, print internal wall counts:
  - `board = RicochetRobotsEnv(fixed_layout=fixed_layout).get_board()`
  - `print(board.h_walls[1:-1].any(), board.v_walls[:,1:-1].any())` should be True if internal walls exist.
- You can also switch rendering to draw bright walls and use np.maximum temporarily, but the correct fix is to change the compositing.

Proposed fix (no code applied yet):
- Replace both `np.maximum(img, self._rgb_internal_walls_overlay)` with either:
  - `np.minimum(img, self._rgb_internal_walls_overlay)` and ensure overlay uses dark walls on white background; or
  - Masked overwrite:
    - Compute a boolean mask where overlay is nonzero and set `img[mask] = overlay[mask]`.

Summary of impact:
- After this change, edge-T and central-L walls will be visible in both rgb render and rgb_image observations.

```python
        with profile("env_rgb_draw_internal_walls", track_memory=True):
            if self._rgb_internal_walls_overlay is None:
                self._build_rgb_internal_walls_overlay(board)
            if self._rgb_internal_walls_overlay is not None:
                img = np.maximum(img, self._rgb_internal_walls_overlay)
```

```python
        bg = self._get_rgb_background(H, W, cell_size, grid_color, grid_th, wall_color, wall_th)
        img = bg.copy()
        if self._rgb_internal_walls_overlay is None or self._rgb_internal_walls_overlay.shape[:2] != (H_px, W_px):
            self._build_rgb_internal_walls_overlay(board, cell_size_override=cell_size)
        if self._rgb_internal_walls_overlay is not None:
            img = np.maximum(img, self._rgb_internal_walls_overlay)
```

---

**User**

Please can you implement whichever fix you think is best?

---

**Cursor**

I'll replace the np.maximum-based compositing with a mask-based overwrite so internal walls draw correctly over the background in both RGB rendering paths.

```python
from __future__ import annotations

from dataclasses import dataclass
from typing import Dict, Optional, Tuple, List, Literal, Set

import numpy as np

from .ricochet_core import Board, apply_action, reached_goal

# Provide a runtime gymnasium binding if available, otherwise a minimal fallback
try:  # pragma: no cover
    import importlib
    _gym = importlib.import_module("gymnasium")  # type: ignore
    _spaces = importlib.import_module("gymnasium.spaces")  # type: ignore
    _seeding = importlib.import_module("gymnasium.utils.seeding")  # type: ignore
    _error = importlib.import_module("gymnasium.error")  # type: ignore
    GymEnvBase = _gym.Env  # type: ignore
    spaces = _spaces  # type: ignore
    GymInvalidAction = getattr(_error, "InvalidAction", None)  # type: ignore
except (ModuleNotFoundError, ImportError):  # pragma: no cover
    class GymEnvBase(object):
        pass
    class _SpacesDummy:  # minimal placeholders to satisfy type usage
        class Box:  # type: ignore
            def __init__(self, low=None, high=None, shape=None, dtype=None):
                self.low = low
                self.high = high
                self.shape = shape
                self.dtype = dtype
        class Discrete:  # type: ignore
            def __init__(self, n: int):
                self.n = n
    spaces = _SpacesDummy()  # type: ignore
    GymInvalidAction = None  # type: ignore


@dataclass
class FixedLayout:
    height: int
    width: int
    # Edge walls
    h_walls: np.ndarray  # (H+1, W) bool
    v_walls: np.ndarray  # (H, W+1) bool
    robot_positions: Dict[int, Tuple[int, int]]
    goal_position: Tuple[int, int]
    target_robot: int


ObsMode = Literal["image", "symbolic", "rgb_image"]


class RicochetRobotsEnv(GymEnvBase):
    """Ricochet Robots Gymnasium-compatible environment.

    Determinism and seeding:
    - Deterministic given the seed provided to __init__ or reset(seed=...).
    - Randomness flows through self.np_random (Gymnasium seeding utils); the episode's resolved
      seed is exposed via info["episode_seed"].
    - When ensure_solvable=True, the environment may retry board generation; retries do not add
      extra non-determinism beyond RNG draws. The included solver is deterministic.

    Observations:
    - Image mode: float32 in [0,1]. Channels are named by channel_names; note that the target robot
      is represented both by a dedicated mask (index 5) and its robot channel (duplication by design).
    - Symbolic mode: float32 with absolute grid indices (not normalized). Downstream code may wish to
      normalize to [0,1]. The Box low/high reflect board bounds.

    Rendering:
    - Only render_mode="ascii" is implemented and returns a string frame.
    - Other modes are not implemented and will raise a clear error.
    """
    metadata = {"render_modes": ["ascii", "rgb"], "render_fps": 4}
    # Class-level cache for RGB backgrounds per (H, W, cell_size, grid_color, grid_th, wall_color, wall_th)
    _RGB_BG_CACHE: Dict[Tuple[int, int, int, Tuple[int, int, int], int, Tuple[int, int, int], int], np.ndarray] = {}

    def __init__(
        self,
        height: int = 8,
        width: int = 8,
        num_robots: int = 2,
        include_noop: bool = True,
        step_penalty: float = -0.01,
        goal_reward: float = 1.0,
        noop_penalty: Optional[float] = None,
        max_steps: int = 10,
        fixed_layout: Optional[FixedLayout] = None,
        seed: Optional[int] = None,
        ensure_solvable: bool = False,
        solver_max_depth: int = 30,
        solver_max_nodes: int = 20000,
        obs_mode: ObsMode = "image",
        channels_first: bool = False,
        render_mode: Optional[str] = None,
        ensure_attempt_limit: int = 500,
        # Spec-driven generation configuration
        edge_t_per_quadrant: Optional[int] = None,
        central_l_per_quadrant: Optional[int] = None,
        # RGB render configuration
        render_rgb_config: Optional[Dict[str, object]] = None,
    ) -> None:
        super().__init__()
        self.height = height
        self.width = width
        self.num_robots = num_robots
        self.include_noop = include_noop
        self.step_penalty = step_penalty
        self.goal_reward = goal_reward
        self.noop_penalty = step_penalty if noop_penalty is None else noop_penalty
        self.max_steps = max_steps
        self.fixed_layout = fixed_layout
        self.ensure_solvable = ensure_solvable
        self.solver_max_depth = solver_max_depth
        self.solver_max_nodes = solver_max_nodes
        self.obs_mode: ObsMode = obs_mode
        self.channels_first = channels_first
        # If a fixed layout is provided, align env geometry before building spaces
        if self.fixed_layout is not None:
            self.height = int(self.fixed_layout.height)
            self.width = int(self.fixed_layout.width)
            self.num_robots = int(len(self.fixed_layout.robot_positions))
        # Render API
        self.render_mode = render_mode
        if self.render_mode is not None and self.render_mode not in self.metadata["render_modes"]:
            raise ValueError(f"Unsupported render_mode {self.render_mode}. Supported: {self.metadata['render_modes']}")
        # RGB render defaults
        self._render_rgb_cfg: Dict[str, object] = {
            "cell_size": 20,
            "grid_color": (200, 200, 200),
            "grid_thickness": 1,
            "wall_color": (0, 0, 0),
            "wall_thickness": 3,
            "robot_colors": [
                (220, 50, 50),     # red
                (50, 120, 220),    # blue
                (50, 180, 80),     # green
                (230, 200, 40),    # yellow
                (180, 80, 180),    # purple (extra if more robots)
            ],
            "circle_fill": True,
            "circle_radius_frac": 0.35,
            "target_dark_factor": 0.6,
            "star_thickness": 2,
        }
        if render_rgb_config is not None:
            self._render_rgb_cfg.update(render_rgb_config)
        # Per-instance cache: RGB internal walls overlay built for current layout (excludes boundary walls)
        self._rgb_internal_walls_overlay: Optional[np.ndarray] = None

        # Seeding: adopt Gymnasium convention with self.np_random
        self.np_random: np.random.Generator
        self._episode_seed: Optional[int] = None
        try:  # gymnasium present
            # _seeding.np_random returns (rng, seed)
            self.np_random, used_seed = _seeding.np_random(seed)  # type: ignore[name-defined]
            self._episode_seed = int(used_seed) if used_seed is not None else None
        except NameError:
            self.np_random = np.random.default_rng(seed)
            self._episode_seed = int(seed) if seed is not None else None
        # Preserve a base seed to enforce deterministic resets when no new seed is provided
        self._base_seed: Optional[int] = int(seed) if seed is not None else self._episode_seed
        self._board: Optional[Board] = None
        self._num_steps = 0
        self._cached_wall_obs: Optional[np.ndarray] = None  # (H,W,4) float32 for wall channels
        self._ensure_attempt_limit = int(ensure_attempt_limit)
        # Spec default counts per quadrant (based on smaller board side)
        side_min = min(self.height, self.width)
        default_per_quad = 2 if side_min < 10 else 4
        self._edge_t_per_quadrant = default_per_quad if edge_t_per_quadrant is None else int(edge_t_per_quadrant)
        self._central_l_per_quadrant = default_per_quad if central_l_per_quadrant is None else int(central_l_per_quadrant)
        # Track the exact per-episode board seed used to generate the current board
        self._last_board_seed: Optional[int] = None

        # Observation spaces
        if self.obs_mode == "image":
            # channels: up_wall, down_wall, left_wall, right_wall, goal, target_mask, robots...
            self._base_wall_channels = 4
            self._base_misc_channels = 2  # goal, target
            self._num_channels = self._base_wall_channels + self._base_misc_channels + self.num_robots
            if self.channels_first:
                obs_shape = (self._num_channels, self.height, self.width)
            else:
                obs_shape = (self.height, self.width, self._num_channels)
            self.observation_space = spaces.Box(
                low=0.0,
                high=1.0,
                shape=obs_shape,
                dtype=np.float32,
            )
        elif self.obs_mode == "rgb_image":
            # RGB image observation: uses fixed pixel dimensions for consistent observation space
            # This ensures all curriculum levels have the same observation space size
            # Lowered from 128 to 96 to reduce memory/compute; resize handled in _board_to_rgb_obs
            FIXED_PIXEL_SIZE = 256
            
            if self.channels_first:
                obs_shape = (3, FIXED_PIXEL_SIZE, FIXED_PIXEL_SIZE)
            else:
                obs_shape = (FIXED_PIXEL_SIZE, FIXED_PIXEL_SIZE, 3)
            self.observation_space = spaces.Box(
                low=0,
                high=255,
                shape=obs_shape,
                dtype=np.uint8,
            )
        elif self.obs_mode == "symbolic":
            # [goal_r, goal_c] in [0..H-1], [0..W-1]; one-hot target in [0,1]; robots positions in bounds
            vec_len = 2 + self.num_robots + (self.num_robots * 2)
            low = np.full((vec_len,), 0.0, dtype=np.float32)
            high = np.full((vec_len,), 1.0, dtype=np.float32)
            # goal bounds
            low[0] = 0.0
            high[0] = float(self.height - 1)
            low[1] = 0.0
            high[1] = float(self.width - 1)
            # target one-hot already [0,1]
            # robots positions
            base = 2 + self.num_robots
            for i in range(self.num_robots):
                low[base + 2 * i] = 0.0
                high[base + 2 * i] = float(self.height - 1)
                low[base + 2 * i + 1] = 0.0
                high[base + 2 * i + 1] = float(self.width - 1)
            self.observation_space = spaces.Box(low=low, high=high, shape=(vec_len,), dtype=np.float32)
        else:
            raise ValueError(f"Unknown obs_mode: {self.obs_mode}")

        # Action space: num_robots * 4 directions (+ 1 for noop)
        self._noop_action = self.num_robots * 4 if include_noop else None
        self.action_space = spaces.Discrete(self.num_robots * 4 + (1 if include_noop else 0))

    # Gymnasium API
    def reset(self, *, seed: Optional[int] = None, options: Optional[dict] = None):
        """Reset the environment and start a new episode.

        Returns (obs, info) per Gymnasium API. info contains keys: 'target_robot',
        'level_solvable' (optional), 'optimal_length' (if ensured), 'solver_limits' (if ensured),
        and 'episode_seed' with the seed used for this episode.
        """
        # Import profiling here to avoid circular imports
        try:
            from profiling import profile
        except ImportError:
            # Fallback if profiling not available
            def profile(name, track_memory=True):
                from contextlib import nullcontext
                return nullcontext()

        # Standard seeding protocol
        if seed is not None:
            try:
                self.np_random, used_seed = _seeding.np_random(seed)  # type: ignore[name-defined]
                self._episode_seed = int(used_seed) if used_seed is not None else None
            except NameError:
                self.np_random = np.random.default_rng(seed)
                self._episode_seed = int(seed)
            self._base_seed = int(self._episode_seed) if self._episode_seed is not None else None
        else:
            # Re-seed with base seed so that resets are deterministic when no new seed is provided
            if self._base_seed is not None:
                try:
                    self.np_random, used_seed = _seeding.np_random(self._base_seed)  # type: ignore[name-defined]
                    self._episode_seed = int(used_seed) if used_seed is not None else int(self._base_seed)
                except NameError:
                    self.np_random = np.random.default_rng(self._base_seed)
                    self._episode_seed = int(self._base_seed)
        _ = options  # silence unused
        self._num_steps = 0
        # clear cached walls in case layout changes
        self._cached_wall_obs = None

        if self.fixed_layout is not None:
            with profile("env_reset_fixed_layout", track_memory=True):
                self._board = Board(
                height=self.fixed_layout.height,
                width=self.fixed_layout.width,
                h_walls=self.fixed_layout.h_walls.copy(),
                v_walls=self.fixed_layout.v_walls.copy(),
                robot_positions=dict(self.fixed_layout.robot_positions),
                goal_position=self.fixed_layout.goal_position,
                target_robot=self.fixed_layout.target_robot,
            )
            # cache static walls for this layout
            with profile("env_build_cached_walls", track_memory=True):
                self._build_cached_walls(self._board)
            # Build per-layout internal walls RGB overlay cache (for rgb render mode)
            try:
                if self.render_mode == "rgb":
                    self._build_rgb_internal_walls_overlay(self._board)
            except Exception:
                self._rgb_internal_walls_overlay = None
            with profile("env_make_obs", track_memory=True):
                obs = self._make_obs(self._board)
            info = {
                "level_solvable": False,
                "ensure_solvable_enabled": False,
                "target_robot": self._board.target_robot,
                "episode_seed": self._episode_seed,
            }
            if self.obs_mode == "image":
                info["channel_names"] = self.channel_names
            return obs, info

        # Otherwise, generate random board, optionally ensure solvable via solver
        attempts = 0
        while True:
            attempts += 1
            with profile("env_generate_board", track_memory=True):
                board = self._generate_random_board()
            if not self.ensure_solvable:
                self._board = board
                break
            try:
                from .solver import solve_bfs  # type: ignore
                with profile("env_solver_check", track_memory=True):
                    solution = solve_bfs(board, max_depth=self.solver_max_depth, max_nodes=self.solver_max_nodes)
                if solution is not None:
                    self._board = board
                    optimal_length = len(solution)
                    break
            except ImportError as exc:
                raise RuntimeError("ensure_solvable=True but solver unavailable (ImportError)") from exc
            if attempts >= self._ensure_attempt_limit:
                raise RuntimeError("Failed to generate solvable board within attempt limit")

        # cache static walls for this layout
        assert self._board is not None
        with profile("env_build_cached_walls", track_memory=True):
            self._build_cached_walls(self._board)
        # Build per-layout internal walls RGB overlay cache (for rgb render mode)
        try:
            if self.render_mode == "rgb":
                self._build_rgb_internal_walls_overlay(self._board)
        except Exception:
            self._rgb_internal_walls_overlay = None
        with profile("env_make_obs", track_memory=True):
            obs = self._make_obs(self._board)
        # Note: solver None during attempts may be due to cutoffs (depth/nodes), not true unsolvability.
        info = {
            "level_solvable": bool(self.ensure_solvable),
            "ensure_solvable_enabled": bool(self.ensure_solvable),
            "target_robot": self._board.target_robot,
            "episode_seed": self._episode_seed,
        }
        # Expose the exact per-episode board seed used to build this board
        if hasattr(self, "_last_board_seed") and self._last_board_seed is not None:
            info["board_seed"] = int(self._last_board_seed)
        if self.obs_mode == "image":
            info["channel_names"] = self.channel_names
        if self.ensure_solvable:
            info["optimal_length"] = optimal_length
            info["solver_limits"] = {"max_depth": self.solver_max_depth, "max_nodes": self.solver_max_nodes}
        return obs, info

    def step(self, action: int):
        """Apply an action and return (obs, reward, terminated, truncated, info).

        info always includes 'steps', 'target_robot', and 'is_success' (bool). When truncated due to
        max steps, info includes 'TimeLimit.truncated'=True.
        """
        # Import profiling here to avoid circular imports
        try:
            from profiling import profile
        except ImportError:
            # Fallback if profiling not available
            def profile(name, track_memory=True):
                from contextlib import nullcontext
                return nullcontext()

        assert self._board is not None, "Call reset() first"
        self._num_steps += 1

        terminated = False
        truncated = False
        info: Dict[str, object] = {}

        reward = self.step_penalty
        # Validate action range early
        if not (0 <= action < self.action_space.n):
            if GymInvalidAction is not None:
                raise GymInvalidAction(f"Invalid action {action}")  # type: ignore[misc]
            raise ValueError(f"Invalid action {action}")

        if self.include_noop and action == self._noop_action:
            reward = self.noop_penalty
        else:
            with profile("env_apply_action", track_memory=True):
                robot_id, direction = self._decode_action(action)
                self._board = apply_action(self._board, robot_id, direction)

        if reached_goal(self._board):
            reward += self.goal_reward
            terminated = True
            info["is_success"] = True

        if not terminated and self._num_steps >= self.max_steps:
            truncated = True
            info["TimeLimit.truncated"] = True

        with profile("env_make_obs", track_memory=True):
            obs = self._make_obs(self._board)
        info["steps"] = self._num_steps
        info["target_robot"] = self._board.target_robot
        if "is_success" not in info:
            info["is_success"] = False
        return obs, reward, terminated, truncated, info

    def render(self):
        """Render according to render_mode.

        - render_mode=="ascii": returns a string frame.
        - render_mode=="rgb": returns an (H_px, W_px, 3) uint8 array.
        - render_mode is None or unsupported: returns None.
        """
        if self.render_mode is None:
            return None
        if self._board is None:
            return "" if self.render_mode == "ascii" else None
        if self.render_mode == "ascii":
            from .ricochet_core import render_ascii  # local import to avoid circulars in lints
            return render_ascii(self._board)
        if self.render_mode == "rgb":
            return self._render_rgb(self._board)
        if self._board is None:
            return ""
        raise ValueError(f"Unsupported render_mode {self.render_mode}. Supported: {self.metadata['render_modes']}")

    def close(self) -> None:
        """Close environment resources (no-op)."""
        return None

    def seed(self, seed: Optional[int] = None):  # legacy convenience
        """Legacy seeding helper returning [seed]. Prefer reset(seed=...)."""
        if seed is None:
            return [self._episode_seed]
        try:
            self.np_random, used_seed = _seeding.np_random(seed)  # type: ignore[name-defined]
            self._episode_seed = int(used_seed) if used_seed is not None else int(seed)
        except NameError:
            self.np_random = np.random.default_rng(seed)
            self._episode_seed = int(seed)
        return [self._episode_seed]

    # Helpers
    def _decode_action(self, action: int) -> Tuple[int, int]:
        if not (0 <= action < self.action_space.n):
            raise ValueError(f"Invalid action {action}")
        if self.include_noop and action == self._noop_action:
            raise ValueError("_decode_action called on noop action")
        robot_id = action // 4
        direction = action % 4
        if robot_id >= self.num_robots:
            raise ValueError(f"Invalid robot_id {robot_id} for action {action}")
        return robot_id, direction

    def _make_obs(self, board: Board) -> np.ndarray:
        if self.obs_mode == "image":
            return self._board_to_image_obs(board)
        elif self.obs_mode == "rgb_image":
            return self._board_to_rgb_obs(board)
        return self._board_to_symbolic_obs(board)

    def _board_to_image_obs(self, board: Board) -> np.ndarray:
        # Import profiling here to avoid circular imports
        try:
            from profiling import profile
        except ImportError:
            # Fallback if profiling not available
            def profile(name, track_memory=True):
                from contextlib import nullcontext
                return nullcontext()

        with profile("env_image_observation_generation", track_memory=True):
            h, w = board.height, board.width
            C = self._num_channels
            obs = np.zeros((h, w, C), dtype=np.float32)
            # wall channels 0..3 from cache
            if self._cached_wall_obs is not None:
                obs[:, :, 0:4] = self._cached_wall_obs
            else:
                # Fallback if cache not built (should not happen)
                for r in range(h):
                    for c in range(w):
                        obs[r, c, 0] = 1.0 if board.has_wall_up(r, c) else 0.0
                        obs[r, c, 1] = 1.0 if board.has_wall_down(r, c) else 0.0
                        obs[r, c, 2] = 1.0 if board.has_wall_left(r, c) else 0.0
                        obs[r, c, 3] = 1.0 if board.has_wall_right(r, c) else 0.0
            # goal channel 4
            gr, gc = board.goal_position
            obs[gr, gc, 4] = 1.0
            # target mask channel 5
            tr = board.target_robot
            tr_r, tr_c = board.robot_positions[tr]
            obs[tr_r, tr_c, 5] = 1.0
            # robot channels start at 6
            base = 6
            for rid, (r, c) in board.robot_positions.items():
                obs[r, c, base + rid] = 1.0
            if self.channels_first:
                obs = np.transpose(obs, (2, 0, 1))
            return obs

    def _board_to_symbolic_obs(self, board: Board) -> np.ndarray:
        # Import profiling here to avoid circular imports
        try:
            from profiling import profile
        except ImportError:
            # Fallback if profiling not available
            def profile(name, track_memory=True):
                from contextlib import nullcontext
                return nullcontext()

        with profile("env_symbolic_observation_generation", track_memory=True):
            gr, gc = board.goal_position
            target_one_hot = np.zeros((self.num_robots,), dtype=np.float32)
            target_one_hot[board.target_robot] = 1.0
            robots_vec: List[float] = []
            for rid in range(self.num_robots):
                r, c = board.robot_positions[rid]
                robots_vec.extend([float(r), float(c)])
            vec = np.concatenate([
                np.array([float(gr), float(gc)], dtype=np.float32),
                target_one_hot.astype(np.float32),
                np.array(robots_vec, dtype=np.float32),
            ])
            return vec

    def _board_to_rgb_obs(self, board: Board) -> np.ndarray:
        """Convert board to RGB image observation with fixed pixel dimensions.
        
        This creates an RGB image with fixed pixel dimensions (128x128) by adjusting
        the cell size to fit the board within the fixed pixel space. This ensures
        consistent observation space across all curriculum levels.
        """
        # Import profiling here to avoid circular imports
        try:
            from profiling import profile
        except ImportError:
            # Fallback if profiling not available
            def profile(name, track_memory=True):
                from contextlib import nullcontext
                return nullcontext()
        
        with profile("env_rgb_observation_generation", track_memory=True):
            FIXED_PIXEL_SIZE = 256
            # Padding around the board within the fixed output
            padding = 4
            available_size = FIXED_PIXEL_SIZE - 2 * padding
            cell_size = max(1, available_size // max(board.height, board.width))
            # Compute board pixel dims and canvas
            H_px = int(board.height * cell_size)
            W_px = int(board.width * cell_size)
            # Compose into fixed-size canvas without resizing
            canvas = np.ones((FIXED_PIXEL_SIZE, FIXED_PIXEL_SIZE, 3), dtype=np.uint8) * 255
            y0 = padding + (available_size - H_px) // 2
            x0 = padding + (available_size - W_px) // 2
            # Render board region with caching
            with profile("env_rgb_rendering_cached", track_memory=True):
                region = self._render_rgb_cached(board, cell_size)
            canvas[y0:y0+H_px, x0:x0+W_px] = region
            if self.channels_first:
                canvas = np.transpose(canvas, (2, 0, 1))
            return canvas

    def _generate_random_board(self) -> Board:
        # Draw a per-episode board seed from the env RNG
        seed_used = int(self.np_random.integers(0, 2**31 - 1))
        # Record it for reproducibility/debugging
        self._last_board_seed = seed_used
        # Generate deterministically from the explicit seed only (no external RNG)
        return generate_board_from_spec_with_seed(
            height=self.height,
            width=self.width,
            num_robots=self.num_robots,
            edge_t_per_quadrant=self._edge_t_per_quadrant,
            central_l_per_quadrant=self._central_l_per_quadrant,
            seed=seed_used,
        )

    # Public channel names for image observations
    @property
    def channel_names(self) -> List[str]:
        """Human-readable names for image observation channels."""
        names = ["wall_up", "wall_down", "wall_left", "wall_right", "goal", "target_robot_mask"]
        names.extend([f"robot_{i}" for i in range(self.num_robots)])
        return names

    def _build_cached_walls(self, board: Board) -> None:
        """Precompute static wall indicator channels (H,W,4)."""
        h, w = board.height, board.width
        wall = np.zeros((h, w, 4), dtype=np.float32)
        for r in range(h):
            for c in range(w):
                wall[r, c, 0] = 1.0 if board.has_wall_up(r, c) else 0.0
                wall[r, c, 1] = 1.0 if board.has_wall_down(r, c) else 0.0
                wall[r, c, 2] = 1.0 if board.has_wall_left(r, c) else 0.0
                wall[r, c, 3] = 1.0 if board.has_wall_right(r, c) else 0.0
        self._cached_wall_obs = wall

    # Public accessor for current board (read-only clone)
    def get_board(self) -> Board:
        if self._board is None:
            raise RuntimeError("Environment not initialized. Call reset() first.")
        return self._board.clone()

    def _render_rgb(self, board: Board) -> np.ndarray:
        # Import profiling here to avoid circular imports
        try:
            from profiling import profile
        except ImportError:
            # Fallback if profiling not available
            def profile(name, track_memory=True):
                from contextlib import nullcontext
                return nullcontext()
        
        with profile("env_rgb_render_setup", track_memory=True):
            cfg = self._render_rgb_cfg
            cell_size = int(cfg["cell_size"])  # pixels
            grid_color = tuple(int(x) for x in cfg["grid_color"])  # type: ignore[index]
            grid_th = int(cfg["grid_thickness"])  # px
            wall_color = tuple(int(x) for x in cfg["wall_color"])  # type: ignore[index]
            wall_th = int(cfg["wall_thickness"])  # px
            robot_colors: List[Tuple[int, int, int]] = [tuple(int(a) for a in t) for t in cfg["robot_colors"]]  # type: ignore[index]
            circle_fill: bool = bool(cfg["circle_fill"])  # type: ignore[index]
            circle_r_frac: float = float(cfg["circle_radius_frac"])  # type: ignore[index]
            target_dark_factor: float = float(cfg["target_dark_factor"])  # type: ignore[index]
            star_th: int = int(cfg["star_thickness"])  # type: ignore[index]

            H, W = board.height, board.width
            H_px, W_px = H * cell_size, W * cell_size
            # Start from cached background with grid+boundary walls
            bg = self._get_rgb_background(H, W, cell_size, grid_color, grid_th, wall_color, wall_th)
            img = bg.copy()

        # Helpers to draw
        def draw_hline(y: int, x0: int, x1: int, color: Tuple[int, int, int], thickness: int) -> None:
            y0 = max(0, y - thickness // 2)
            y1 = min(H_px, y + (thickness - thickness // 2))
            x0c = max(0, min(x0, x1))
            x1c = min(W_px, max(x0, x1))
            img[y0:y1, x0c:x1c] = color

        def draw_vline(x: int, y0: int, y1: int, color: Tuple[int, int, int], thickness: int) -> None:
            x0 = max(0, x - thickness // 2)
            x1 = min(W_px, x + (thickness - thickness // 2))
            y0c = max(0, min(y0, y1))
            y1c = min(H_px, max(y0, y1))
            img[y0c:y1c, x0:x1] = color

        def draw_circle(cx: float, cy: float, radius: float, color: Tuple[int, int, int], fill: bool) -> None:
            # Bounding box
            x0 = int(max(0, np.floor(cx - radius)))
            x1 = int(min(W_px - 1, np.ceil(cx + radius)))
            y0 = int(max(0, np.floor(cy - radius)))
            y1 = int(min(H_px - 1, np.ceil(cy + radius)))
            yy, xx = np.ogrid[y0:y1 + 1, x0:x1 + 1]
            mask = (xx - cx) ** 2 + (yy - cy) ** 2 <= radius ** 2
            if fill:
                img[y0:y1 + 1, x0:x1 + 1][mask] = color
            else:
                # outline 1px
                inner = (xx - cx) ** 2 + (yy - cy) ** 2 <= (radius - 1) ** 2
                ring = np.logical_and(mask, np.logical_not(inner))
                img[y0:y1 + 1, x0:x1 + 1][ring] = color

        def draw_star(cx: float, cy: float, arm_len: float, color: Tuple[int, int, int], thickness: int) -> None:
            # Draw + arms
            draw_hline(int(cy), int(cx - arm_len), int(cx + arm_len), color, thickness)
            draw_vline(int(cx), int(cy - arm_len), int(cy + arm_len), color, thickness)
            # Draw x arms (diagonals) by sampling small line thickness via plotting short lines of squares
            # Approximate diagonals by drawing small rectangles along the diagonal
            steps = int(arm_len)
            for s in range(-steps, steps + 1):
                x = int(cx + s)
                y1 = int(cy + s)
                y2 = int(cy - s)
                y0a = max(0, y1 - thickness // 2)
                y1a = min(H_px, y1 + (thickness - thickness // 2))
                x0a = max(0, x - thickness // 2)
                x1a = min(W_px, x + (thickness - thickness // 2))
                img[y0a:y1a, x0a:x1a] = color
                y0b = max(0, y2 - thickness // 2)
                y1b = min(H_px, y2 + (thickness - thickness // 2))
                img[y0b:y1b, x0a:x1a] = color

        # 1) Internal walls overlay (excluding boundary walls) drawn once per layout
        with profile("env_rgb_draw_internal_walls", track_memory=True):
            if self._rgb_internal_walls_overlay is None:
                self._build_rgb_internal_walls_overlay(board)
            if self._rgb_internal_walls_overlay is not None:
                overlay = self._rgb_internal_walls_overlay
                mask = (overlay.sum(axis=2) > 0)
                img[mask] = overlay[mask]

        # 3) Draw robots as filled circles
        with profile("env_rgb_draw_robots", track_memory=True):
            radius = cell_size * float(circle_r_frac)
            for rid, (rr, cc) in board.robot_positions.items():
                color = robot_colors[rid % len(robot_colors)]
                cy = rr * cell_size + cell_size * 0.5
                cx = cc * cell_size + cell_size * 0.5
                draw_circle(cx, cy, radius, color, circle_fill)

        # 4) Draw target as a star in darker color of target robot
        with profile("env_rgb_draw_target", track_memory=True):
            tr = board.target_robot
            tr_color = robot_colors[tr % len(robot_colors)]
            dark_color = tuple(int(max(0, min(255, target_dark_factor * v))) for v in tr_color)
            gr, gc = board.goal_position
            cy = gr * cell_size + cell_size * 0.5
            cx = gc * cell_size + cell_size * 0.5
            draw_star(cx, cy, arm_len=cell_size * 0.35, color=dark_color, thickness=star_th)

        return img

    def _render_rgb_cached(self, board: Board, cell_size: int) -> np.ndarray:
        # Compose cached background + internal walls overlay + dynamic robots/target into board-sized image
        cfg = self._render_rgb_cfg
        grid_color = tuple(int(x) for x in cfg["grid_color"])  # type: ignore[index]
        grid_th = int(cfg["grid_thickness"])  # px
        wall_color = tuple(int(x) for x in cfg["wall_color"])  # type: ignore[index]
        wall_th = int(cfg["wall_thickness"])  # px
        robot_colors: List[Tuple[int, int, int]] = [tuple(int(a) for a in t) for t in cfg["robot_colors"]]  # type: ignore[index]
        circle_fill: bool = bool(cfg["circle_fill"])  # type: ignore[index]
        circle_r_frac: float = float(cfg["circle_radius_frac"])  # type: ignore[index]
        target_dark_factor: float = float(cfg["target_dark_factor"])  # type: ignore[index]
        star_th: int = int(cfg["star_thickness"])  # type: ignore[index]

        H, W = board.height, board.width
        H_px, W_px = H * cell_size, W * cell_size
        bg = self._get_rgb_background(H, W, cell_size, grid_color, grid_th, wall_color, wall_th)
        img = bg.copy()
        if self._rgb_internal_walls_overlay is None or self._rgb_internal_walls_overlay.shape[:2] != (H_px, W_px):
            self._build_rgb_internal_walls_overlay(board, cell_size_override=cell_size)
        if self._rgb_internal_walls_overlay is not None:
            overlay = self._rgb_internal_walls_overlay
            mask = (overlay.sum(axis=2) > 0)
            img[mask] = overlay[mask]

        # Inline draw helpers for robots/target
        def draw_hline(y: int, x0: int, x1: int, color: Tuple[int, int, int], thickness: int) -> None:
            y0 = max(0, y - thickness // 2)
            y1 = min(H_px, y + (thickness - thickness // 2))
            x0c = max(0, min(x0, x1))
            x1c = min(W_px, max(x0, x1))
            img[y0:y1, x0c:x1c] = color

        def draw_vline(x: int, y0: int, y1: int, color: Tuple[int, int, int], thickness: int) -> None:
            x0 = max(0, x - thickness // 2)
            x1 = min(W_px, x + (thickness - thickness // 2))
            y0c = max(0, min(y0, y1))
            y1c = min(H_px, max(y0, y1))
            img[y0c:y1c, x0:x1] = color

        def draw_circle(cx: float, cy: float, radius: float, color: Tuple[int, int, int], fill: bool) -> None:
            x0 = int(max(0, np.floor(cx - radius)))
            x1 = int(min(W_px - 1, np.ceil(cx + radius)))
            y0 = int(max(0, np.floor(cy - radius)))
            y1 = int(min(H_px - 1, np.ceil(cy + radius)))
            yy, xx = np.ogrid[y0:y1 + 1, x0:x1 + 1]
            mask = (xx - cx) ** 2 + (yy - cy) ** 2 <= radius ** 2
            if fill:
                img[y0:y1 + 1, x0:x1 + 1][mask] = color
            else:
                inner = (xx - cx) ** 2 + (yy - cy) ** 2 <= (radius - 1) ** 2
                ring = np.logical_and(mask, np.logical_not(inner))
                img[y0:y1 + 1, x0:x1 + 1][ring] = color

        def draw_star(cx: float, cy: float, arm_len: float, color: Tuple[int, int, int], thickness: int) -> None:
            draw_hline(int(cy), int(cx - arm_len), int(cx + arm_len), color, thickness)
            draw_vline(int(cx), int(cy - arm_len), int(cy + arm_len), color, thickness)
            steps = int(arm_len)
            for s in range(-steps, steps + 1):
                x = int(cx + s)
                y1 = int(cy + s)
                y2 = int(cy - s)
                y0a = max(0, y1 - thickness // 2)
                y1a = min(H_px, y1 + (thickness - thickness // 2))
                x0a = max(0, x - thickness // 2)
                x1a = min(W_px, x + (thickness - thickness // 2))
                img[y0a:y1a, x0a:x1a] = color
                y0b = max(0, y2 - thickness // 2)
                y1b = min(H_px, y2 + (thickness - thickness // 2))
                img[y0b:y1b, x0a:x1a] = color

        # Draw robots
        radius = cell_size * float(circle_r_frac)
        for rid, (rr, cc) in board.robot_positions.items():
            color = robot_colors[rid % len(robot_colors)]
            cy = rr * cell_size + cell_size * 0.5
            cx = cc * cell_size + cell_size * 0.5
            draw_circle(cx, cy, radius, color, circle_fill)

        # Draw target star
        tr = board.target_robot
        tr_color = robot_colors[tr % len(robot_colors)]
        dark_color = tuple(int(max(0, min(255, target_dark_factor * v))) for v in tr_color)
        gr, gc = board.goal_position
        cy = gr * cell_size + cell_size * 0.5
        cx = gc * cell_size + cell_size * 0.5
        draw_star(cx, cy, arm_len=cell_size * 0.35, color=dark_color, thickness=star_th)
        return img

    def _get_rgb_background(self, H: int, W: int, cell_size: int, grid_color: Tuple[int, int, int], grid_th: int, wall_color: Tuple[int, int, int], wall_th: int) -> np.ndarray:
        key = (int(H), int(W), int(cell_size), tuple(int(x) for x in grid_color), int(grid_th), tuple(int(x) for x in wall_color), int(wall_th))
        cached = RicochetRobotsEnv._RGB_BG_CACHE.get(key)
        if cached is not None:
            return cached
        H_px, W_px = int(H * cell_size), int(W * cell_size)
        img = np.ones((H_px, W_px, 3), dtype=np.uint8) * 255
        # Draw grid
        for r in range(H + 1):
            y = int(r * cell_size)
            y0 = max(0, y - grid_th // 2)
            y1 = min(H_px, y + (grid_th - grid_th // 2))
            img[y0:y1, 0:W_px] = grid_color
        for c in range(W + 1):
            x = int(c * cell_size)
            x0 = max(0, x - grid_th // 2)
            x1 = min(W_px, x + (grid_th - grid_th // 2))
            img[0:H_px, x0:x1] = grid_color
        # Draw boundary walls (top/bottom/left/right)
        # Horizontal top and bottom
        y = 0
        y0 = max(0, y - wall_th // 2)
        y1 = min(H_px, y + (wall_th - wall_th // 2))
        img[y0:y1, 0:W_px] = wall_color
        y = H_px
        y0 = max(0, y - wall_th // 2)
        y1 = min(H_px, y + (wall_th - wall_th // 2))
        img[y0:y1, 0:W_px] = wall_color
        # Vertical left and right
        x = 0
        x0 = max(0, x - wall_th // 2)
        x1 = min(W_px, x + (wall_th - wall_th // 2))
        img[0:H_px, x0:x1] = wall_color
        x = W_px
        x0 = max(0, x - wall_th // 2)
        x1 = min(W_px, x + (wall_th - wall_th // 2))
        img[0:H_px, x0:x1] = wall_color
        RicochetRobotsEnv._RGB_BG_CACHE[key] = img
        return img

    def _build_rgb_internal_walls_overlay(self, board: Board, cell_size_override: Optional[int] = None) -> None:
        cfg = self._render_rgb_cfg
        cell_size = int(cell_size_override if cell_size_override is not None else cfg["cell_size"])  # type: ignore[index]
        wall_color = tuple(int(x) for x in cfg["wall_color"])  # type: ignore[index]
        wall_th = int(cfg["wall_thickness"])  # type: ignore[index]
        H, W = board.height, board.width
        H_px, W_px = H * cell_size, W * cell_size
        overlay = np.zeros((H_px, W_px, 3), dtype=np.uint8)
        # Draw only interior walls (exclude boundaries at r=0,r=H,c=0,c=W)
        for r in range(1, H):
            for c in range(W):
                if board.h_walls[r, c]:
                    y = int(r * cell_size)
                    y0 = max(0, y - wall_th // 2)
                    y1 = min(H_px, y + (wall_th - wall_th // 2))
                    x0 = int(c * cell_size)
                    x1 = int((c + 1) * cell_size)
                    overlay[y0:y1, x0:x1] = wall_color
        for r in range(H):
            for c in range(1, W):
                if board.v_walls[r, c]:
                    x = int(c * cell_size)
                    x0 = max(0, x - wall_th // 2)
                    x1 = min(W_px, x + (wall_th - wall_th // 2))
                    y0 = int(r * cell_size)
                    y1 = int((r + 1) * cell_size)
                    overlay[y0:y1, x0:x1] = wall_color
        self._rgb_internal_walls_overlay = overlay


def generate_board_from_spec_with_seed(
    *,
    height: int,
    width: int,
    num_robots: int,
    edge_t_per_quadrant: int,
    central_l_per_quadrant: int,
    seed: int,
    _rng: Optional[np.random.Generator] = None,
) -> Board:
    """Generate a Board deterministically from spec and seed without constructing the env.

    If an RNG is provided, it will be used (and its state advanced deterministically);
    otherwise, a fresh Generator seeded with 'seed' is created.
    """
    h, w = int(height), int(width)
    assert h == w, "Boards should always be square per specification"
    rng = _rng if _rng is not None else np.random.default_rng(int(seed))

    # Initialize empty walls then set borders
    h_walls = np.zeros((h + 1, w), dtype=bool)
    v_walls = np.zeros((h, w + 1), dtype=bool)
    h_walls[0, :] = True
    h_walls[h, :] = True
    v_walls[:, 0] = True
    v_walls[:, w] = True

    reserved_cells: Set[Tuple[int, int]] = set()
    def reserve_cell(cell: Tuple[int, int]) -> None:
        r, c = cell
        if 0 <= r < h and 0 <= c < w:
            reserved_cells.add((r, c))

    # Forbidden central block
    if h % 2 == 1:
        mid = h // 2
        h_walls[mid, mid] = True
        h_walls[mid + 1, mid] = True
        v_walls[mid, mid] = True
        v_walls[mid, mid + 1] = True
        reserve_cell((mid, mid))
        central_rows = [mid]
        central_cols = [mid]
    else:
        mid = h // 2
        h_walls[mid-1, mid - 1] = True
        h_walls[mid-1, mid] = True
        h_walls[mid + 1, mid - 1] = True
        h_walls[mid + 1, mid] = True
        v_walls[mid - 1, mid-1] = True
        v_walls[mid, mid-1] = True
        v_walls[mid - 1, mid + 1] = True
        v_walls[mid, mid + 1] = True
        for rr in (mid - 1, mid):
            for cc in (mid - 1, mid):
                reserve_cell((rr, cc))
        central_rows = [mid - 1, mid]
        central_cols = [mid - 1, mid]

    def ranges_excluding_central(size: int, central_idxs: List[int]) -> Tuple[Tuple[int, int], Tuple[int, int]]:
        unique = sorted(set(central_idxs))
        if len(unique) == 1:
            mid_idx = unique[0]
            return (0, mid_idx - 1), (mid_idx + 1, size - 1)
        else:
            low_end = unique[0] - 1
            high_start = unique[1] + 1
            return (0, low_end), (high_start, size - 1)

    (r0_start, r0_end), (r1_start, r1_end) = ranges_excluding_central(h, central_rows)
    (c0_start, c0_end), (c1_start, c1_end) = ranges_excluding_central(w, central_cols)
    quadrants = {
        "NW": (r0_start, r0_end, c0_start, c0_end),
        "NE": (r0_start, r0_end, c1_start, c1_end),
        "SW": (r1_start, r1_end, c0_start, c0_end),
        "SE": (r1_start, r1_end, c1_start, c1_end),
    }

    used_interior_h: Set[Tuple[int, int]] = set()
    used_interior_v: Set[Tuple[int, int]] = set()

    def cell_has_nonboundary_wall(r: int, c: int) -> bool:
        if r > 0 and h_walls[r, c]:
            return True
        if r + 1 < h and h_walls[r + 1, c]:
            return True
        if c > 0 and v_walls[r, c]:
            return True
        if c + 1 < w and v_walls[r, c + 1]:
            return True
        return False

    def neighbors4(r: int, c: int) -> List[Tuple[int, int]]:
        nbrs: List[Tuple[int, int]] = []
        if r - 1 >= 0:
            nbrs.append((r - 1, c))
        if r + 1 < h:
            nbrs.append((r + 1, c))
        if c - 1 >= 0:
            nbrs.append((r, c - 1))
        if c + 1 < w:
            nbrs.append((r, c + 1))
        return nbrs

    def can_place_h(row: int, col: int) -> bool:
        if not (1 <= row <= h - 1 and 0 <= col <= w - 1):
            return False
        if h_walls[row, col]:
            return False
        neighbors = [
            (row - 1, col), (row + 1, col)
        ]
        for nr, nc in neighbors:
            if 1 <= nr <= h - 1 and 0 <= nc <= w - 1 and h_walls[nr, nc]:
                return False
        return True

    def can_place_v(row: int, col: int) -> bool:
        if not (0 <= row <= h - 1 and 1 <= col <= w - 1):
            return False
        if v_walls[row, col]:
            return False
        neighbors = [
            (row, col - 1), (row, col + 1)
        ]
        for nr, nc in neighbors:
            if 0 <= nr <= h - 1 and 1 <= nc <= w - 1 and v_walls[nr, nc]:
                return False
        return True

    def place_edge_t_in_quadrant(q_bounds: Tuple[int, int, int, int]) -> bool:
        rs, re, cs, ce = q_bounds
        if rs > re or cs > ce:
            return False
        choices: List[Tuple[str, Tuple[int, int]]] = []
        if rs == 0:
            for c in range(cs + 1, ce + 1):
                choices.append(("TOP", (0, c)))
        if re == h - 1:
            for c in range(cs + 1, ce + 1):
                choices.append(("BOTTOM", (h - 1, c)))
        if cs == 0:
            for r in range(rs + 1, re + 1):
                choices.append(("LEFT", (r, 0)))
        if ce == w - 1:
            for r in range(rs + 1, re + 1):
                choices.append(("RIGHT", (r, w - 1)))
        if not choices:
            return False
        idxs = np.arange(len(choices))
        rng.shuffle(idxs)
        for idx in idxs:
            side, pos = choices[idx]
            if side in ("TOP", "BOTTOM"):
                r0, c = pos
                if c in (1, w - 1):
                    continue
                if can_place_v(r0, c):
                    adj_cell = (1 if side == "TOP" else h - 2, c - 1)
                    blocked = any(cell_has_nonboundary_wall(rr, cc) for rr, cc in neighbors4(adj_cell[0], adj_cell[1]))
                    if not blocked and (0 <= adj_cell[0] < h and 0 <= adj_cell[1] < w) and adj_cell not in reserved_cells:
                        v_walls[r0, c] = True
                        used_interior_v.add((r0, c))
                        reserve_cell(adj_cell)
                        return True
            else:
                r, c0 = pos
                if r in (1, h - 1):
                    continue
                if can_place_h(r, c0):
                    adj_cell = (r - 1, 1 if side == "LEFT" else w - 2)
                    blocked = any(cell_has_nonboundary_wall(rr, cc) for rr, cc in neighbors4(adj_cell[0], adj_cell[1]))
                    if not blocked and (0 <= adj_cell[0] < h and 0 <= adj_cell[1] < w) and adj_cell not in reserved_cells:
                        h_walls[r, c0] = True
                        used_interior_h.add((r, c0))
                        reserve_cell(adj_cell)
                        return True
        return False

    orientations = ["TL", "TR", "BL", "BR"]
    central_l_centers: List[Tuple[int, int]] = []

    def place_central_l_in_quadrant(q_bounds: Tuple[int, int, int, int], desired_orientation: str) -> bool:
        rs, re, cs, ce = q_bounds
        if rs > re or cs > ce:
            return False
        candidates: List[Tuple[int, int]] = []
        for r in range(rs, re + 1):
            for c in range(cs, ce + 1):
                if r == 0 or r == h - 1 or c == 0 or c == w - 1:
                    continue
                if (r, c) in reserved_cells:
                    continue
                if any(cell_has_nonboundary_wall(rr, cc) for rr, cc in neighbors4(r, c)):
                    continue
                candidates.append((r, c))
        if not candidates:
            return False
        idxs = np.arange(len(candidates))
        rng.shuffle(idxs)
        for idx in idxs:
            r, c = candidates[idx]
            need_h: Optional[Tuple[int, int]] = None
            need_v: Optional[Tuple[int, int]] = None
            if desired_orientation == "TL":
                need_h = (r, c)
                need_v = (r, c)
            elif desired_orientation == "TR":
                need_h = (r, c)
                need_v = (r, c + 1)
            elif desired_orientation == "BL":
                need_h = (r + 1, c)
                need_v = (r, c)
            else:  # BR
                need_h = (r + 1, c)
                need_v = (r, c + 1)
            if need_h is None or need_v is None:
                continue
            hr, hc = need_h
            vr, vc = need_v
            if not can_place_h(hr, hc):
                continue
            if not can_place_v(vr, vc):
                continue
            if hr in (0, h) or vc in (0, w):
                continue
            h_walls[hr, hc] = True
            v_walls[vr, vc] = True
            used_interior_h.add((hr, hc))
            used_interior_v.add((vr, vc))
            reserve_cell((r, c))
            central_l_centers.append((r, c))
            return True
        return False

    # 1) Edge-T per quadrant
    for _, bounds in quadrants.items():
        count = int(edge_t_per_quadrant)
        placed = 0
        attempts = 0
        while placed < count and attempts < count * 10:
            if place_edge_t_in_quadrant(bounds):
                placed += 1
            attempts += 1

    # 2) Central-L per quadrant with even orientation distribution
    for _, bounds in quadrants.items():
        count = int(central_l_per_quadrant)
        if count <= 0:
            continue
        orient_list: List[str] = []
        for i in range(count):
            orient_list.append(orientations[i % 4])
        idxs = np.arange(len(orient_list))
        rng.shuffle(idxs)
        orient_list = [orient_list[i] for i in idxs]
        placed = 0
        attempts = 0
        i = 0
        while placed < count and attempts < count * 20 and i < len(orient_list):
            if place_central_l_in_quadrant(bounds, orient_list[i]):
                placed += 1
            i += 1
            attempts += 1

    # Goal inside central-L
    central_set = set((r, c) for r in central_rows for c in central_cols)
    candidates_goal = [rc for rc in central_l_centers if rc not in central_set]
    if not candidates_goal:
        candidates_goal = [(r, c) for r in range(h) for c in range(w) if (r, c) not in central_set]
    idx = int(rng.integers(0, len(candidates_goal)))
    goal = candidates_goal[idx]

    # Robots
    free_cells: List[Tuple[int, int]] = []
    for r in range(h):
        for c in range(w):
            if (r, c) in central_set or (r, c) == goal:
                continue
            free_cells.append((r, c))
    rng.shuffle(free_cells)
    robot_positions: Dict[int, Tuple[int, int]] = {}
    for rid in range(int(num_robots)):
        robot_positions[rid] = free_cells.pop()
    target_robot = int(rng.integers(0, int(num_robots)))
    return Board(height=h, width=w, h_walls=h_walls, v_walls=v_walls, robot_positions=robot_positions, goal_position=goal, target_robot=target_robot)


# Optional helper for registration with gymnasium
def register_env(env_id: str = "RicochetRobots-v0", *, max_episode_steps: Optional[int] = None) -> None:
    """Register this environment with gymnasium so it can be created via gym.make(env_id).

    Note: This env already enforces a time limit via its max_steps argument. If you also set
    max_episode_steps here and wrap with Gym's TimeLimit implicitly via gym.make, you may see
    duplicate truncation behavior. Prefer one source of time limits.
    """
    try:
        from gymnasium.envs.registration import register  # type: ignore
        from gymnasium.error import Error as GymError  # type: ignore
        try:
            kwargs = {}
            if max_episode_steps is not None:
                kwargs["max_episode_steps"] = int(max_episode_steps)
            register(
                id=env_id,
                entry_point="env.ricochet_env:RicochetRobotsEnv",
                **kwargs,
            )
        except GymError as e:  # pragma: no cover - depends on gym version behavior
            # Swallow duplicate registration errors; re-raise others
            if "already registered" in str(e).lower():
                return
            raise
    except (ModuleNotFoundError, ImportError):
        # Silently ignore if gymnasium not available
        pass

```

I updated the RGB compositing to use a mask-based overwrite so internal walls draw over the background instead of being lost with np.maximum. Let me run a quick lint check on the modified file.

Done. Internal walls should now be visible in both rendered RGB frames and `rgb_image` observations.

- Change: compositing now uses a per-pixel mask to overwrite background with the internal walls overlay, instead of np.maximum.
- Files touched: `env/ricochet_env.py`

You can re-run `initial_bank_preview_cells.py` to confirm the edge-T and central-L structures show up.

---

**User**

I still don't see the (internal) walls

---

**Cursor**

```python
from __future__ import annotations

from dataclasses import dataclass
from typing import Dict, Optional, Tuple, List, Literal, Set

import numpy as np

from .ricochet_core import Board, apply_action, reached_goal

# Provide a runtime gymnasium binding if available, otherwise a minimal fallback
try:  # pragma: no cover
    import importlib
    _gym = importlib.import_module("gymnasium")  # type: ignore
    _spaces = importlib.import_module("gymnasium.spaces")  # type: ignore
    _seeding = importlib.import_module("gymnasium.utils.seeding")  # type: ignore
    _error = importlib.import_module("gymnasium.error")  # type: ignore
    GymEnvBase = _gym.Env  # type: ignore
    spaces = _spaces  # type: ignore
    GymInvalidAction = getattr(_error, "InvalidAction", None)  # type: ignore
except (ModuleNotFoundError, ImportError):  # pragma: no cover
    class GymEnvBase(object):
        pass
    class _SpacesDummy:  # minimal placeholders to satisfy type usage
        class Box:  # type: ignore
            def __init__(self, low=None, high=None, shape=None, dtype=None):
                self.low = low
                self.high = high
                self.shape = shape
                self.dtype = dtype
        class Discrete:  # type: ignore
            def __init__(self, n: int):
                self.n = n
    spaces = _SpacesDummy()  # type: ignore
    GymInvalidAction = None  # type: ignore


@dataclass
class FixedLayout:
    height: int
    width: int
    # Edge walls
    h_walls: np.ndarray  # (H+1, W) bool
    v_walls: np.ndarray  # (H, W+1) bool
    robot_positions: Dict[int, Tuple[int, int]]
    goal_position: Tuple[int, int]
    target_robot: int


ObsMode = Literal["image", "symbolic", "rgb_image"]


class RicochetRobotsEnv(GymEnvBase):
    """Ricochet Robots Gymnasium-compatible environment.

    Determinism and seeding:
    - Deterministic given the seed provided to __init__ or reset(seed=...).
    - Randomness flows through self.np_random (Gymnasium seeding utils); the episode's resolved
      seed is exposed via info["episode_seed"].
    - When ensure_solvable=True, the environment may retry board generation; retries do not add
      extra non-determinism beyond RNG draws. The included solver is deterministic.

    Observations:
    - Image mode: float32 in [0,1]. Channels are named by channel_names; note that the target robot
      is represented both by a dedicated mask (index 5) and its robot channel (duplication by design).
    - Symbolic mode: float32 with absolute grid indices (not normalized). Downstream code may wish to
      normalize to [0,1]. The Box low/high reflect board bounds.

    Rendering:
    - Only render_mode="ascii" is implemented and returns a string frame.
    - Other modes are not implemented and will raise a clear error.
    """
    metadata = {"render_modes": ["ascii", "rgb"], "render_fps": 4}
    # Class-level cache for RGB backgrounds per (H, W, cell_size, grid_color, grid_th, wall_color, wall_th)
    _RGB_BG_CACHE: Dict[Tuple[int, int, int, Tuple[int, int, int], int, Tuple[int, int, int], int], np.ndarray] = {}

    def __init__(
        self,
        height: int = 8,
        width: int = 8,
        num_robots: int = 2,
        include_noop: bool = True,
        step_penalty: float = -0.01,
        goal_reward: float = 1.0,
        noop_penalty: Optional[float] = None,
        max_steps: int = 10,
        fixed_layout: Optional[FixedLayout] = None,
        seed: Optional[int] = None,
        ensure_solvable: bool = False,
        solver_max_depth: int = 30,
        solver_max_nodes: int = 20000,
        obs_mode: ObsMode = "image",
        channels_first: bool = False,
        render_mode: Optional[str] = None,
        ensure_attempt_limit: int = 500,
        # Spec-driven generation configuration
        edge_t_per_quadrant: Optional[int] = None,
        central_l_per_quadrant: Optional[int] = None,
        # RGB render configuration
        render_rgb_config: Optional[Dict[str, object]] = None,
    ) -> None:
        super().__init__()
        self.height = height
        self.width = width
        self.num_robots = num_robots
        self.include_noop = include_noop
        self.step_penalty = step_penalty
        self.goal_reward = goal_reward
        self.noop_penalty = step_penalty if noop_penalty is None else noop_penalty
        self.max_steps = max_steps
        self.fixed_layout = fixed_layout
        self.ensure_solvable = ensure_solvable
        self.solver_max_depth = solver_max_depth
        self.solver_max_nodes = solver_max_nodes
        self.obs_mode: ObsMode = obs_mode
        self.channels_first = channels_first
        # If a fixed layout is provided, align env geometry before building spaces
        if self.fixed_layout is not None:
            self.height = int(self.fixed_layout.height)
            self.width = int(self.fixed_layout.width)
            self.num_robots = int(len(self.fixed_layout.robot_positions))
        # Render API
        self.render_mode = render_mode
        if self.render_mode is not None and self.render_mode not in self.metadata["render_modes"]:
            raise ValueError(f"Unsupported render_mode {self.render_mode}. Supported: {self.metadata['render_modes']}")
        # RGB render defaults
        self._render_rgb_cfg: Dict[str, object] = {
            "cell_size": 20,
            "grid_color": (200, 200, 200),
            "grid_thickness": 1,
            "wall_color": (0, 0, 0),
            "wall_thickness": 3,
            "robot_colors": [
                (220, 50, 50),     # red
                (50, 120, 220),    # blue
                (50, 180, 80),     # green
                (230, 200, 40),    # yellow
                (180, 80, 180),    # purple (extra if more robots)
            ],
            "circle_fill": True,
            "circle_radius_frac": 0.35,
            "target_dark_factor": 0.6,
            "star_thickness": 2,
        }
        if render_rgb_config is not None:
            self._render_rgb_cfg.update(render_rgb_config)
        # Per-instance cache: RGB internal walls overlay built for current layout (excludes boundary walls)
        self._rgb_internal_walls_overlay: Optional[np.ndarray] = None

        # Seeding: adopt Gymnasium convention with self.np_random
        self.np_random: np.random.Generator
        self._episode_seed: Optional[int] = None
        try:  # gymnasium present
            # _seeding.np_random returns (rng, seed)
            self.np_random, used_seed = _seeding.np_random(seed)  # type: ignore[name-defined]
            self._episode_seed = int(used_seed) if used_seed is not None else None
        except NameError:
            self.np_random = np.random.default_rng(seed)
            self._episode_seed = int(seed) if seed is not None else None
        # Preserve a base seed to enforce deterministic resets when no new seed is provided
        self._base_seed: Optional[int] = int(seed) if seed is not None else self._episode_seed
        self._board: Optional[Board] = None
        self._num_steps = 0
        self._cached_wall_obs: Optional[np.ndarray] = None  # (H,W,4) float32 for wall channels
        self._ensure_attempt_limit = int(ensure_attempt_limit)
        # Spec default counts per quadrant (based on smaller board side)
        side_min = min(self.height, self.width)
        default_per_quad = 2 if side_min < 10 else 4
        self._edge_t_per_quadrant = default_per_quad if edge_t_per_quadrant is None else int(edge_t_per_quadrant)
        self._central_l_per_quadrant = default_per_quad if central_l_per_quadrant is None else int(central_l_per_quadrant)
        # Track the exact per-episode board seed used to generate the current board
        self._last_board_seed: Optional[int] = None

        # Observation spaces
        if self.obs_mode == "image":
            # channels: up_wall, down_wall, left_wall, right_wall, goal, target_mask, robots...
            self._base_wall_channels = 4
            self._base_misc_channels = 2  # goal, target
            self._num_channels = self._base_wall_channels + self._base_misc_channels + self.num_robots
            if self.channels_first:
                obs_shape = (self._num_channels, self.height, self.width)
            else:
                obs_shape = (self.height, self.width, self._num_channels)
            self.observation_space = spaces.Box(
                low=0.0,
                high=1.0,
                shape=obs_shape,
                dtype=np.float32,
            )
        elif self.obs_mode == "rgb_image":
            # RGB image observation: uses fixed pixel dimensions for consistent observation space
            # This ensures all curriculum levels have the same observation space size
            # Lowered from 128 to 96 to reduce memory/compute; resize handled in _board_to_rgb_obs
            FIXED_PIXEL_SIZE = 256
            
            if self.channels_first:
                obs_shape = (3, FIXED_PIXEL_SIZE, FIXED_PIXEL_SIZE)
            else:
                obs_shape = (FIXED_PIXEL_SIZE, FIXED_PIXEL_SIZE, 3)
            self.observation_space = spaces.Box(
                low=0,
                high=255,
                shape=obs_shape,
                dtype=np.uint8,
            )
        elif self.obs_mode == "symbolic":
            # [goal_r, goal_c] in [0..H-1], [0..W-1]; one-hot target in [0,1]; robots positions in bounds
            vec_len = 2 + self.num_robots + (self.num_robots * 2)
            low = np.full((vec_len,), 0.0, dtype=np.float32)
            high = np.full((vec_len,), 1.0, dtype=np.float32)
            # goal bounds
            low[0] = 0.0
            high[0] = float(self.height - 1)
            low[1] = 0.0
            high[1] = float(self.width - 1)
            # target one-hot already [0,1]
            # robots positions
            base = 2 + self.num_robots
            for i in range(self.num_robots):
                low[base + 2 * i] = 0.0
                high[base + 2 * i] = float(self.height - 1)
                low[base + 2 * i + 1] = 0.0
                high[base + 2 * i + 1] = float(self.width - 1)
            self.observation_space = spaces.Box(low=low, high=high, shape=(vec_len,), dtype=np.float32)
        else:
            raise ValueError(f"Unknown obs_mode: {self.obs_mode}")

        # Action space: num_robots * 4 directions (+ 1 for noop)
        self._noop_action = self.num_robots * 4 if include_noop else None
        self.action_space = spaces.Discrete(self.num_robots * 4 + (1 if include_noop else 0))

    # Gymnasium API
    def reset(self, *, seed: Optional[int] = None, options: Optional[dict] = None):
        """Reset the environment and start a new episode.

        Returns (obs, info) per Gymnasium API. info contains keys: 'target_robot',
        'level_solvable' (optional), 'optimal_length' (if ensured), 'solver_limits' (if ensured),
        and 'episode_seed' with the seed used for this episode.
        """
        # Import profiling here to avoid circular imports
        try:
            from profiling import profile
        except ImportError:
            # Fallback if profiling not available
            def profile(name, track_memory=True):
                from contextlib import nullcontext
                return nullcontext()

        # Standard seeding protocol
        if seed is not None:
            try:
                self.np_random, used_seed = _seeding.np_random(seed)  # type: ignore[name-defined]
                self._episode_seed = int(used_seed) if used_seed is not None else None
            except NameError:
                self.np_random = np.random.default_rng(seed)
                self._episode_seed = int(seed)
            self._base_seed = int(self._episode_seed) if self._episode_seed is not None else None
        else:
            # Re-seed with base seed so that resets are deterministic when no new seed is provided
            if self._base_seed is not None:
                try:
                    self.np_random, used_seed = _seeding.np_random(self._base_seed)  # type: ignore[name-defined]
                    self._episode_seed = int(used_seed) if used_seed is not None else int(self._base_seed)
                except NameError:
                    self.np_random = np.random.default_rng(self._base_seed)
                    self._episode_seed = int(self._base_seed)
        _ = options  # silence unused
        self._num_steps = 0
        # clear cached walls in case layout changes
        self._cached_wall_obs = None

        if self.fixed_layout is not None:
            with profile("env_reset_fixed_layout", track_memory=True):
                self._board = Board(
                height=self.fixed_layout.height,
                width=self.fixed_layout.width,
                h_walls=self.fixed_layout.h_walls.copy(),
                v_walls=self.fixed_layout.v_walls.copy(),
                robot_positions=dict(self.fixed_layout.robot_positions),
                goal_position=self.fixed_layout.goal_position,
                target_robot=self.fixed_layout.target_robot,
            )
            # cache static walls for this layout
            with profile("env_build_cached_walls", track_memory=True):
                self._build_cached_walls(self._board)
            # Build per-layout internal walls RGB overlay cache (for rgb render mode)
            try:
                if self.render_mode == "rgb":
                    self._build_rgb_internal_walls_overlay(self._board)
            except Exception:
                self._rgb_internal_walls_overlay = None
            with profile("env_make_obs", track_memory=True):
                obs = self._make_obs(self._board)
            info = {
                "level_solvable": False,
                "ensure_solvable_enabled": False,
                "target_robot": self._board.target_robot,
                "episode_seed": self._episode_seed,
            }
            if self.obs_mode == "image":
                info["channel_names"] = self.channel_names
            return obs, info

        # Otherwise, generate random board, optionally ensure solvable via solver
        attempts = 0
        while True:
            attempts += 1
            with profile("env_generate_board", track_memory=True):
                board = self._generate_random_board()
            if not self.ensure_solvable:
                self._board = board
                break
            try:
                from .solver import solve_bfs  # type: ignore
                with profile("env_solver_check", track_memory=True):
                    solution = solve_bfs(board, max_depth=self.solver_max_depth, max_nodes=self.solver_max_nodes)
                if solution is not None:
                    self._board = board
                    optimal_length = len(solution)
                    break
            except ImportError as exc:
                raise RuntimeError("ensure_solvable=True but solver unavailable (ImportError)") from exc
            if attempts >= self._ensure_attempt_limit:
                raise RuntimeError("Failed to generate solvable board within attempt limit")

        # cache static walls for this layout
        assert self._board is not None
        with profile("env_build_cached_walls", track_memory=True):
            self._build_cached_walls(self._board)
        # Build per-layout internal walls RGB overlay cache (for rgb render mode)
        try:
            if self.render_mode == "rgb":
                self._build_rgb_internal_walls_overlay(self._board)
        except Exception:
            self._rgb_internal_walls_overlay = None
        with profile("env_make_obs", track_memory=True):
            obs = self._make_obs(self._board)
        # Note: solver None during attempts may be due to cutoffs (depth/nodes), not true unsolvability.
        info = {
            "level_solvable": bool(self.ensure_solvable),
            "ensure_solvable_enabled": bool(self.ensure_solvable),
            "target_robot": self._board.target_robot,
            "episode_seed": self._episode_seed,
        }
        # Expose the exact per-episode board seed used to build this board
        if hasattr(self, "_last_board_seed") and self._last_board_seed is not None:
            info["board_seed"] = int(self._last_board_seed)
        if self.obs_mode == "image":
            info["channel_names"] = self.channel_names
        if self.ensure_solvable:
            info["optimal_length"] = optimal_length
            info["solver_limits"] = {"max_depth": self.solver_max_depth, "max_nodes": self.solver_max_nodes}
        return obs, info

    def step(self, action: int):
        """Apply an action and return (obs, reward, terminated, truncated, info).

        info always includes 'steps', 'target_robot', and 'is_success' (bool). When truncated due to
        max steps, info includes 'TimeLimit.truncated'=True.
        """
        # Import profiling here to avoid circular imports
        try:
            from profiling import profile
        except ImportError:
            # Fallback if profiling not available
            def profile(name, track_memory=True):
                from contextlib import nullcontext
                return nullcontext()

        assert self._board is not None, "Call reset() first"
        self._num_steps += 1

        terminated = False
        truncated = False
        info: Dict[str, object] = {}

        reward = self.step_penalty
        # Validate action range early
        if not (0 <= action < self.action_space.n):
            if GymInvalidAction is not None:
                raise GymInvalidAction(f"Invalid action {action}")  # type: ignore[misc]
            raise ValueError(f"Invalid action {action}")

        if self.include_noop and action == self._noop_action:
            reward = self.noop_penalty
        else:
            with profile("env_apply_action", track_memory=True):
                robot_id, direction = self._decode_action(action)
                self._board = apply_action(self._board, robot_id, direction)

        if reached_goal(self._board):
            reward += self.goal_reward
            terminated = True
            info["is_success"] = True

        if not terminated and self._num_steps >= self.max_steps:
            truncated = True
            info["TimeLimit.truncated"] = True

        with profile("env_make_obs", track_memory=True):
            obs = self._make_obs(self._board)
        info["steps"] = self._num_steps
        info["target_robot"] = self._board.target_robot
        if "is_success" not in info:
            info["is_success"] = False
        return obs, reward, terminated, truncated, info

    def render(self):
        """Render according to render_mode.

        - render_mode=="ascii": returns a string frame.
        - render_mode=="rgb": returns an (H_px, W_px, 3) uint8 array.
        - render_mode is None or unsupported: returns None.
        """
        if self.render_mode is None:
            return None
        if self._board is None:
            return "" if self.render_mode == "ascii" else None
        if self.render_mode == "ascii":
            from .ricochet_core import render_ascii  # local import to avoid circulars in lints
            return render_ascii(self._board)
        if self.render_mode == "rgb":
            return self._render_rgb(self._board)
        if self._board is None:
            return ""
        raise ValueError(f"Unsupported render_mode {self.render_mode}. Supported: {self.metadata['render_modes']}")

    def close(self) -> None:
        """Close environment resources (no-op)."""
        return None

    def seed(self, seed: Optional[int] = None):  # legacy convenience
        """Legacy seeding helper returning [seed]. Prefer reset(seed=...)."""
        if seed is None:
            return [self._episode_seed]
        try:
            self.np_random, used_seed = _seeding.np_random(seed)  # type: ignore[name-defined]
            self._episode_seed = int(used_seed) if used_seed is not None else int(seed)
        except NameError:
            self.np_random = np.random.default_rng(seed)
            self._episode_seed = int(seed)
        return [self._episode_seed]

    # Helpers
    def _decode_action(self, action: int) -> Tuple[int, int]:
        if not (0 <= action < self.action_space.n):
            raise ValueError(f"Invalid action {action}")
        if self.include_noop and action == self._noop_action:
            raise ValueError("_decode_action called on noop action")
        robot_id = action // 4
        direction = action % 4
        if robot_id >= self.num_robots:
            raise ValueError(f"Invalid robot_id {robot_id} for action {action}")
        return robot_id, direction

    def _make_obs(self, board: Board) -> np.ndarray:
        if self.obs_mode == "image":
            return self._board_to_image_obs(board)
        elif self.obs_mode == "rgb_image":
            return self._board_to_rgb_obs(board)
        return self._board_to_symbolic_obs(board)

    def _board_to_image_obs(self, board: Board) -> np.ndarray:
        # Import profiling here to avoid circular imports
        try:
            from profiling import profile
        except ImportError:
            # Fallback if profiling not available
            def profile(name, track_memory=True):
                from contextlib import nullcontext
                return nullcontext()

        with profile("env_image_observation_generation", track_memory=True):
            h, w = board.height, board.width
            C = self._num_channels
            obs = np.zeros((h, w, C), dtype=np.float32)
            # wall channels 0..3 from cache
            if self._cached_wall_obs is not None:
                obs[:, :, 0:4] = self._cached_wall_obs
            else:
                # Fallback if cache not built (should not happen)
                for r in range(h):
                    for c in range(w):
                        obs[r, c, 0] = 1.0 if board.has_wall_up(r, c) else 0.0
                        obs[r, c, 1] = 1.0 if board.has_wall_down(r, c) else 0.0
                        obs[r, c, 2] = 1.0 if board.has_wall_left(r, c) else 0.0
                        obs[r, c, 3] = 1.0 if board.has_wall_right(r, c) else 0.0
            # goal channel 4
            gr, gc = board.goal_position
            obs[gr, gc, 4] = 1.0
            # target mask channel 5
            tr = board.target_robot
            tr_r, tr_c = board.robot_positions[tr]
            obs[tr_r, tr_c, 5] = 1.0
            # robot channels start at 6
            base = 6
            for rid, (r, c) in board.robot_positions.items():
                obs[r, c, base + rid] = 1.0
            if self.channels_first:
                obs = np.transpose(obs, (2, 0, 1))
            return obs

    def _board_to_symbolic_obs(self, board: Board) -> np.ndarray:
        # Import profiling here to avoid circular imports
        try:
            from profiling import profile
        except ImportError:
            # Fallback if profiling not available
            def profile(name, track_memory=True):
                from contextlib import nullcontext
                return nullcontext()

        with profile("env_symbolic_observation_generation", track_memory=True):
            gr, gc = board.goal_position
            target_one_hot = np.zeros((self.num_robots,), dtype=np.float32)
            target_one_hot[board.target_robot] = 1.0
            robots_vec: List[float] = []
            for rid in range(self.num_robots):
                r, c = board.robot_positions[rid]
                robots_vec.extend([float(r), float(c)])
            vec = np.concatenate([
                np.array([float(gr), float(gc)], dtype=np.float32),
                target_one_hot.astype(np.float32),
                np.array(robots_vec, dtype=np.float32),
            ])
            return vec

    def _board_to_rgb_obs(self, board: Board) -> np.ndarray:
        """Convert board to RGB image observation with fixed pixel dimensions.
        
        This creates an RGB image with fixed pixel dimensions (128x128) by adjusting
        the cell size to fit the board within the fixed pixel space. This ensures
        consistent observation space across all curriculum levels.
        """
        # Import profiling here to avoid circular imports
        try:
            from profiling import profile
        except ImportError:
            # Fallback if profiling not available
            def profile(name, track_memory=True):
                from contextlib import nullcontext
                return nullcontext()
        
        with profile("env_rgb_observation_generation", track_memory=True):
            FIXED_PIXEL_SIZE = 256
            # Padding around the board within the fixed output
            padding = 4
            available_size = FIXED_PIXEL_SIZE - 2 * padding
            cell_size = max(1, available_size // max(board.height, board.width))
            # Compute board pixel dims and canvas
            H_px = int(board.height * cell_size)
            W_px = int(board.width * cell_size)
            # Compose into fixed-size canvas without resizing
            canvas = np.ones((FIXED_PIXEL_SIZE, FIXED_PIXEL_SIZE, 3), dtype=np.uint8) * 255
            y0 = padding + (available_size - H_px) // 2
            x0 = padding + (available_size - W_px) // 2
            # Render board region with caching
            with profile("env_rgb_rendering_cached", track_memory=True):
                region = self._render_rgb_cached(board, cell_size)
            canvas[y0:y0+H_px, x0:x0+W_px] = region
            if self.channels_first:
                canvas = np.transpose(canvas, (2, 0, 1))
            return canvas

    def _generate_random_board(self) -> Board:
        # Draw a per-episode board seed from the env RNG
        seed_used = int(self.np_random.integers(0, 2**31 - 1))
        # Record it for reproducibility/debugging
        self._last_board_seed = seed_used
        # Generate deterministically from the explicit seed only (no external RNG)
        return generate_board_from_spec_with_seed(
            height=self.height,
            width=self.width,
            num_robots=self.num_robots,
            edge_t_per_quadrant=self._edge_t_per_quadrant,
            central_l_per_quadrant=self._central_l_per_quadrant,
            seed=seed_used,
        )

    # Public channel names for image observations
    @property
    def channel_names(self) -> List[str]:
        """Human-readable names for image observation channels."""
        names = ["wall_up", "wall_down", "wall_left", "wall_right", "goal", "target_robot_mask"]
        names.extend([f"robot_{i}" for i in range(self.num_robots)])
        return names

    def _build_cached_walls(self, board: Board) -> None:
        """Precompute static wall indicator channels (H,W,4)."""
        h, w = board.height, board.width
        wall = np.zeros((h, w, 4), dtype=np.float32)
        for r in range(h):
            for c in range(w):
                wall[r, c, 0] = 1.0 if board.has_wall_up(r, c) else 0.0
                wall[r, c, 1] = 1.0 if board.has_wall_down(r, c) else 0.0
                wall[r, c, 2] = 1.0 if board.has_wall_left(r, c) else 0.0
                wall[r, c, 3] = 1.0 if board.has_wall_right(r, c) else 0.0
        self._cached_wall_obs = wall

    # Public accessor for current board (read-only clone)
    def get_board(self) -> Board:
        if self._board is None:
            raise RuntimeError("Environment not initialized. Call reset() first.")
        return self._board.clone()

    def _render_rgb(self, board: Board) -> np.ndarray:
        # Import profiling here to avoid circular imports
        try:
            from profiling import profile
        except ImportError:
            # Fallback if profiling not available
            def profile(name, track_memory=True):
                from contextlib import nullcontext
                return nullcontext()
        
        with profile("env_rgb_render_setup", track_memory=True):
            cfg = self._render_rgb_cfg
            cell_size = int(cfg["cell_size"])  # pixels
            grid_color = tuple(int(x) for x in cfg["grid_color"])  # type: ignore[index]
            grid_th = int(cfg["grid_thickness"])  # px
            wall_color = tuple(int(x) for x in cfg["wall_color"])  # type: ignore[index]
            wall_th = int(cfg["wall_thickness"])  # px
            robot_colors: List[Tuple[int, int, int]] = [tuple(int(a) for a in t) for t in cfg["robot_colors"]]  # type: ignore[index]
            circle_fill: bool = bool(cfg["circle_fill"])  # type: ignore[index]
            circle_r_frac: float = float(cfg["circle_radius_frac"])  # type: ignore[index]
            target_dark_factor: float = float(cfg["target_dark_factor"])  # type: ignore[index]
            star_th: int = int(cfg["star_thickness"])  # type: ignore[index]

            H, W = board.height, board.width
            H_px, W_px = H * cell_size, W * cell_size
            # Start from cached background with grid+boundary walls
            bg = self._get_rgb_background(H, W, cell_size, grid_color, grid_th, wall_color, wall_th)
            img = bg.copy()

        # Helpers to draw
        def draw_hline(y: int, x0: int, x1: int, color: Tuple[int, int, int], thickness: int) -> None:
            y0 = max(0, y - thickness // 2)
            y1 = min(H_px, y + (thickness - thickness // 2))
            x0c = max(0, min(x0, x1))
            x1c = min(W_px, max(x0, x1))
            img[y0:y1, x0c:x1c] = color

        def draw_vline(x: int, y0: int, y1: int, color: Tuple[int, int, int], thickness: int) -> None:
            x0 = max(0, x - thickness // 2)
            x1 = min(W_px, x + (thickness - thickness // 2))
            y0c = max(0, min(y0, y1))
            y1c = min(H_px, max(y0, y1))
            img[y0c:y1c, x0:x1] = color

        def draw_circle(cx: float, cy: float, radius: float, color: Tuple[int, int, int], fill: bool) -> None:
            # Bounding box
            x0 = int(max(0, np.floor(cx - radius)))
            x1 = int(min(W_px - 1, np.ceil(cx + radius)))
            y0 = int(max(0, np.floor(cy - radius)))
            y1 = int(min(H_px - 1, np.ceil(cy + radius)))
            yy, xx = np.ogrid[y0:y1 + 1, x0:x1 + 1]
            mask = (xx - cx) ** 2 + (yy - cy) ** 2 <= radius ** 2
            if fill:
                img[y0:y1 + 1, x0:x1 + 1][mask] = color
            else:
                # outline 1px
                inner = (xx - cx) ** 2 + (yy - cy) ** 2 <= (radius - 1) ** 2
                ring = np.logical_and(mask, np.logical_not(inner))
                img[y0:y1 + 1, x0:x1 + 1][ring] = color

        def draw_star(cx: float, cy: float, arm_len: float, color: Tuple[int, int, int], thickness: int) -> None:
            # Draw + arms
            draw_hline(int(cy), int(cx - arm_len), int(cx + arm_len), color, thickness)
            draw_vline(int(cx), int(cy - arm_len), int(cy + arm_len), color, thickness)
            # Draw x arms (diagonals) by sampling small line thickness via plotting short lines of squares
            # Approximate diagonals by drawing small rectangles along the diagonal
            steps = int(arm_len)
            for s in range(-steps, steps + 1):
                x = int(cx + s)
                y1 = int(cy + s)
                y2 = int(cy - s)
                y0a = max(0, y1 - thickness // 2)
                y1a = min(H_px, y1 + (thickness - thickness // 2))
                x0a = max(0, x - thickness // 2)
                x1a = min(W_px, x + (thickness - thickness // 2))
                img[y0a:y1a, x0a:x1a] = color
                y0b = max(0, y2 - thickness // 2)
                y1b = min(H_px, y2 + (thickness - thickness // 2))
                img[y0b:y1b, x0a:x1a] = color

        # 1) Internal walls overlay (excluding boundary walls) drawn once per layout
        with profile("env_rgb_draw_internal_walls", track_memory=True):
            if self._rgb_internal_walls_overlay is None:
                self._build_rgb_internal_walls_overlay(board)
            if self._rgb_internal_walls_overlay is not None:
                overlay = self._rgb_internal_walls_overlay
                mask = (overlay.sum(axis=2) < 3 * 255)
                img[mask] = overlay[mask]

        # 3) Draw robots as filled circles
        with profile("env_rgb_draw_robots", track_memory=True):
            radius = cell_size * float(circle_r_frac)
            for rid, (rr, cc) in board.robot_positions.items():
                color = robot_colors[rid % len(robot_colors)]
                cy = rr * cell_size + cell_size * 0.5
                cx = cc * cell_size + cell_size * 0.5
                draw_circle(cx, cy, radius, color, circle_fill)

        # 4) Draw target as a star in darker color of target robot
        with profile("env_rgb_draw_target", track_memory=True):
            tr = board.target_robot
            tr_color = robot_colors[tr % len(robot_colors)]
            dark_color = tuple(int(max(0, min(255, target_dark_factor * v))) for v in tr_color)
            gr, gc = board.goal_position
            cy = gr * cell_size + cell_size * 0.5
            cx = gc * cell_size + cell_size * 0.5
            draw_star(cx, cy, arm_len=cell_size * 0.35, color=dark_color, thickness=star_th)

        return img

    def _render_rgb_cached(self, board: Board, cell_size: int) -> np.ndarray:
        # Compose cached background + internal walls overlay + dynamic robots/target into board-sized image
        cfg = self._render_rgb_cfg
        grid_color = tuple(int(x) for x in cfg["grid_color"])  # type: ignore[index]
        grid_th = int(cfg["grid_thickness"])  # px
        wall_color = tuple(int(x) for x in cfg["wall_color"])  # type: ignore[index]
        wall_th = int(cfg["wall_thickness"])  # px
        robot_colors: List[Tuple[int, int, int]] = [tuple(int(a) for a in t) for t in cfg["robot_colors"]]  # type: ignore[index]
        circle_fill: bool = bool(cfg["circle_fill"])  # type: ignore[index]
        circle_r_frac: float = float(cfg["circle_radius_frac"])  # type: ignore[index]
        target_dark_factor: float = float(cfg["target_dark_factor"])  # type: ignore[index]
        star_th: int = int(cfg["star_thickness"])  # type: ignore[index]

        H, W = board.height, board.width
        H_px, W_px = H * cell_size, W * cell_size
        bg = self._get_rgb_background(H, W, cell_size, grid_color, grid_th, wall_color, wall_th)
        img = bg.copy()
        if self._rgb_internal_walls_overlay is None or self._rgb_internal_walls_overlay.shape[:2] != (H_px, W_px):
            self._build_rgb_internal_walls_overlay(board, cell_size_override=cell_size)
        if self._rgb_internal_walls_overlay is not None:
            overlay = self._rgb_internal_walls_overlay
            mask = (overlay.sum(axis=2) < 3 * 255)
            img[mask] = overlay[mask]

        # Inline draw helpers for robots/target
        def draw_hline(y: int, x0: int, x1: int, color: Tuple[int, int, int], thickness: int) -> None:
            y0 = max(0, y - thickness // 2)
            y1 = min(H_px, y + (thickness - thickness // 2))
            x0c = max(0, min(x0, x1))
            x1c = min(W_px, max(x0, x1))
            img[y0:y1, x0c:x1c] = color

        def draw_vline(x: int, y0: int, y1: int, color: Tuple[int, int, int], thickness: int) -> None:
            x0 = max(0, x - thickness // 2)
            x1 = min(W_px, x + (thickness - thickness // 2))
            y0c = max(0, min(y0, y1))
            y1c = min(H_px, max(y0, y1))
            img[y0c:y1c, x0:x1] = color

        def draw_circle(cx: float, cy: float, radius: float, color: Tuple[int, int, int], fill: bool) -> None:
            x0 = int(max(0, np.floor(cx - radius)))
            x1 = int(min(W_px - 1, np.ceil(cx + radius)))
            y0 = int(max(0, np.floor(cy - radius)))
            y1 = int(min(H_px - 1, np.ceil(cy + radius)))
            yy, xx = np.ogrid[y0:y1 + 1, x0:x1 + 1]
            mask = (xx - cx) ** 2 + (yy - cy) ** 2 <= radius ** 2
            if fill:
                img[y0:y1 + 1, x0:x1 + 1][mask] = color
            else:
                inner = (xx - cx) ** 2 + (yy - cy) ** 2 <= (radius - 1) ** 2
                ring = np.logical_and(mask, np.logical_not(inner))
                img[y0:y1 + 1, x0:x1 + 1][ring] = color

        def draw_star(cx: float, cy: float, arm_len: float, color: Tuple[int, int, int], thickness: int) -> None:
            draw_hline(int(cy), int(cx - arm_len), int(cx + arm_len), color, thickness)
            draw_vline(int(cx), int(cy - arm_len), int(cy + arm_len), color, thickness)
            steps = int(arm_len)
            for s in range(-steps, steps + 1):
                x = int(cx + s)
                y1 = int(cy + s)
                y2 = int(cy - s)
                y0a = max(0, y1 - thickness // 2)
                y1a = min(H_px, y1 + (thickness - thickness // 2))
                x0a = max(0, x - thickness // 2)
                x1a = min(W_px, x + (thickness - thickness // 2))
                img[y0a:y1a, x0a:x1a] = color
                y0b = max(0, y2 - thickness // 2)
                y1b = min(H_px, y2 + (thickness - thickness // 2))
                img[y0b:y1b, x0a:x1a] = color

        # Draw robots
        radius = cell_size * float(circle_r_frac)
        for rid, (rr, cc) in board.robot_positions.items():
            color = robot_colors[rid % len(robot_colors)]
            cy = rr * cell_size + cell_size * 0.5
            cx = cc * cell_size + cell_size * 0.5
            draw_circle(cx, cy, radius, color, circle_fill)

        # Draw target star
        tr = board.target_robot
        tr_color = robot_colors[tr % len(robot_colors)]
        dark_color = tuple(int(max(0, min(255, target_dark_factor * v))) for v in tr_color)
        gr, gc = board.goal_position
        cy = gr * cell_size + cell_size * 0.5
        cx = gc * cell_size + cell_size * 0.5
        draw_star(cx, cy, arm_len=cell_size * 0.35, color=dark_color, thickness=star_th)
        return img

    def _get_rgb_background(self, H: int, W: int, cell_size: int, grid_color: Tuple[int, int, int], grid_th: int, wall_color: Tuple[int, int, int], wall_th: int) -> np.ndarray:
        key = (int(H), int(W), int(cell_size), tuple(int(x) for x in grid_color), int(grid_th), tuple(int(x) for x in wall_color), int(wall_th))
        cached = RicochetRobotsEnv._RGB_BG_CACHE.get(key)
        if cached is not None:
            return cached
        H_px, W_px = int(H * cell_size), int(W * cell_size)
        img = np.ones((H_px, W_px, 3), dtype=np.uint8) * 255
        # Draw grid
        for r in range(H + 1):
            y = int(r * cell_size)
            y0 = max(0, y - grid_th // 2)
            y1 = min(H_px, y + (grid_th - grid_th // 2))
            img[y0:y1, 0:W_px] = grid_color
        for c in range(W + 1):
            x = int(c * cell_size)
            x0 = max(0, x - grid_th // 2)
            x1 = min(W_px, x + (grid_th - grid_th // 2))
            img[0:H_px, x0:x1] = grid_color
        # Draw boundary walls (top/bottom/left/right)
        # Horizontal top and bottom
        y = 0
        y0 = max(0, y - wall_th // 2)
        y1 = min(H_px, y + (wall_th - wall_th // 2))
        img[y0:y1, 0:W_px] = wall_color
        y = H_px
        y0 = max(0, y - wall_th // 2)
        y1 = min(H_px, y + (wall_th - wall_th // 2))
        img[y0:y1, 0:W_px] = wall_color
        # Vertical left and right
        x = 0
        x0 = max(0, x - wall_th // 2)
        x1 = min(W_px, x + (wall_th - wall_th // 2))
        img[0:H_px, x0:x1] = wall_color
        x = W_px
        x0 = max(0, x - wall_th // 2)
        x1 = min(W_px, x + (wall_th - wall_th // 2))
        img[0:H_px, x0:x1] = wall_color
        RicochetRobotsEnv._RGB_BG_CACHE[key] = img
        return img

    def _build_rgb_internal_walls_overlay(self, board: Board, cell_size_override: Optional[int] = None) -> None:
        cfg = self._render_rgb_cfg
        cell_size = int(cell_size_override if cell_size_override is not None else cfg["cell_size"])  # type: ignore[index]
        wall_color = tuple(int(x) for x in cfg["wall_color"])  # type: ignore[index]
        wall_th = int(cfg["wall_thickness"])  # type: ignore[index]
        H, W = board.height, board.width
        H_px, W_px = H * cell_size, W * cell_size
        overlay = np.zeros((H_px, W_px, 3), dtype=np.uint8)
        # Draw only interior walls (exclude boundaries at r=0,r=H,c=0,c=W)
        for r in range(1, H):
            for c in range(W):
                if board.h_walls[r, c]:
                    y = int(r * cell_size)
                    y0 = max(0, y - wall_th // 2)
                    y1 = min(H_px, y + (wall_th - wall_th // 2))
                    x0 = int(c * cell_size)
                    x1 = int((c + 1) * cell_size)
                    overlay[y0:y1, x0:x1] = wall_color
        for r in range(H):
            for c in range(1, W):
                if board.v_walls[r, c]:
                    x = int(c * cell_size)
                    x0 = max(0, x - wall_th // 2)
                    x1 = min(W_px, x + (wall_th - wall_th // 2))
                    y0 = int(r * cell_size)
                    y1 = int((r + 1) * cell_size)
                    overlay[y0:y1, x0:x1] = wall_color
        self._rgb_internal_walls_overlay = overlay


def generate_board_from_spec_with_seed(
    *,
    height: int,
    width: int,
    num_robots: int,
    edge_t_per_quadrant: int,
    central_l_per_quadrant: int,
    seed: int,
    _rng: Optional[np.random.Generator] = None,
) -> Board:
    """Generate a Board deterministically from spec and seed without constructing the env.

    If an RNG is provided, it will be used (and its state advanced deterministically);
    otherwise, a fresh Generator seeded with 'seed' is created.
    """
    h, w = int(height), int(width)
    assert h == w, "Boards should always be square per specification"
    rng = _rng if _rng is not None else np.random.default_rng(int(seed))

    # Initialize empty walls then set borders
    h_walls = np.zeros((h + 1, w), dtype=bool)
    v_walls = np.zeros((h, w + 1), dtype=bool)
    h_walls[0, :] = True
    h_walls[h, :] = True
    v_walls[:, 0] = True
    v_walls[:, w] = True

    reserved_cells: Set[Tuple[int, int]] = set()
    def reserve_cell(cell: Tuple[int, int]) -> None:
        r, c = cell
        if 0 <= r < h and 0 <= c < w:
            reserved_cells.add((r, c))

    # Forbidden central block
    if h % 2 == 1:
        mid = h // 2
        h_walls[mid, mid] = True
        h_walls[mid + 1, mid] = True
        v_walls[mid, mid] = True
        v_walls[mid, mid + 1] = True
        reserve_cell((mid, mid))
        central_rows = [mid]
        central_cols = [mid]
    else:
        mid = h // 2
        h_walls[mid-1, mid - 1] = True
        h_walls[mid-1, mid] = True
        h_walls[mid + 1, mid - 1] = True
        h_walls[mid + 1, mid] = True
        v_walls[mid - 1, mid-1] = True
        v_walls[mid, mid-1] = True
        v_walls[mid - 1, mid + 1] = True
        v_walls[mid, mid + 1] = True
        for rr in (mid - 1, mid):
            for cc in (mid - 1, mid):
                reserve_cell((rr, cc))
        central_rows = [mid - 1, mid]
        central_cols = [mid - 1, mid]

    def ranges_excluding_central(size: int, central_idxs: List[int]) -> Tuple[Tuple[int, int], Tuple[int, int]]:
        unique = sorted(set(central_idxs))
        if len(unique) == 1:
            mid_idx = unique[0]
            return (0, mid_idx - 1), (mid_idx + 1, size - 1)
        else:
            low_end = unique[0] - 1
            high_start = unique[1] + 1
            return (0, low_end), (high_start, size - 1)

    (r0_start, r0_end), (r1_start, r1_end) = ranges_excluding_central(h, central_rows)
    (c0_start, c0_end), (c1_start, c1_end) = ranges_excluding_central(w, central_cols)
    quadrants = {
        "NW": (r0_start, r0_end, c0_start, c0_end),
        "NE": (r0_start, r0_end, c1_start, c1_end),
        "SW": (r1_start, r1_end, c0_start, c0_end),
        "SE": (r1_start, r1_end, c1_start, c1_end),
    }

    used_interior_h: Set[Tuple[int, int]] = set()
    used_interior_v: Set[Tuple[int, int]] = set()

    def cell_has_nonboundary_wall(r: int, c: int) -> bool:
        if r > 0 and h_walls[r, c]:
            return True
        if r + 1 < h and h_walls[r + 1, c]:
            return True
        if c > 0 and v_walls[r, c]:
            return True
        if c + 1 < w and v_walls[r, c + 1]:
            return True
        return False

    def neighbors4(r: int, c: int) -> List[Tuple[int, int]]:
        nbrs: List[Tuple[int, int]] = []
        if r - 1 >= 0:
            nbrs.append((r - 1, c))
        if r + 1 < h:
            nbrs.append((r + 1, c))
        if c - 1 >= 0:
            nbrs.append((r, c - 1))
        if c + 1 < w:
            nbrs.append((r, c + 1))
        return nbrs

    def can_place_h(row: int, col: int) -> bool:
        if not (1 <= row <= h - 1 and 0 <= col <= w - 1):
            return False
        if h_walls[row, col]:
            return False
        neighbors = [
            (row - 1, col), (row + 1, col)
        ]
        for nr, nc in neighbors:
            if 1 <= nr <= h - 1 and 0 <= nc <= w - 1 and h_walls[nr, nc]:
                return False
        return True

    def can_place_v(row: int, col: int) -> bool:
        if not (0 <= row <= h - 1 and 1 <= col <= w - 1):
            return False
        if v_walls[row, col]:
            return False
        neighbors = [
            (row, col - 1), (row, col + 1)
        ]
        for nr, nc in neighbors:
            if 0 <= nr <= h - 1 and 1 <= nc <= w - 1 and v_walls[nr, nc]:
                return False
        return True

    def place_edge_t_in_quadrant(q_bounds: Tuple[int, int, int, int]) -> bool:
        rs, re, cs, ce = q_bounds
        if rs > re or cs > ce:
            return False
        choices: List[Tuple[str, Tuple[int, int]]] = []
        if rs == 0:
            for c in range(cs + 1, ce + 1):
                choices.append(("TOP", (0, c)))
        if re == h - 1:
            for c in range(cs + 1, ce + 1):
                choices.append(("BOTTOM", (h - 1, c)))
        if cs == 0:
            for r in range(rs + 1, re + 1):
                choices.append(("LEFT", (r, 0)))
        if ce == w - 1:
            for r in range(rs + 1, re + 1):
                choices.append(("RIGHT", (r, w - 1)))
        if not choices:
            return False
        idxs = np.arange(len(choices))
        rng.shuffle(idxs)
        for idx in idxs:
            side, pos = choices[idx]
            if side in ("TOP", "BOTTOM"):
                r0, c = pos
                if c in (1, w - 1):
                    continue
                if can_place_v(r0, c):
                    adj_cell = (1 if side == "TOP" else h - 2, c - 1)
                    blocked = any(cell_has_nonboundary_wall(rr, cc) for rr, cc in neighbors4(adj_cell[0], adj_cell[1]))
                    if not blocked and (0 <= adj_cell[0] < h and 0 <= adj_cell[1] < w) and adj_cell not in reserved_cells:
                        v_walls[r0, c] = True
                        used_interior_v.add((r0, c))
                        reserve_cell(adj_cell)
                        return True
            else:
                r, c0 = pos
                if r in (1, h - 1):
                    continue
                if can_place_h(r, c0):
                    adj_cell = (r - 1, 1 if side == "LEFT" else w - 2)
                    blocked = any(cell_has_nonboundary_wall(rr, cc) for rr, cc in neighbors4(adj_cell[0], adj_cell[1]))
                    if not blocked and (0 <= adj_cell[0] < h and 0 <= adj_cell[1] < w) and adj_cell not in reserved_cells:
                        h_walls[r, c0] = True
                        used_interior_h.add((r, c0))
                        reserve_cell(adj_cell)
                        return True
        return False

    orientations = ["TL", "TR", "BL", "BR"]
    central_l_centers: List[Tuple[int, int]] = []

    def place_central_l_in_quadrant(q_bounds: Tuple[int, int, int, int], desired_orientation: str) -> bool:
        rs, re, cs, ce = q_bounds
        if rs > re or cs > ce:
            return False
        candidates: List[Tuple[int, int]] = []
        for r in range(rs, re + 1):
            for c in range(cs, ce + 1):
                if r == 0 or r == h - 1 or c == 0 or c == w - 1:
                    continue
                if (r, c) in reserved_cells:
                    continue
                if any(cell_has_nonboundary_wall(rr, cc) for rr, cc in neighbors4(r, c)):
                    continue
                candidates.append((r, c))
        if not candidates:
            return False
        idxs = np.arange(len(candidates))
        rng.shuffle(idxs)
        for idx in idxs:
            r, c = candidates[idx]
            need_h: Optional[Tuple[int, int]] = None
            need_v: Optional[Tuple[int, int]] = None
            if desired_orientation == "TL":
                need_h = (r, c)
                need_v = (r, c)
            elif desired_orientation == "TR":
                need_h = (r, c)
                need_v = (r, c + 1)
            elif desired_orientation == "BL":
                need_h = (r + 1, c)
                need_v = (r, c)
            else:  # BR
                need_h = (r + 1, c)
                need_v = (r, c + 1)
            if need_h is None or need_v is None:
                continue
            hr, hc = need_h
            vr, vc = need_v
            if not can_place_h(hr, hc):
                continue
            if not can_place_v(vr, vc):
                continue
            if hr in (0, h) or vc in (0, w):
                continue
            h_walls[hr, hc] = True
            v_walls[vr, vc] = True
            used_interior_h.add((hr, hc))
            used_interior_v.add((vr, vc))
            reserve_cell((r, c))
            central_l_centers.append((r, c))
            return True
        return False

    # 1) Edge-T per quadrant
    for _, bounds in quadrants.items():
        count = int(edge_t_per_quadrant)
        placed = 0
        attempts = 0
        while placed < count and attempts < count * 10:
            if place_edge_t_in_quadrant(bounds):
                placed += 1
            attempts += 1

    # 2) Central-L per quadrant with even orientation distribution
    for _, bounds in quadrants.items():
        count = int(central_l_per_quadrant)
        if count <= 0:
            continue
        orient_list: List[str] = []
        for i in range(count):
            orient_list.append(orientations[i % 4])
        idxs = np.arange(len(orient_list))
        rng.shuffle(idxs)
        orient_list = [orient_list[i] for i in idxs]
        placed = 0
        attempts = 0
        i = 0
        while placed < count and attempts < count * 20 and i < len(orient_list):
            if place_central_l_in_quadrant(bounds, orient_list[i]):
                placed += 1
            i += 1
            attempts += 1

    # Goal inside central-L
    central_set = set((r, c) for r in central_rows for c in central_cols)
    candidates_goal = [rc for rc in central_l_centers if rc not in central_set]
    if not candidates_goal:
        candidates_goal = [(r, c) for r in range(h) for c in range(w) if (r, c) not in central_set]
    idx = int(rng.integers(0, len(candidates_goal)))
    goal = candidates_goal[idx]

    # Robots
    free_cells: List[Tuple[int, int]] = []
    for r in range(h):
        for c in range(w):
            if (r, c) in central_set or (r, c) == goal:
                continue
            free_cells.append((r, c))
    rng.shuffle(free_cells)
    robot_positions: Dict[int, Tuple[int, int]] = {}
    for rid in range(int(num_robots)):
        robot_positions[rid] = free_cells.pop()
    target_robot = int(rng.integers(0, int(num_robots)))
    return Board(height=h, width=w, h_walls=h_walls, v_walls=v_walls, robot_positions=robot_positions, goal_position=goal, target_robot=target_robot)


# Optional helper for registration with gymnasium
def register_env(env_id: str = "RicochetRobots-v0", *, max_episode_steps: Optional[int] = None) -> None:
    """Register this environment with gymnasium so it can be created via gym.make(env_id).

    Note: This env already enforces a time limit via its max_steps argument. If you also set
    max_episode_steps here and wrap with Gym's TimeLimit implicitly via gym.make, you may see
    duplicate truncation behavior. Prefer one source of time limits.
    """
    try:
        from gymnasium.envs.registration import register  # type: ignore
        from gymnasium.error import Error as GymError  # type: ignore
        try:
            kwargs = {}
            if max_episode_steps is not None:
                kwargs["max_episode_steps"] = int(max_episode_steps)
            register(
                id=env_id,
                entry_point="env.ricochet_env:RicochetRobotsEnv",
                **kwargs,
            )
        except GymError as e:  # pragma: no cover - depends on gym version behavior
            # Swallow duplicate registration errors; re-raise others
            if "already registered" in str(e).lower():
                return
            raise
    except (ModuleNotFoundError, ImportError):
        # Silently ignore if gymnasium not available
        pass

```

```python
from __future__ import annotations

from dataclasses import dataclass
from typing import Dict, Optional, Tuple, List, Literal, Set

import numpy as np

from .ricochet_core import Board, apply_action, reached_goal

# Provide a runtime gymnasium binding if available, otherwise a minimal fallback
try:  # pragma: no cover
    import importlib
    _gym = importlib.import_module("gymnasium")  # type: ignore
    _spaces = importlib.import_module("gymnasium.spaces")  # type: ignore
    _seeding = importlib.import_module("gymnasium.utils.seeding")  # type: ignore
    _error = importlib.import_module("gymnasium.error")  # type: ignore
    GymEnvBase = _gym.Env  # type: ignore
    spaces = _spaces  # type: ignore
    GymInvalidAction = getattr(_error, "InvalidAction", None)  # type: ignore
except (ModuleNotFoundError, ImportError):  # pragma: no cover
    class GymEnvBase(object):
        pass
    class _SpacesDummy:  # minimal placeholders to satisfy type usage
        class Box:  # type: ignore
            def __init__(self, low=None, high=None, shape=None, dtype=None):
                self.low = low
                self.high = high
                self.shape = shape
                self.dtype = dtype
        class Discrete:  # type: ignore
            def __init__(self, n: int):
                self.n = n
    spaces = _SpacesDummy()  # type: ignore
    GymInvalidAction = None  # type: ignore


@dataclass
class FixedLayout:
    height: int
    width: int
    # Edge walls
    h_walls: np.ndarray  # (H+1, W) bool
    v_walls: np.ndarray  # (H, W+1) bool
    robot_positions: Dict[int, Tuple[int, int]]
    goal_position: Tuple[int, int]
    target_robot: int


ObsMode = Literal["image", "symbolic", "rgb_image"]


class RicochetRobotsEnv(GymEnvBase):
    """Ricochet Robots Gymnasium-compatible environment.

    Determinism and seeding:
    - Deterministic given the seed provided to __init__ or reset(seed=...).
    - Randomness flows through self.np_random (Gymnasium seeding utils); the episode's resolved
      seed is exposed via info["episode_seed"].
    - When ensure_solvable=True, the environment may retry board generation; retries do not add
      extra non-determinism beyond RNG draws. The included solver is deterministic.

    Observations:
    - Image mode: float32 in [0,1]. Channels are named by channel_names; note that the target robot
      is represented both by a dedicated mask (index 5) and its robot channel (duplication by design).
    - Symbolic mode: float32 with absolute grid indices (not normalized). Downstream code may wish to
      normalize to [0,1]. The Box low/high reflect board bounds.

    Rendering:
    - Only render_mode="ascii" is implemented and returns a string frame.
    - Other modes are not implemented and will raise a clear error.
    """
    metadata = {"render_modes": ["ascii", "rgb"], "render_fps": 4}
    # Class-level cache for RGB backgrounds per (H, W, cell_size, grid_color, grid_th, wall_color, wall_th)
    _RGB_BG_CACHE: Dict[Tuple[int, int, int, Tuple[int, int, int], int, Tuple[int, int, int], int], np.ndarray] = {}

    def __init__(
        self,
        height: int = 8,
        width: int = 8,
        num_robots: int = 2,
        include_noop: bool = True,
        step_penalty: float = -0.01,
        goal_reward: float = 1.0,
        noop_penalty: Optional[float] = None,
        max_steps: int = 10,
        fixed_layout: Optional[FixedLayout] = None,
        seed: Optional[int] = None,
        ensure_solvable: bool = False,
        solver_max_depth: int = 30,
        solver_max_nodes: int = 20000,
        obs_mode: ObsMode = "image",
        channels_first: bool = False,
        render_mode: Optional[str] = None,
        ensure_attempt_limit: int = 500,
        # Spec-driven generation configuration
        edge_t_per_quadrant: Optional[int] = None,
        central_l_per_quadrant: Optional[int] = None,
        # RGB render configuration
        render_rgb_config: Optional[Dict[str, object]] = None,
    ) -> None:
        super().__init__()
        self.height = height
        self.width = width
        self.num_robots = num_robots
        self.include_noop = include_noop
        self.step_penalty = step_penalty
        self.goal_reward = goal_reward
        self.noop_penalty = step_penalty if noop_penalty is None else noop_penalty
        self.max_steps = max_steps
        self.fixed_layout = fixed_layout
        self.ensure_solvable = ensure_solvable
        self.solver_max_depth = solver_max_depth
        self.solver_max_nodes = solver_max_nodes
        self.obs_mode: ObsMode = obs_mode
        self.channels_first = channels_first
        # If a fixed layout is provided, align env geometry before building spaces
        if self.fixed_layout is not None:
            self.height = int(self.fixed_layout.height)
            self.width = int(self.fixed_layout.width)
            self.num_robots = int(len(self.fixed_layout.robot_positions))
        # Render API
        self.render_mode = render_mode
        if self.render_mode is not None and self.render_mode not in self.metadata["render_modes"]:
            raise ValueError(f"Unsupported render_mode {self.render_mode}. Supported: {self.metadata['render_modes']}")
        # RGB render defaults
        self._render_rgb_cfg: Dict[str, object] = {
            "cell_size": 20,
            "grid_color": (200, 200, 200),
            "grid_thickness": 1,
            "wall_color": (0, 0, 0),
            "wall_thickness": 3,
            "robot_colors": [
                (220, 50, 50),     # red
                (50, 120, 220),    # blue
                (50, 180, 80),     # green
                (230, 200, 40),    # yellow
                (180, 80, 180),    # purple (extra if more robots)
            ],
            "circle_fill": True,
            "circle_radius_frac": 0.35,
            "target_dark_factor": 0.6,
            "star_thickness": 2,
        }
        if render_rgb_config is not None:
            self._render_rgb_cfg.update(render_rgb_config)
        # Per-instance cache: RGB internal walls overlay built for current layout (excludes boundary walls)
        self._rgb_internal_walls_overlay: Optional[np.ndarray] = None

        # Seeding: adopt Gymnasium convention with self.np_random
        self.np_random: np.random.Generator
        self._episode_seed: Optional[int] = None
        try:  # gymnasium present
            # _seeding.np_random returns (rng, seed)
            self.np_random, used_seed = _seeding.np_random(seed)  # type: ignore[name-defined]
            self._episode_seed = int(used_seed) if used_seed is not None else None
        except NameError:
            self.np_random = np.random.default_rng(seed)
            self._episode_seed = int(seed) if seed is not None else None
        # Preserve a base seed to enforce deterministic resets when no new seed is provided
        self._base_seed: Optional[int] = int(seed) if seed is not None else self._episode_seed
        self._board: Optional[Board] = None
        self._num_steps = 0
        self._cached_wall_obs: Optional[np.ndarray] = None  # (H,W,4) float32 for wall channels
        self._ensure_attempt_limit = int(ensure_attempt_limit)
        # Spec default counts per quadrant (based on smaller board side)
        side_min = min(self.height, self.width)
        default_per_quad = 2 if side_min < 10 else 4
        self._edge_t_per_quadrant = default_per_quad if edge_t_per_quadrant is None else int(edge_t_per_quadrant)
        self._central_l_per_quadrant = default_per_quad if central_l_per_quadrant is None else int(central_l_per_quadrant)
        # Track the exact per-episode board seed used to generate the current board
        self._last_board_seed: Optional[int] = None

        # Observation spaces
        if self.obs_mode == "image":
            # channels: up_wall, down_wall, left_wall, right_wall, goal, target_mask, robots...
            self._base_wall_channels = 4
            self._base_misc_channels = 2  # goal, target
            self._num_channels = self._base_wall_channels + self._base_misc_channels + self.num_robots
            if self.channels_first:
                obs_shape = (self._num_channels, self.height, self.width)
            else:
                obs_shape = (self.height, self.width, self._num_channels)
            self.observation_space = spaces.Box(
                low=0.0,
                high=1.0,
                shape=obs_shape,
                dtype=np.float32,
            )
        elif self.obs_mode == "rgb_image":
            # RGB image observation: uses fixed pixel dimensions for consistent observation space
            # This ensures all curriculum levels have the same observation space size
            # Lowered from 128 to 96 to reduce memory/compute; resize handled in _board_to_rgb_obs
            FIXED_PIXEL_SIZE = 256
            
            if self.channels_first:
                obs_shape = (3, FIXED_PIXEL_SIZE, FIXED_PIXEL_SIZE)
            else:
                obs_shape = (FIXED_PIXEL_SIZE, FIXED_PIXEL_SIZE, 3)
            self.observation_space = spaces.Box(
                low=0,
                high=255,
                shape=obs_shape,
                dtype=np.uint8,
            )
        elif self.obs_mode == "symbolic":
            # [goal_r, goal_c] in [0..H-1], [0..W-1]; one-hot target in [0,1]; robots positions in bounds
            vec_len = 2 + self.num_robots + (self.num_robots * 2)
            low = np.full((vec_len,), 0.0, dtype=np.float32)
            high = np.full((vec_len,), 1.0, dtype=np.float32)
            # goal bounds
            low[0] = 0.0
            high[0] = float(self.height - 1)
            low[1] = 0.0
            high[1] = float(self.width - 1)
            # target one-hot already [0,1]
            # robots positions
            base = 2 + self.num_robots
            for i in range(self.num_robots):
                low[base + 2 * i] = 0.0
                high[base + 2 * i] = float(self.height - 1)
                low[base + 2 * i + 1] = 0.0
                high[base + 2 * i + 1] = float(self.width - 1)
            self.observation_space = spaces.Box(low=low, high=high, shape=(vec_len,), dtype=np.float32)
        else:
            raise ValueError(f"Unknown obs_mode: {self.obs_mode}")

        # Action space: num_robots * 4 directions (+ 1 for noop)
        self._noop_action = self.num_robots * 4 if include_noop else None
        self.action_space = spaces.Discrete(self.num_robots * 4 + (1 if include_noop else 0))

    # Gymnasium API
    def reset(self, *, seed: Optional[int] = None, options: Optional[dict] = None):
        """Reset the environment and start a new episode.

        Returns (obs, info) per Gymnasium API. info contains keys: 'target_robot',
        'level_solvable' (optional), 'optimal_length' (if ensured), 'solver_limits' (if ensured),
        and 'episode_seed' with the seed used for this episode.
        """
        # Import profiling here to avoid circular imports
        try:
            from profiling import profile
        except ImportError:
            # Fallback if profiling not available
            def profile(name, track_memory=True):
                from contextlib import nullcontext
                return nullcontext()

        # Standard seeding protocol
        if seed is not None:
            try:
                self.np_random, used_seed = _seeding.np_random(seed)  # type: ignore[name-defined]
                self._episode_seed = int(used_seed) if used_seed is not None else None
            except NameError:
                self.np_random = np.random.default_rng(seed)
                self._episode_seed = int(seed)
            self._base_seed = int(self._episode_seed) if self._episode_seed is not None else None
        else:
            # Re-seed with base seed so that resets are deterministic when no new seed is provided
            if self._base_seed is not None:
                try:
                    self.np_random, used_seed = _seeding.np_random(self._base_seed)  # type: ignore[name-defined]
                    self._episode_seed = int(used_seed) if used_seed is not None else int(self._base_seed)
                except NameError:
                    self.np_random = np.random.default_rng(self._base_seed)
                    self._episode_seed = int(self._base_seed)
        _ = options  # silence unused
        self._num_steps = 0
        # clear cached walls in case layout changes
        self._cached_wall_obs = None

        if self.fixed_layout is not None:
            with profile("env_reset_fixed_layout", track_memory=True):
                self._board = Board(
                height=self.fixed_layout.height,
                width=self.fixed_layout.width,
                h_walls=self.fixed_layout.h_walls.copy(),
                v_walls=self.fixed_layout.v_walls.copy(),
                robot_positions=dict(self.fixed_layout.robot_positions),
                goal_position=self.fixed_layout.goal_position,
                target_robot=self.fixed_layout.target_robot,
            )
            # cache static walls for this layout
            with profile("env_build_cached_walls", track_memory=True):
                self._build_cached_walls(self._board)
            # Build per-layout internal walls RGB overlay cache (for rgb render mode)
            try:
                if self.render_mode == "rgb":
                    self._build_rgb_internal_walls_overlay(self._board)
            except Exception:
                self._rgb_internal_walls_overlay = None
            with profile("env_make_obs", track_memory=True):
                obs = self._make_obs(self._board)
            info = {
                "level_solvable": False,
                "ensure_solvable_enabled": False,
                "target_robot": self._board.target_robot,
                "episode_seed": self._episode_seed,
            }
            if self.obs_mode == "image":
                info["channel_names"] = self.channel_names
            return obs, info

        # Otherwise, generate random board, optionally ensure solvable via solver
        attempts = 0
        while True:
            attempts += 1
            with profile("env_generate_board", track_memory=True):
                board = self._generate_random_board()
            if not self.ensure_solvable:
                self._board = board
                break
            try:
                from .solver import solve_bfs  # type: ignore
                with profile("env_solver_check", track_memory=True):
                    solution = solve_bfs(board, max_depth=self.solver_max_depth, max_nodes=self.solver_max_nodes)
                if solution is not None:
                    self._board = board
                    optimal_length = len(solution)
                    break
            except ImportError as exc:
                raise RuntimeError("ensure_solvable=True but solver unavailable (ImportError)") from exc
            if attempts >= self._ensure_attempt_limit:
                raise RuntimeError("Failed to generate solvable board within attempt limit")

        # cache static walls for this layout
        assert self._board is not None
        with profile("env_build_cached_walls", track_memory=True):
            self._build_cached_walls(self._board)
        # Build per-layout internal walls RGB overlay cache (for rgb render mode)
        try:
            if self.render_mode == "rgb":
                self._build_rgb_internal_walls_overlay(self._board)
        except Exception:
            self._rgb_internal_walls_overlay = None
        with profile("env_make_obs", track_memory=True):
            obs = self._make_obs(self._board)
        # Note: solver None during attempts may be due to cutoffs (depth/nodes), not true unsolvability.
        info = {
            "level_solvable": bool(self.ensure_solvable),
            "ensure_solvable_enabled": bool(self.ensure_solvable),
            "target_robot": self._board.target_robot,
            "episode_seed": self._episode_seed,
        }
        # Expose the exact per-episode board seed used to build this board
        if hasattr(self, "_last_board_seed") and self._last_board_seed is not None:
            info["board_seed"] = int(self._last_board_seed)
        if self.obs_mode == "image":
            info["channel_names"] = self.channel_names
        if self.ensure_solvable:
            info["optimal_length"] = optimal_length
            info["solver_limits"] = {"max_depth": self.solver_max_depth, "max_nodes": self.solver_max_nodes}
        return obs, info

    def step(self, action: int):
        """Apply an action and return (obs, reward, terminated, truncated, info).

        info always includes 'steps', 'target_robot', and 'is_success' (bool). When truncated due to
        max steps, info includes 'TimeLimit.truncated'=True.
        """
        # Import profiling here to avoid circular imports
        try:
            from profiling import profile
        except ImportError:
            # Fallback if profiling not available
            def profile(name, track_memory=True):
                from contextlib import nullcontext
                return nullcontext()

        assert self._board is not None, "Call reset() first"
        self._num_steps += 1

        terminated = False
        truncated = False
        info: Dict[str, object] = {}

        reward = self.step_penalty
        # Validate action range early
        if not (0 <= action < self.action_space.n):
            if GymInvalidAction is not None:
                raise GymInvalidAction(f"Invalid action {action}")  # type: ignore[misc]
            raise ValueError(f"Invalid action {action}")

        if self.include_noop and action == self._noop_action:
            reward = self.noop_penalty
        else:
            with profile("env_apply_action", track_memory=True):
                robot_id, direction = self._decode_action(action)
                self._board = apply_action(self._board, robot_id, direction)

        if reached_goal(self._board):
            reward += self.goal_reward
            terminated = True
            info["is_success"] = True

        if not terminated and self._num_steps >= self.max_steps:
            truncated = True
            info["TimeLimit.truncated"] = True

        with profile("env_make_obs", track_memory=True):
            obs = self._make_obs(self._board)
        info["steps"] = self._num_steps
        info["target_robot"] = self._board.target_robot
        if "is_success" not in info:
            info["is_success"] = False
        return obs, reward, terminated, truncated, info

    def render(self):
        """Render according to render_mode.

        - render_mode=="ascii": returns a string frame.
        - render_mode=="rgb": returns an (H_px, W_px, 3) uint8 array.
        - render_mode is None or unsupported: returns None.
        """
        if self.render_mode is None:
            return None
        if self._board is None:
            return "" if self.render_mode == "ascii" else None
        if self.render_mode == "ascii":
            from .ricochet_core import render_ascii  # local import to avoid circulars in lints
            return render_ascii(self._board)
        if self.render_mode == "rgb":
            return self._render_rgb(self._board)
        if self._board is None:
            return ""
        raise ValueError(f"Unsupported render_mode {self.render_mode}. Supported: {self.metadata['render_modes']}")

    def close(self) -> None:
        """Close environment resources (no-op)."""
        return None

    def seed(self, seed: Optional[int] = None):  # legacy convenience
        """Legacy seeding helper returning [seed]. Prefer reset(seed=...)."""
        if seed is None:
            return [self._episode_seed]
        try:
            self.np_random, used_seed = _seeding.np_random(seed)  # type: ignore[name-defined]
            self._episode_seed = int(used_seed) if used_seed is not None else int(seed)
        except NameError:
            self.np_random = np.random.default_rng(seed)
            self._episode_seed = int(seed)
        return [self._episode_seed]

    # Helpers
    def _decode_action(self, action: int) -> Tuple[int, int]:
        if not (0 <= action < self.action_space.n):
            raise ValueError(f"Invalid action {action}")
        if self.include_noop and action == self._noop_action:
            raise ValueError("_decode_action called on noop action")
        robot_id = action // 4
        direction = action % 4
        if robot_id >= self.num_robots:
            raise ValueError(f"Invalid robot_id {robot_id} for action {action}")
        return robot_id, direction

    def _make_obs(self, board: Board) -> np.ndarray:
        if self.obs_mode == "image":
            return self._board_to_image_obs(board)
        elif self.obs_mode == "rgb_image":
            return self._board_to_rgb_obs(board)
        return self._board_to_symbolic_obs(board)

    def _board_to_image_obs(self, board: Board) -> np.ndarray:
        # Import profiling here to avoid circular imports
        try:
            from profiling import profile
        except ImportError:
            # Fallback if profiling not available
            def profile(name, track_memory=True):
                from contextlib import nullcontext
                return nullcontext()

        with profile("env_image_observation_generation", track_memory=True):
            h, w = board.height, board.width
            C = self._num_channels
            obs = np.zeros((h, w, C), dtype=np.float32)
            # wall channels 0..3 from cache
            if self._cached_wall_obs is not None:
                obs[:, :, 0:4] = self._cached_wall_obs
            else:
                # Fallback if cache not built (should not happen)
                for r in range(h):
                    for c in range(w):
                        obs[r, c, 0] = 1.0 if board.has_wall_up(r, c) else 0.0
                        obs[r, c, 1] = 1.0 if board.has_wall_down(r, c) else 0.0
                        obs[r, c, 2] = 1.0 if board.has_wall_left(r, c) else 0.0
                        obs[r, c, 3] = 1.0 if board.has_wall_right(r, c) else 0.0
            # goal channel 4
            gr, gc = board.goal_position
            obs[gr, gc, 4] = 1.0
            # target mask channel 5
            tr = board.target_robot
            tr_r, tr_c = board.robot_positions[tr]
            obs[tr_r, tr_c, 5] = 1.0
            # robot channels start at 6
            base = 6
            for rid, (r, c) in board.robot_positions.items():
                obs[r, c, base + rid] = 1.0
            if self.channels_first:
                obs = np.transpose(obs, (2, 0, 1))
            return obs

    def _board_to_symbolic_obs(self, board: Board) -> np.ndarray:
        # Import profiling here to avoid circular imports
        try:
            from profiling import profile
        except ImportError:
            # Fallback if profiling not available
            def profile(name, track_memory=True):
                from contextlib import nullcontext
                return nullcontext()

        with profile("env_symbolic_observation_generation", track_memory=True):
            gr, gc = board.goal_position
            target_one_hot = np.zeros((self.num_robots,), dtype=np.float32)
            target_one_hot[board.target_robot] = 1.0
            robots_vec: List[float] = []
            for rid in range(self.num_robots):
                r, c = board.robot_positions[rid]
                robots_vec.extend([float(r), float(c)])
            vec = np.concatenate([
                np.array([float(gr), float(gc)], dtype=np.float32),
                target_one_hot.astype(np.float32),
                np.array(robots_vec, dtype=np.float32),
            ])
            return vec

    def _board_to_rgb_obs(self, board: Board) -> np.ndarray:
        """Convert board to RGB image observation with fixed pixel dimensions.
        
        This creates an RGB image with fixed pixel dimensions (128x128) by adjusting
        the cell size to fit the board within the fixed pixel space. This ensures
        consistent observation space across all curriculum levels.
        """
        # Import profiling here to avoid circular imports
        try:
            from profiling import profile
        except ImportError:
            # Fallback if profiling not available
            def profile(name, track_memory=True):
                from contextlib import nullcontext
                return nullcontext()
        
        with profile("env_rgb_observation_generation", track_memory=True):
            FIXED_PIXEL_SIZE = 256
            # Padding around the board within the fixed output
            padding = 4
            available_size = FIXED_PIXEL_SIZE - 2 * padding
            cell_size = max(1, available_size // max(board.height, board.width))
            # Compute board pixel dims and canvas
            H_px = int(board.height * cell_size)
            W_px = int(board.width * cell_size)
            # Compose into fixed-size canvas without resizing
            canvas = np.ones((FIXED_PIXEL_SIZE, FIXED_PIXEL_SIZE, 3), dtype=np.uint8) * 255
            y0 = padding + (available_size - H_px) // 2
            x0 = padding + (available_size - W_px) // 2
            # Render board region with caching
            with profile("env_rgb_rendering_cached", track_memory=True):
                region = self._render_rgb_cached(board, cell_size)
            canvas[y0:y0+H_px, x0:x0+W_px] = region
            if self.channels_first:
                canvas = np.transpose(canvas, (2, 0, 1))
            return canvas

    def _generate_random_board(self) -> Board:
        # Draw a per-episode board seed from the env RNG
        seed_used = int(self.np_random.integers(0, 2**31 - 1))
        # Record it for reproducibility/debugging
        self._last_board_seed = seed_used
        # Generate deterministically from the explicit seed only (no external RNG)
        return generate_board_from_spec_with_seed(
            height=self.height,
            width=self.width,
            num_robots=self.num_robots,
            edge_t_per_quadrant=self._edge_t_per_quadrant,
            central_l_per_quadrant=self._central_l_per_quadrant,
            seed=seed_used,
        )

    # Public channel names for image observations
    @property
    def channel_names(self) -> List[str]:
        """Human-readable names for image observation channels."""
        names = ["wall_up", "wall_down", "wall_left", "wall_right", "goal", "target_robot_mask"]
        names.extend([f"robot_{i}" for i in range(self.num_robots)])
        return names

    def _build_cached_walls(self, board: Board) -> None:
        """Precompute static wall indicator channels (H,W,4)."""
        h, w = board.height, board.width
        wall = np.zeros((h, w, 4), dtype=np.float32)
        for r in range(h):
            for c in range(w):
                wall[r, c, 0] = 1.0 if board.has_wall_up(r, c) else 0.0
                wall[r, c, 1] = 1.0 if board.has_wall_down(r, c) else 0.0
                wall[r, c, 2] = 1.0 if board.has_wall_left(r, c) else 0.0
                wall[r, c, 3] = 1.0 if board.has_wall_right(r, c) else 0.0
        self._cached_wall_obs = wall

    # Public accessor for current board (read-only clone)
    def get_board(self) -> Board:
        if self._board is None:
            raise RuntimeError("Environment not initialized. Call reset() first.")
        return self._board.clone()

    def _render_rgb(self, board: Board) -> np.ndarray:
        # Import profiling here to avoid circular imports
        try:
            from profiling import profile
        except ImportError:
            # Fallback if profiling not available
            def profile(name, track_memory=True):
                from contextlib import nullcontext
                return nullcontext()
        
        with profile("env_rgb_render_setup", track_memory=True):
            cfg = self._render_rgb_cfg
            cell_size = int(cfg["cell_size"])  # pixels
            grid_color = tuple(int(x) for x in cfg["grid_color"])  # type: ignore[index]
            grid_th = int(cfg["grid_thickness"])  # px
            wall_color = tuple(int(x) for x in cfg["wall_color"])  # type: ignore[index]
            wall_th = int(cfg["wall_thickness"])  # px
            robot_colors: List[Tuple[int, int, int]] = [tuple(int(a) for a in t) for t in cfg["robot_colors"]]  # type: ignore[index]
            circle_fill: bool = bool(cfg["circle_fill"])  # type: ignore[index]
            circle_r_frac: float = float(cfg["circle_radius_frac"])  # type: ignore[index]
            target_dark_factor: float = float(cfg["target_dark_factor"])  # type: ignore[index]
            star_th: int = int(cfg["star_thickness"])  # type: ignore[index]

            H, W = board.height, board.width
            H_px, W_px = H * cell_size, W * cell_size
            # Start from cached background with grid+boundary walls
            bg = self._get_rgb_background(H, W, cell_size, grid_color, grid_th, wall_color, wall_th)
            img = bg.copy()

        # Helpers to draw
        def draw_hline(y: int, x0: int, x1: int, color: Tuple[int, int, int], thickness: int) -> None:
            y0 = max(0, y - thickness // 2)
            y1 = min(H_px, y + (thickness - thickness // 2))
            x0c = max(0, min(x0, x1))
            x1c = min(W_px, max(x0, x1))
            img[y0:y1, x0c:x1c] = color

        def draw_vline(x: int, y0: int, y1: int, color: Tuple[int, int, int], thickness: int) -> None:
            x0 = max(0, x - thickness // 2)
            x1 = min(W_px, x + (thickness - thickness // 2))
            y0c = max(0, min(y0, y1))
            y1c = min(H_px, max(y0, y1))
            img[y0c:y1c, x0:x1] = color

        def draw_circle(cx: float, cy: float, radius: float, color: Tuple[int, int, int], fill: bool) -> None:
            # Bounding box
            x0 = int(max(0, np.floor(cx - radius)))
            x1 = int(min(W_px - 1, np.ceil(cx + radius)))
            y0 = int(max(0, np.floor(cy - radius)))
            y1 = int(min(H_px - 1, np.ceil(cy + radius)))
            yy, xx = np.ogrid[y0:y1 + 1, x0:x1 + 1]
            mask = (xx - cx) ** 2 + (yy - cy) ** 2 <= radius ** 2
            if fill:
                img[y0:y1 + 1, x0:x1 + 1][mask] = color
            else:
                # outline 1px
                inner = (xx - cx) ** 2 + (yy - cy) ** 2 <= (radius - 1) ** 2
                ring = np.logical_and(mask, np.logical_not(inner))
                img[y0:y1 + 1, x0:x1 + 1][ring] = color

        def draw_star(cx: float, cy: float, arm_len: float, color: Tuple[int, int, int], thickness: int) -> None:
            # Draw + arms
            draw_hline(int(cy), int(cx - arm_len), int(cx + arm_len), color, thickness)
            draw_vline(int(cx), int(cy - arm_len), int(cy + arm_len), color, thickness)
            # Draw x arms (diagonals) by sampling small line thickness via plotting short lines of squares
            # Approximate diagonals by drawing small rectangles along the diagonal
            steps = int(arm_len)
            for s in range(-steps, steps + 1):
                x = int(cx + s)
                y1 = int(cy + s)
                y2 = int(cy - s)
                y0a = max(0, y1 - thickness // 2)
                y1a = min(H_px, y1 + (thickness - thickness // 2))
                x0a = max(0, x - thickness // 2)
                x1a = min(W_px, x + (thickness - thickness // 2))
                img[y0a:y1a, x0a:x1a] = color
                y0b = max(0, y2 - thickness // 2)
                y1b = min(H_px, y2 + (thickness - thickness // 2))
                img[y0b:y1b, x0a:x1a] = color

        # 1) Internal walls overlay (excluding boundary walls) drawn once per layout
        with profile("env_rgb_draw_internal_walls", track_memory=True):
            if self._rgb_internal_walls_overlay is None:
                self._build_rgb_internal_walls_overlay(board)
            if self._rgb_internal_walls_overlay is not None:
                overlay = self._rgb_internal_walls_overlay
                mask = (overlay.sum(axis=2) < 3 * 255)
                img[mask] = overlay[mask]

        # 3) Draw robots as filled circles
        with profile("env_rgb_draw_robots", track_memory=True):
            radius = cell_size * float(circle_r_frac)
            for rid, (rr, cc) in board.robot_positions.items():
                color = robot_colors[rid % len(robot_colors)]
                cy = rr * cell_size + cell_size * 0.5
                cx = cc * cell_size + cell_size * 0.5
                draw_circle(cx, cy, radius, color, circle_fill)

        # 4) Draw target as a star in darker color of target robot
        with profile("env_rgb_draw_target", track_memory=True):
            tr = board.target_robot
            tr_color = robot_colors[tr % len(robot_colors)]
            dark_color = tuple(int(max(0, min(255, target_dark_factor * v))) for v in tr_color)
            gr, gc = board.goal_position
            cy = gr * cell_size + cell_size * 0.5
            cx = gc * cell_size + cell_size * 0.5
            draw_star(cx, cy, arm_len=cell_size * 0.35, color=dark_color, thickness=star_th)

        return img

    def _render_rgb_cached(self, board: Board, cell_size: int) -> np.ndarray:
        # Compose cached background + internal walls overlay + dynamic robots/target into board-sized image
        cfg = self._render_rgb_cfg
        grid_color = tuple(int(x) for x in cfg["grid_color"])  # type: ignore[index]
        grid_th = int(cfg["grid_thickness"])  # px
        wall_color = tuple(int(x) for x in cfg["wall_color"])  # type: ignore[index]
        wall_th = int(cfg["wall_thickness"])  # px
        robot_colors: List[Tuple[int, int, int]] = [tuple(int(a) for a in t) for t in cfg["robot_colors"]]  # type: ignore[index]
        circle_fill: bool = bool(cfg["circle_fill"])  # type: ignore[index]
        circle_r_frac: float = float(cfg["circle_radius_frac"])  # type: ignore[index]
        target_dark_factor: float = float(cfg["target_dark_factor"])  # type: ignore[index]
        star_th: int = int(cfg["star_thickness"])  # type: ignore[index]

        H, W = board.height, board.width
        H_px, W_px = H * cell_size, W * cell_size
        bg = self._get_rgb_background(H, W, cell_size, grid_color, grid_th, wall_color, wall_th)
        img = bg.copy()
        if self._rgb_internal_walls_overlay is None or self._rgb_internal_walls_overlay.shape[:2] != (H_px, W_px):
            self._build_rgb_internal_walls_overlay(board, cell_size_override=cell_size)
        if self._rgb_internal_walls_overlay is not None:
            overlay = self._rgb_internal_walls_overlay
            mask = (overlay.sum(axis=2) < 3 * 255)
            img[mask] = overlay[mask]

        # Inline draw helpers for robots/target
        def draw_hline(y: int, x0: int, x1: int, color: Tuple[int, int, int], thickness: int) -> None:
            y0 = max(0, y - thickness // 2)
            y1 = min(H_px, y + (thickness - thickness // 2))
            x0c = max(0, min(x0, x1))
            x1c = min(W_px, max(x0, x1))
            img[y0:y1, x0c:x1c] = color

        def draw_vline(x: int, y0: int, y1: int, color: Tuple[int, int, int], thickness: int) -> None:
            x0 = max(0, x - thickness // 2)
            x1 = min(W_px, x + (thickness - thickness // 2))
            y0c = max(0, min(y0, y1))
            y1c = min(H_px, max(y0, y1))
            img[y0c:y1c, x0:x1] = color

        def draw_circle(cx: float, cy: float, radius: float, color: Tuple[int, int, int], fill: bool) -> None:
            x0 = int(max(0, np.floor(cx - radius)))
            x1 = int(min(W_px - 1, np.ceil(cx + radius)))
            y0 = int(max(0, np.floor(cy - radius)))
            y1 = int(min(H_px - 1, np.ceil(cy + radius)))
            yy, xx = np.ogrid[y0:y1 + 1, x0:x1 + 1]
            mask = (xx - cx) ** 2 + (yy - cy) ** 2 <= radius ** 2
            if fill:
                img[y0:y1 + 1, x0:x1 + 1][mask] = color
            else:
                inner = (xx - cx) ** 2 + (yy - cy) ** 2 <= (radius - 1) ** 2
                ring = np.logical_and(mask, np.logical_not(inner))
                img[y0:y1 + 1, x0:x1 + 1][ring] = color

        def draw_star(cx: float, cy: float, arm_len: float, color: Tuple[int, int, int], thickness: int) -> None:
            draw_hline(int(cy), int(cx - arm_len), int(cx + arm_len), color, thickness)
            draw_vline(int(cx), int(cy - arm_len), int(cy + arm_len), color, thickness)
            steps = int(arm_len)
            for s in range(-steps, steps + 1):
                x = int(cx + s)
                y1 = int(cy + s)
                y2 = int(cy - s)
                y0a = max(0, y1 - thickness // 2)
                y1a = min(H_px, y1 + (thickness - thickness // 2))
                x0a = max(0, x - thickness // 2)
                x1a = min(W_px, x + (thickness - thickness // 2))
                img[y0a:y1a, x0a:x1a] = color
                y0b = max(0, y2 - thickness // 2)
                y1b = min(H_px, y2 + (thickness - thickness // 2))
                img[y0b:y1b, x0a:x1a] = color

        # Draw robots
        radius = cell_size * float(circle_r_frac)
        for rid, (rr, cc) in board.robot_positions.items():
            color = robot_colors[rid % len(robot_colors)]
            cy = rr * cell_size + cell_size * 0.5
            cx = cc * cell_size + cell_size * 0.5
            draw_circle(cx, cy, radius, color, circle_fill)

        # Draw target star
        tr = board.target_robot
        tr_color = robot_colors[tr % len(robot_colors)]
        dark_color = tuple(int(max(0, min(255, target_dark_factor * v))) for v in tr_color)
        gr, gc = board.goal_position
        cy = gr * cell_size + cell_size * 0.5
        cx = gc * cell_size + cell_size * 0.5
        draw_star(cx, cy, arm_len=cell_size * 0.35, color=dark_color, thickness=star_th)
        return img

    def _get_rgb_background(self, H: int, W: int, cell_size: int, grid_color: Tuple[int, int, int], grid_th: int, wall_color: Tuple[int, int, int], wall_th: int) -> np.ndarray:
        key = (int(H), int(W), int(cell_size), tuple(int(x) for x in grid_color), int(grid_th), tuple(int(x) for x in wall_color), int(wall_th))
        cached = RicochetRobotsEnv._RGB_BG_CACHE.get(key)
        if cached is not None:
            return cached
        H_px, W_px = int(H * cell_size), int(W * cell_size)
        img = np.ones((H_px, W_px, 3), dtype=np.uint8) * 255
        # Draw grid
        for r in range(H + 1):
            y = int(r * cell_size)
            y0 = max(0, y - grid_th // 2)
            y1 = min(H_px, y + (grid_th - grid_th // 2))
            img[y0:y1, 0:W_px] = grid_color
        for c in range(W + 1):
            x = int(c * cell_size)
            x0 = max(0, x - grid_th // 2)
            x1 = min(W_px, x + (grid_th - grid_th // 2))
            img[0:H_px, x0:x1] = grid_color
        # Draw boundary walls (top/bottom/left/right)
        # Horizontal top and bottom
        y = 0
        y0 = max(0, y - wall_th // 2)
        y1 = min(H_px, y + (wall_th - wall_th // 2))
        img[y0:y1, 0:W_px] = wall_color
        y = H_px
        y0 = max(0, y - wall_th // 2)
        y1 = min(H_px, y + (wall_th - wall_th // 2))
        img[y0:y1, 0:W_px] = wall_color
        # Vertical left and right
        x = 0
        x0 = max(0, x - wall_th // 2)
        x1 = min(W_px, x + (wall_th - wall_th // 2))
        img[0:H_px, x0:x1] = wall_color
        x = W_px
        x0 = max(0, x - wall_th // 2)
        x1 = min(W_px, x + (wall_th - wall_th // 2))
        img[0:H_px, x0:x1] = wall_color
        RicochetRobotsEnv._RGB_BG_CACHE[key] = img
        return img

    def _build_rgb_internal_walls_overlay(self, board: Board, cell_size_override: Optional[int] = None) -> None:
        cfg = self._render_rgb_cfg
        cell_size = int(cell_size_override if cell_size_override is not None else cfg["cell_size"])  # type: ignore[index]
        wall_color = tuple(int(x) for x in cfg["wall_color"])  # type: ignore[index]
        wall_th = int(cfg["wall_thickness"])  # type: ignore[index]
        H, W = board.height, board.width
        H_px, W_px = H * cell_size, W * cell_size
        overlay = np.ones((H_px, W_px, 3), dtype=np.uint8) * 255
        # Draw only interior walls (exclude boundaries at r=0,r=H,c=0,c=W)
        for r in range(1, H):
            for c in range(W):
                if board.h_walls[r, c]:
                    y = int(r * cell_size)
                    y0 = max(0, y - wall_th // 2)
                    y1 = min(H_px, y + (wall_th - wall_th // 2))
                    x0 = int(c * cell_size)
                    x1 = int((c + 1) * cell_size)
                    overlay[y0:y1, x0:x1] = wall_color
        for r in range(H):
            for c in range(1, W):
                if board.v_walls[r, c]:
                    x = int(c * cell_size)
                    x0 = max(0, x - wall_th // 2)
                    x1 = min(W_px, x + (wall_th - wall_th // 2))
                    y0 = int(r * cell_size)
                    y1 = int((r + 1) * cell_size)
                    overlay[y0:y1, x0:x1] = wall_color
        self._rgb_internal_walls_overlay = overlay


def generate_board_from_spec_with_seed(
    *,
    height: int,
    width: int,
    num_robots: int,
    edge_t_per_quadrant: int,
    central_l_per_quadrant: int,
    seed: int,
    _rng: Optional[np.random.Generator] = None,
) -> Board:
    """Generate a Board deterministically from spec and seed without constructing the env.

    If an RNG is provided, it will be used (and its state advanced deterministically);
    otherwise, a fresh Generator seeded with 'seed' is created.
    """
    h, w = int(height), int(width)
    assert h == w, "Boards should always be square per specification"
    rng = _rng if _rng is not None else np.random.default_rng(int(seed))

    # Initialize empty walls then set borders
    h_walls = np.zeros((h + 1, w), dtype=bool)
    v_walls = np.zeros((h, w + 1), dtype=bool)
    h_walls[0, :] = True
    h_walls[h, :] = True
    v_walls[:, 0] = True
    v_walls[:, w] = True

    reserved_cells: Set[Tuple[int, int]] = set()
    def reserve_cell(cell: Tuple[int, int]) -> None:
        r, c = cell
        if 0 <= r < h and 0 <= c < w:
            reserved_cells.add((r, c))

    # Forbidden central block
    if h % 2 == 1:
        mid = h // 2
        h_walls[mid, mid] = True
        h_walls[mid + 1, mid] = True
        v_walls[mid, mid] = True
        v_walls[mid, mid + 1] = True
        reserve_cell((mid, mid))
        central_rows = [mid]
        central_cols = [mid]
    else:
        mid = h // 2
        h_walls[mid-1, mid - 1] = True
        h_walls[mid-1, mid] = True
        h_walls[mid + 1, mid - 1] = True
        h_walls[mid + 1, mid] = True
        v_walls[mid - 1, mid-1] = True
        v_walls[mid, mid-1] = True
        v_walls[mid - 1, mid + 1] = True
        v_walls[mid, mid + 1] = True
        for rr in (mid - 1, mid):
            for cc in (mid - 1, mid):
                reserve_cell((rr, cc))
        central_rows = [mid - 1, mid]
        central_cols = [mid - 1, mid]

    def ranges_excluding_central(size: int, central_idxs: List[int]) -> Tuple[Tuple[int, int], Tuple[int, int]]:
        unique = sorted(set(central_idxs))
        if len(unique) == 1:
            mid_idx = unique[0]
            return (0, mid_idx - 1), (mid_idx + 1, size - 1)
        else:
            low_end = unique[0] - 1
            high_start = unique[1] + 1
            return (0, low_end), (high_start, size - 1)

    (r0_start, r0_end), (r1_start, r1_end) = ranges_excluding_central(h, central_rows)
    (c0_start, c0_end), (c1_start, c1_end) = ranges_excluding_central(w, central_cols)
    quadrants = {
        "NW": (r0_start, r0_end, c0_start, c0_end),
        "NE": (r0_start, r0_end, c1_start, c1_end),
        "SW": (r1_start, r1_end, c0_start, c0_end),
        "SE": (r1_start, r1_end, c1_start, c1_end),
    }

    used_interior_h: Set[Tuple[int, int]] = set()
    used_interior_v: Set[Tuple[int, int]] = set()

    def cell_has_nonboundary_wall(r: int, c: int) -> bool:
        if r > 0 and h_walls[r, c]:
            return True
        if r + 1 < h and h_walls[r + 1, c]:
            return True
        if c > 0 and v_walls[r, c]:
            return True
        if c + 1 < w and v_walls[r, c + 1]:
            return True
        return False

    def neighbors4(r: int, c: int) -> List[Tuple[int, int]]:
        nbrs: List[Tuple[int, int]] = []
        if r - 1 >= 0:
            nbrs.append((r - 1, c))
        if r + 1 < h:
            nbrs.append((r + 1, c))
        if c - 1 >= 0:
            nbrs.append((r, c - 1))
        if c + 1 < w:
            nbrs.append((r, c + 1))
        return nbrs

    def can_place_h(row: int, col: int) -> bool:
        if not (1 <= row <= h - 1 and 0 <= col <= w - 1):
            return False
        if h_walls[row, col]:
            return False
        neighbors = [
            (row - 1, col), (row + 1, col)
        ]
        for nr, nc in neighbors:
            if 1 <= nr <= h - 1 and 0 <= nc <= w - 1 and h_walls[nr, nc]:
                return False
        return True

    def can_place_v(row: int, col: int) -> bool:
        if not (0 <= row <= h - 1 and 1 <= col <= w - 1):
            return False
        if v_walls[row, col]:
            return False
        neighbors = [
            (row, col - 1), (row, col + 1)
        ]
        for nr, nc in neighbors:
            if 0 <= nr <= h - 1 and 1 <= nc <= w - 1 and v_walls[nr, nc]:
                return False
        return True

    def place_edge_t_in_quadrant(q_bounds: Tuple[int, int, int, int]) -> bool:
        rs, re, cs, ce = q_bounds
        if rs > re or cs > ce:
            return False
        choices: List[Tuple[str, Tuple[int, int]]] = []
        if rs == 0:
            for c in range(cs + 1, ce + 1):
                choices.append(("TOP", (0, c)))
        if re == h - 1:
            for c in range(cs + 1, ce + 1):
                choices.append(("BOTTOM", (h - 1, c)))
        if cs == 0:
            for r in range(rs + 1, re + 1):
                choices.append(("LEFT", (r, 0)))
        if ce == w - 1:
            for r in range(rs + 1, re + 1):
                choices.append(("RIGHT", (r, w - 1)))
        if not choices:
            return False
        idxs = np.arange(len(choices))
        rng.shuffle(idxs)
        for idx in idxs:
            side, pos = choices[idx]
            if side in ("TOP", "BOTTOM"):
                r0, c = pos
                if c in (1, w - 1):
                    continue
                if can_place_v(r0, c):
                    adj_cell = (1 if side == "TOP" else h - 2, c - 1)
                    blocked = any(cell_has_nonboundary_wall(rr, cc) for rr, cc in neighbors4(adj_cell[0], adj_cell[1]))
                    if not blocked and (0 <= adj_cell[0] < h and 0 <= adj_cell[1] < w) and adj_cell not in reserved_cells:
                        v_walls[r0, c] = True
                        used_interior_v.add((r0, c))
                        reserve_cell(adj_cell)
                        return True
            else:
                r, c0 = pos
                if r in (1, h - 1):
                    continue
                if can_place_h(r, c0):
                    adj_cell = (r - 1, 1 if side == "LEFT" else w - 2)
                    blocked = any(cell_has_nonboundary_wall(rr, cc) for rr, cc in neighbors4(adj_cell[0], adj_cell[1]))
                    if not blocked and (0 <= adj_cell[0] < h and 0 <= adj_cell[1] < w) and adj_cell not in reserved_cells:
                        h_walls[r, c0] = True
                        used_interior_h.add((r, c0))
                        reserve_cell(adj_cell)
                        return True
        return False

    orientations = ["TL", "TR", "BL", "BR"]
    central_l_centers: List[Tuple[int, int]] = []

    def place_central_l_in_quadrant(q_bounds: Tuple[int, int, int, int], desired_orientation: str) -> bool:
        rs, re, cs, ce = q_bounds
        if rs > re or cs > ce:
            return False
        candidates: List[Tuple[int, int]] = []
        for r in range(rs, re + 1):
            for c in range(cs, ce + 1):
                if r == 0 or r == h - 1 or c == 0 or c == w - 1:
                    continue
                if (r, c) in reserved_cells:
                    continue
                if any(cell_has_nonboundary_wall(rr, cc) for rr, cc in neighbors4(r, c)):
                    continue
                candidates.append((r, c))
        if not candidates:
            return False
        idxs = np.arange(len(candidates))
        rng.shuffle(idxs)
        for idx in idxs:
            r, c = candidates[idx]
            need_h: Optional[Tuple[int, int]] = None
            need_v: Optional[Tuple[int, int]] = None
            if desired_orientation == "TL":
                need_h = (r, c)
                need_v = (r, c)
            elif desired_orientation == "TR":
                need_h = (r, c)
                need_v = (r, c + 1)
            elif desired_orientation == "BL":
                need_h = (r + 1, c)
                need_v = (r, c)
            else:  # BR
                need_h = (r + 1, c)
                need_v = (r, c + 1)
            if need_h is None or need_v is None:
                continue
            hr, hc = need_h
            vr, vc = need_v
            if not can_place_h(hr, hc):
                continue
            if not can_place_v(vr, vc):
                continue
            if hr in (0, h) or vc in (0, w):
                continue
            h_walls[hr, hc] = True
            v_walls[vr, vc] = True
            used_interior_h.add((hr, hc))
            used_interior_v.add((vr, vc))
            reserve_cell((r, c))
            central_l_centers.append((r, c))
            return True
        return False

    # 1) Edge-T per quadrant
    for _, bounds in quadrants.items():
        count = int(edge_t_per_quadrant)
        placed = 0
        attempts = 0
        while placed < count and attempts < count * 10:
            if place_edge_t_in_quadrant(bounds):
                placed += 1
            attempts += 1

    # 2) Central-L per quadrant with even orientation distribution
    for _, bounds in quadrants.items():
        count = int(central_l_per_quadrant)
        if count <= 0:
            continue
        orient_list: List[str] = []
        for i in range(count):
            orient_list.append(orientations[i % 4])
        idxs = np.arange(len(orient_list))
        rng.shuffle(idxs)
        orient_list = [orient_list[i] for i in idxs]
        placed = 0
        attempts = 0
        i = 0
        while placed < count and attempts < count * 20 and i < len(orient_list):
            if place_central_l_in_quadrant(bounds, orient_list[i]):
                placed += 1
            i += 1
            attempts += 1

    # Goal inside central-L
    central_set = set((r, c) for r in central_rows for c in central_cols)
    candidates_goal = [rc for rc in central_l_centers if rc not in central_set]
    if not candidates_goal:
        candidates_goal = [(r, c) for r in range(h) for c in range(w) if (r, c) not in central_set]
    idx = int(rng.integers(0, len(candidates_goal)))
    goal = candidates_goal[idx]

    # Robots
    free_cells: List[Tuple[int, int]] = []
    for r in range(h):
        for c in range(w):
            if (r, c) in central_set or (r, c) == goal:
                continue
            free_cells.append((r, c))
    rng.shuffle(free_cells)
    robot_positions: Dict[int, Tuple[int, int]] = {}
    for rid in range(int(num_robots)):
        robot_positions[rid] = free_cells.pop()
    target_robot = int(rng.integers(0, int(num_robots)))
    return Board(height=h, width=w, h_walls=h_walls, v_walls=v_walls, robot_positions=robot_positions, goal_position=goal, target_robot=target_robot)


# Optional helper for registration with gymnasium
def register_env(env_id: str = "RicochetRobots-v0", *, max_episode_steps: Optional[int] = None) -> None:
    """Register this environment with gymnasium so it can be created via gym.make(env_id).

    Note: This env already enforces a time limit via its max_steps argument. If you also set
    max_episode_steps here and wrap with Gym's TimeLimit implicitly via gym.make, you may see
    duplicate truncation behavior. Prefer one source of time limits.
    """
    try:
        from gymnasium.envs.registration import register  # type: ignore
        from gymnasium.error import Error as GymError  # type: ignore
        try:
            kwargs = {}
            if max_episode_steps is not None:
                kwargs["max_episode_steps"] = int(max_episode_steps)
            register(
                id=env_id,
                entry_point="env.ricochet_env:RicochetRobotsEnv",
                **kwargs,
            )
        except GymError as e:  # pragma: no cover - depends on gym version behavior
            # Swallow duplicate registration errors; re-raise others
            if "already registered" in str(e).lower():
                return
            raise
    except (ModuleNotFoundError, ImportError):
        # Silently ignore if gymnasium not available
        pass

```

I updated the RGB overlay to draw on a white canvas and overwrite via a non-white mask. Please restart your Python kernel and rerun the cells so `env/ricochet_env.py` changes are picked up.

If you still don’t see walls, let’s quickly verify whether they exist (generation) vs rendering:

- Add this cell to your preview script and run it after sampling a `metadata`:
```python
#%% Quick wall diagnostics for the last sample 'md'
from env.puzzle_bank import create_fixed_layout_from_seed
from env.ricochet_env import RicochetRobotsEnv

fixed = create_fixed_layout_from_seed(md)
print("Internal h walls present:", bool(fixed.h_walls[1:-1, :].any()))
print("Internal v walls present:", bool(fixed.v_walls[:, 1:-1].any()))

env = RicochetRobotsEnv(fixed_layout=fixed, obs_mode="rgb_image", channels_first=False, render_mode="rgb")
obs, _ = env.reset()
rgb = env.render()  # full renderer
env.close()

import matplotlib.pyplot as plt
plt.figure(figsize=(5,5)); plt.imshow(rgb); plt.title("RGB render"); plt.axis("off"); plt.show()
```

- If the prints show True/True but the image still lacks walls, it’s rendering; if they’re False/False, it’s generation or the bank contents for that spec lack internal walls.

Let me know the True/False outputs and I’ll adjust accordingly.

```python
#%% Quick wall diagnostics for the last sample 'md'
from env.puzzle_bank import create_fixed_layout_from_seed
from env.ricochet_env import RicochetRobotsEnv

fixed = create_fixed_layout_from_seed(md)
print("Internal h walls present:", bool(fixed.h_walls[1:-1, :].any()))
print("Internal v walls present:", bool(fixed.v_walls[:, 1:-1].any()))

env = RicochetRobotsEnv(fixed_layout=fixed, obs_mode="rgb_image", channels_first=False, render_mode="rgb")
obs, _ = env.reset()
rgb = env.render()  # full renderer
env.close()

import matplotlib.pyplot as plt
plt.figure(figsize=(5,5)); plt.imshow(rgb); plt.title("RGB render"); plt.axis("off"); plt.show()
```

---

**User**

Okay this is wonderful - now, lets get back to the review task. Good work reviewing the implementation of the latest work. Please can you now go ahead and start making the changes you recommended in the work in progress doc? Make sure to keep track of what you're doing, under a 'progress' subsection within your 'review #1' section.

---

**Cursor**

I'll create a structured todo list for the implementation tasks and begin with wiring solver metadata, implementing layout serialization/deserialization, adding an LRU cache to the bank sampler, and making fallback optional in the criteria-filtered env. I'll update progress in WorkInProgress.md as I complete each step.

I’m going to implement multiple improvements in one go: add layout serialization/deserialization and use it during precompute, wire solver metadata into the precompute pipeline, add LRU eviction and reallocation in the bank sampler, make fallback optional in the criteria-filtered environment, and clarify the precompute override naming. I’ll then add a “Progress” subsection under “Review #1” in WorkInProgress.md.

```python
"""
Puzzle Bank System for Curriculum Learning

This module provides a storage and retrieval system for precomputed Ricochet Robots
puzzles with their solution metadata. The bank stores puzzles in a lightweight format
using seeds and generation specs, with solution traces for deriving additional features.
"""

from __future__ import annotations

import hashlib
import json
import time
from dataclasses import dataclass, asdict
import struct
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Iterator, Any, Union
import numpy as np
try:
    import pandas as pd
    import pyarrow as pa
    import pyarrow.parquet as pq
except ImportError:
    # Fallback for environments without pyarrow
    import pandas as pd
    pa = None
    pq = None
from collections import defaultdict

from .ricochet_core import Board, apply_action, reached_goal
from .ricochet_env import RicochetRobotsEnv, FixedLayout, generate_board_from_spec_with_seed


@dataclass(frozen=True)
class SpecKey:
    """Specification key that fully determines board generation given a seed."""
    height: int
    width: int
    num_robots: int
    edge_t_per_quadrant: int
    central_l_per_quadrant: int
    generator_rules_version: str = "1.0"
    
    def to_partition_key(self) -> str:
        """Generate partition key for storage organization."""
        return f"size-{self.height}x{self.width}_numR{self.num_robots}_ET{self.edge_t_per_quadrant}_CL{self.central_l_per_quadrant}_gen{self.generator_rules_version.replace('.', '')}"


@dataclass(frozen=True)
class SolverKey:
    """Solver configuration key for solution canonicalization."""
    algorithm: str = "BFS"
    neighbor_order_id: str = "canonical"  # canonical ordering for deterministic results
    max_depth: int = 50
    max_nodes: int = 100000
    solver_version: str = "1.0"
    
    def to_string(self) -> str:
        """String representation for indexing."""
        return f"{self.algorithm}_{self.neighbor_order_id}_d{self.max_depth}_n{self.max_nodes}_v{self.solver_version.replace('.', '')}"


@dataclass
class PuzzleMetadata:
    """Metadata for a single puzzle in the bank."""
    # Core identifiers
    seed: int
    spec_key: SpecKey
    solver_key: SolverKey
    board_hash: bytes  # blake3 hash of full board state (including target_robot)
    
    # Solution metrics
    optimal_length: int
    robots_moved: int  # count of distinct robot IDs in optimal plan
    nodes_expanded: int
    solve_time_ms: int
    
    # Flags
    solved_within_limits: bool
    hit_depth_limit: bool
    hit_node_limit: bool
    
    # Plan trace (for deriving additional features)
    actions_encoded: List[int]  # action = robot_id * 4 + direction

    # Metadata
    created_at: float
    run_id: str

    # Extended reproducibility
    board_seed: Optional[int] = None  # explicit seed used to generate this board
    layout_hash: Optional[bytes] = None  # hash of layout excluding target_robot
    
    # Optional layout blob (for generator drift recovery)
    layout_blob: Optional[bytes] = None
    
    
    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary for Parquet storage."""
        data = asdict(self)
        # Convert SpecKey and SolverKey to JSON strings for proper storage
        data['spec_key'] = json.dumps(asdict(self.spec_key))
        data['solver_key'] = json.dumps(asdict(self.solver_key))
        # Convert bytes to hex for JSON serialization
        data['board_hash'] = self.board_hash.hex()
        if data.get('layout_hash') is not None:
            data['layout_hash'] = self.layout_hash.hex() if isinstance(self.layout_hash, (bytes, bytearray)) else data['layout_hash']
        if self.layout_blob is not None:
            data['layout_blob'] = self.layout_blob.hex()
        return data
    
    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> PuzzleMetadata:
        """Create from dictionary loaded from Parquet."""
        # Convert back from hex - handle potential NaN/None values
        board_hash = data.get('board_hash')
        if board_hash and isinstance(board_hash, str) and board_hash.strip() and board_hash != 'nan':
            try:
                data['board_hash'] = bytes.fromhex(board_hash)
            except ValueError:
                raise ValueError(f"Invalid board_hash hex string: {board_hash}")
        else:
            raise ValueError("board_hash is required and cannot be empty")
        
        # Handle layout_blob - may be None, NaN, or empty string
        layout_blob = data.get('layout_blob')
        if (layout_blob and isinstance(layout_blob, str) and 
            layout_blob.strip() and layout_blob != 'nan'):
            try:
                data['layout_blob'] = bytes.fromhex(layout_blob)
            except ValueError:
                # Invalid hex string, treat as None
                data['layout_blob'] = None
        else:
            data['layout_blob'] = None

        # Optional fields: board_seed, layout_hash
        if 'board_seed' in data and data['board_seed'] == '':
            data['board_seed'] = None
        if 'layout_hash' in data:
            lh = data.get('layout_hash')
            if lh and isinstance(lh, str) and lh.strip() and lh != 'nan':
                try:
                    data['layout_hash'] = bytes.fromhex(lh)
                except ValueError:
                    data['layout_hash'] = None
            else:
                data['layout_hash'] = None
        
        # Reconstruct SpecKey and SolverKey - handle both dict and string formats
        import ast
        spec_key_data = data['spec_key']
        if isinstance(spec_key_data, dict):
            # Already a dictionary, use as-is
            pass
        elif isinstance(spec_key_data, str):
            try:
                # Try JSON first (for properly formatted JSON strings)
                spec_key_data = json.loads(spec_key_data)
            except json.JSONDecodeError:
                try:
                    # Fallback to ast.literal_eval for Python dict string representation
                    spec_key_data = ast.literal_eval(spec_key_data)
                except (ValueError, SyntaxError) as e:
                    raise ValueError(f"Invalid spec_key format: {spec_key_data}") from e
        else:
            raise ValueError(f"spec_key must be dict or string, got {type(spec_key_data)}")
        data['spec_key'] = SpecKey(**spec_key_data)
        
        solver_key_data = data['solver_key']
        if isinstance(solver_key_data, dict):
            # Already a dictionary, use as-is
            pass
        elif isinstance(solver_key_data, str):
            try:
                # Try JSON first (for properly formatted JSON strings)
                solver_key_data = json.loads(solver_key_data)
            except json.JSONDecodeError:
                try:
                    # Fallback to ast.literal_eval for Python dict string representation
                    solver_key_data = ast.literal_eval(solver_key_data)
                except (ValueError, SyntaxError) as e:
                    raise ValueError(f"Invalid solver_key format: {solver_key_data}") from e
        else:
            raise ValueError(f"solver_key must be dict or string, got {type(solver_key_data)}")
        data['solver_key'] = SolverKey(**solver_key_data)
        
        return cls(**data)


class PuzzleBank:
    """Storage and retrieval system for precomputed puzzles."""
    
    def __init__(self, bank_dir: Union[str, Path], run_id: Optional[str] = None):
        """Initialize puzzle bank.
        
        Args:
            bank_dir: Directory to store the puzzle bank
            run_id: Unique identifier for this computation run
        """
        self.bank_dir = Path(bank_dir)
        self.bank_dir.mkdir(parents=True, exist_ok=True)
        self.run_id = run_id or f"run_{int(time.time())}"
        # In-memory cache of partition DataFrames keyed by partition path
        # This avoids re-reading Parquet/CSV on every query during training
        self._partition_df_cache: Dict[Path, "pd.DataFrame"] = {}
        
        # Create manifest file for tracking partitions
        self.manifest_path = self.bank_dir / "manifest.json"
        self._load_manifest()
    
    def _load_manifest(self) -> None:
        """Load or create manifest file."""
        if self.manifest_path.exists():
            with open(self.manifest_path, 'r', encoding='utf-8') as f:
                self.manifest = json.load(f)
            # Backfill new fields for schema v1.1
            if "version" not in self.manifest or self.manifest.get("version") in {"1.0", 1.0}:
                # Upgrade to 1.1 while preserving existing data
                self.manifest["version"] = "1.1"
            # Ensure partitions dict exists
            self.manifest.setdefault("partitions", {})
            # Ensure new per-partition fields
            for part_key, part_info in self.manifest["partitions"].items():
                part_info.setdefault("histogram", {
                    "by_optimal_length_and_robots_moved": {}
                })
                part_info.setdefault("fingerprints", {})
                part_info.setdefault("dedup", {
                    "method": "none",
                    "bloom": None
                })
        else:
            self.manifest = {
                "version": "1.1",
                "partitions": {},
                "created_at": time.time()
            }
            self._save_manifest()
    
    def _save_manifest(self) -> None:
        """Save manifest file."""
        with open(self.manifest_path, 'w', encoding='utf-8') as f:
            json.dump(self.manifest, f, indent=2)
    
    def _get_partition_path(self, spec_key: SpecKey) -> Path:
        """Get path for a partition."""
        partition_key = spec_key.to_partition_key()
        return self.bank_dir / f"{partition_key}.parquet"
    
    def _load_partition_dataframe(self, partition_path: Path) -> "pd.DataFrame":
        """Load a partition into a pandas DataFrame, using an in-memory cache."""
        # Fast path: cached
        if partition_path in self._partition_df_cache:
            return self._partition_df_cache[partition_path]
        # Load from disk
        if not partition_path.exists():
            df = pd.DataFrame([])
            self._partition_df_cache[partition_path] = df
            return df
        if pa is not None and pq is not None:
            table = pq.read_table(partition_path)
            df = table.to_pandas()
        else:
            df = pd.read_csv(partition_path)
        # Clean NaNs to consistent blanks for string fields
        df = df.fillna('')
        self._partition_df_cache[partition_path] = df
        return df
    
    def get_partition_dataframe(self, spec_key: SpecKey) -> "pd.DataFrame":
        """Public helper to access a cached DataFrame for a spec partition."""
        partition_path = self._get_partition_path(spec_key)
        return self._load_partition_dataframe(partition_path)
    
    def add_puzzles(self, puzzles: List[PuzzleMetadata]) -> None:
        """Add puzzles to the bank.
        
        Args:
            puzzles: List of puzzle metadata to add
        """
        if not puzzles:
            return
        
        # Group by spec_key for partitioning
        by_spec = defaultdict(list)
        for puzzle in puzzles:
            by_spec[puzzle.spec_key].append(puzzle)
        
        for spec_key, spec_puzzles in by_spec.items():
            partition_path = self._get_partition_path(spec_key)
            
            # Convert to DataFrame
            data = [puzzle.to_dict() for puzzle in spec_puzzles]
            df = pd.DataFrame(data)
            
            if pa is not None and pq is not None:
                # Create Arrow table
                table = pa.Table.from_pandas(df)
                
                # Write to Parquet (append if exists)
                if partition_path.exists():
                    existing_table = pq.read_table(partition_path)
                    combined_table = pa.concat_tables([existing_table, table])
                    pq.write_table(combined_table, partition_path)
                else:
                    pq.write_table(table, partition_path)
            else:
                # Fallback to CSV if pyarrow not available
                if partition_path.exists():
                    existing_df = pd.read_csv(partition_path)
                    combined_df = pd.concat([existing_df, df], ignore_index=True)
                    combined_df.to_csv(partition_path, index=False)
                else:
                    df.to_csv(partition_path, index=False)
            
            # Update manifest
            partition_key = spec_key.to_partition_key()
            if partition_key not in self.manifest["partitions"]:
                self.manifest["partitions"][partition_key] = {
                    "spec_key": asdict(spec_key),
                    "created_at": time.time(),
                    "puzzle_count": 0,
                    "histogram": {
                        "by_optimal_length_and_robots_moved": {}
                    },
                    "fingerprints": {},
                    "dedup": {
                        "method": "none",
                        "bloom": None
                    }
                }
            self.manifest["partitions"][partition_key]["puzzle_count"] += len(spec_puzzles)
            self.manifest["partitions"][partition_key]["last_updated"] = time.time()

            # Increment histograms for (optimal_length, robots_moved)
            hist = self.manifest["partitions"][partition_key]["histogram"]["by_optimal_length_and_robots_moved"]
            for puzzle in spec_puzzles:
                ol = int(getattr(puzzle, "optimal_length", -1))
                rm = int(getattr(puzzle, "robots_moved", -1))
                key = f"ol={ol}|rm={rm}"
                hist[key] = int(hist.get(key, 0)) + 1
        
        self._save_manifest()
    
    def query_puzzles(
        self,
        spec_key: Optional[SpecKey] = None,
        min_optimal_length: Optional[int] = None,
        max_optimal_length: Optional[int] = None,
        min_robots_moved: Optional[int] = None,
        max_robots_moved: Optional[int] = None,
        limit: Optional[int] = None,
        random_seed: Optional[int] = None
    ) -> Iterator[PuzzleMetadata]:
        """Query puzzles matching criteria.
        
        Args:
            spec_key: Filter by generation spec
            min_optimal_length: Minimum optimal solution length
            max_optimal_length: Maximum optimal solution length
            min_robots_moved: Minimum number of robots moved
            max_robots_moved: Maximum number of robots moved
            limit: Maximum number of results
            random_seed: Random seed for reproducible sampling (None for random)
            
        Yields:
            PuzzleMetadata matching criteria
        """
        # Determine which partitions to search
        if spec_key is not None:
            partition_paths = [self._get_partition_path(spec_key)]
        else:
            partition_paths = [
                self.bank_dir / f"{partition_key}.parquet"
                for partition_key in self.manifest["partitions"].keys()
            ]
        
        count = 0
        for partition_path in partition_paths:
            # Load via cache (empty DataFrame if not present)
            df = self._load_partition_dataframe(partition_path)
            
            # Apply filters
            if len(df) == 0:
                continue
            mask = pd.Series([True] * len(df))
            
            if min_optimal_length is not None:
                mask &= df['optimal_length'] >= min_optimal_length
            if max_optimal_length is not None:
                mask &= df['optimal_length'] <= max_optimal_length
            if min_robots_moved is not None:
                mask &= df['robots_moved'] >= min_robots_moved
            if max_robots_moved is not None:
                mask &= df['robots_moved'] <= max_robots_moved
            
            # Convert filtered rows to PuzzleMetadata
            filtered_df = df[mask]
            
            # Shuffle the filtered data for random sampling
            if len(filtered_df) > 0:
                filtered_df = filtered_df.sample(frac=1.0, random_state=random_seed).reset_index(drop=True)
            
            for _, row in filtered_df.iterrows():
                if limit is not None and count >= limit:
                    return
                
                puzzle = PuzzleMetadata.from_dict(row.to_dict())
                yield puzzle
                count += 1
    
    def get_puzzle_count(self, spec_key: Optional[SpecKey] = None) -> int:
        """Get total number of puzzles in bank.
        
        Args:
            spec_key: If provided, count only for this spec
            
        Returns:
            Number of puzzles
        """
        if spec_key is not None:
            partition_key = spec_key.to_partition_key()
            return self.manifest["partitions"].get(partition_key, {}).get("puzzle_count", 0)
        else:
            return sum(
                partition.get("puzzle_count", 0)
                for partition in self.manifest["partitions"].values()
            )
    
    def get_stats(self) -> Dict[str, Any]:
        """Get bank statistics.
        
        Returns:
            Dictionary with bank statistics
        """
        total_puzzles = self.get_puzzle_count()
        partition_count = len(self.manifest["partitions"])
        
        # Get distribution stats
        all_puzzles = list(self.query_puzzles(limit=10000))  # Sample for stats
        if all_puzzles:
            optimal_lengths = [p.optimal_length for p in all_puzzles]
            robots_moved = [p.robots_moved for p in all_puzzles]
            
            stats = {
                "total_puzzles": total_puzzles,
                "partition_count": partition_count,
                "optimal_length_stats": {
                    "min": min(optimal_lengths),
                    "max": max(optimal_lengths),
                    "mean": np.mean(optimal_lengths),
                    "std": np.std(optimal_lengths)
                },
                "robots_moved_stats": {
                    "min": min(robots_moved),
                    "max": max(robots_moved),
                    "mean": np.mean(robots_moved),
                    "std": np.std(robots_moved)
                }
            }
        else:
            stats = {
                "total_puzzles": total_puzzles,
                "partition_count": partition_count,
                "optimal_length_stats": None,
                "robots_moved_stats": None
            }
        
        return stats


class BankSampler:
    """Sampler for retrieving puzzles from the bank during training."""
    
    def __init__(self, bank: PuzzleBank, sampling_strategy: str = "uniform"):
        """Initialize bank sampler.
        
        Args:
            bank: Puzzle bank to sample from
            sampling_strategy: Strategy for sampling ("uniform", "stratified")
        """
        self.bank = bank
        self.sampling_strategy = sampling_strategy
        # Cache of pre-filtered, shuffled views for fast per-episode sampling
        # key -> { 'df': DataFrame, 'order': np.ndarray[int], 'pos': int, 'rng': np.random.Generator }
        self._criteria_cache: Dict[Tuple[str, int, int, int, int], Dict[str, Any]] = {}
        # Monitoring
        self._last_stratified_plan: Optional[Dict[str, Any]] = None

    def _criteria_key(
        self,
        spec_key: SpecKey,
        min_optimal_length: Optional[int],
        max_optimal_length: Optional[int],
        min_robots_moved: Optional[int],
        max_robots_moved: Optional[int],
    ) -> Tuple[str, int, int, int, int]:
        # Use partition string plus numeric bounds (None -> sentinel -1)
        part = spec_key.to_partition_key()
        def _n(v: Optional[int]) -> int:
            return -1 if v is None else int(v)
        return (part, _n(min_optimal_length), _n(max_optimal_length), _n(min_robots_moved), _n(max_robots_moved))

    def _get_or_build_iterator(
        self,
        spec_key: SpecKey,
        min_optimal_length: Optional[int],
        max_optimal_length: Optional[int],
        min_robots_moved: Optional[int],
        max_robots_moved: Optional[int],
        random_seed: Optional[int],
    ) -> Optional[Dict[str, Any]]:
        key = self._criteria_key(spec_key, min_optimal_length, max_optimal_length, min_robots_moved, max_robots_moved)
        it = self._criteria_cache.get(key)
        if it is None:
            # Build filtered DataFrame from cached partition
            df = self.bank.get_partition_dataframe(spec_key)
            if len(df) == 0:
                return None
            mask = pd.Series([True] * len(df))
            if min_optimal_length is not None:
                mask &= df['optimal_length'] >= min_optimal_length
            if max_optimal_length is not None:
                mask &= df['optimal_length'] <= max_optimal_length
            if min_robots_moved is not None:
                mask &= df['robots_moved'] >= min_robots_moved
            if max_robots_moved is not None:
                mask &= df['robots_moved'] <= max_robots_moved
            filtered = df[mask]
            if len(filtered) == 0:
                return None
            # Initialize RNG and shuffled order
            rng = np.random.default_rng(random_seed)
            order = np.arange(len(filtered))
            rng.shuffle(order)
            it = {"df": filtered.reset_index(drop=True), "order": order, "pos": 0, "rng": rng}
            self._criteria_cache[key] = it
        return it
    
    def _next_row(self, it: Dict[str, Any]) -> Optional[Dict[str, Any]]:
        df = it["df"]
        order = it["order"]
        pos = it["pos"]
        if pos >= len(order):
            # reshuffle and reset
            it["rng"].shuffle(order)
            pos = 0
        if len(order) == 0:
            return None
        idx = int(order[pos])
        it["pos"] = pos + 1
        row = df.iloc[idx]
        return row.to_dict()
    
    def sample_puzzle(
        self,
        spec_key: SpecKey,
        min_optimal_length: Optional[int] = None,
        max_optimal_length: Optional[int] = None,
        min_robots_moved: Optional[int] = None,
        max_robots_moved: Optional[int] = None,
        random_seed: Optional[int] = None
    ) -> Optional[PuzzleMetadata]:
        """Sample a single puzzle matching criteria using a cached iterator."""
        it = self._get_or_build_iterator(
            spec_key,
            min_optimal_length,
            max_optimal_length,
            min_robots_moved,
            max_robots_moved,
            random_seed,
        )
        if it is None:
            return None
        data = self._next_row(it)
        if data is None:
            return None
        return PuzzleMetadata.from_dict(data)
    
    def sample_puzzles(
        self,
        spec_key: SpecKey,
        count: int,
        min_optimal_length: Optional[int] = None,
        max_optimal_length: Optional[int] = None,
        min_robots_moved: Optional[int] = None,
        max_robots_moved: Optional[int] = None,
        random_seed: Optional[int] = None
    ) -> List[PuzzleMetadata]:
        """Sample multiple puzzles matching criteria using the cached iterator."""
        it = self._get_or_build_iterator(
            spec_key,
            min_optimal_length,
            max_optimal_length,
            min_robots_moved,
            max_robots_moved,
            random_seed,
        )
        if it is None:
            return []
        out: List[PuzzleMetadata] = []
        for _ in range(int(max(0, count))):
            data = self._next_row(it)
            if data is None:
                break
            out.append(PuzzleMetadata.from_dict(data))
        # Future: implement stratified strategy here
        return out

    def sample_puzzles_stratified(
        self,
        spec_key: SpecKey,
        total_count: int,
        bands: List[Dict[str, Any]],
        random_seed: Optional[int] = None,
    ) -> List[PuzzleMetadata]:
        """Stratified sampling across bands defined by (ol, rm) ranges with weights.

        Args:
            spec_key: Partition to sample from
            total_count: Total number of puzzles requested
            bands: List of dicts with keys:
                - min_optimal_length, max_optimal_length
                - min_robots_moved, max_robots_moved
                - weight (float) optional; defaults to 1.0
            random_seed: Seed for deterministic allocation and iterators

        Returns:
            List of PuzzleMetadata sampled approximately according to weights
        """
        if total_count <= 0 or len(bands) == 0:
            return []
        rng = np.random.default_rng(random_seed)
        weights = np.array([float(b.get("weight", 1.0)) for b in bands], dtype=float)
        weights = np.maximum(weights, 0.0)
        if weights.sum() <= 0:
            weights = np.ones(len(bands), dtype=float)
        weights = weights / weights.sum()

        # Initial integral allocation via floor, then distribute remainder by largest fractional parts
        raw_alloc = weights * int(total_count)
        base_alloc = np.floor(raw_alloc).astype(int)
        remainder = int(total_count) - int(base_alloc.sum())
        frac = raw_alloc - base_alloc
        if remainder > 0:
            order = np.argsort(-frac)
            for idx in order[:remainder]:
                base_alloc[idx] += 1

        plan_details: List[Dict[str, Any]] = []
        out: List[PuzzleMetadata] = []
        actually_sampled = 0
        for i, band in enumerate(bands):
            req = int(base_alloc[i])
            if req <= 0:
                continue
            min_ol = band.get("min_optimal_length")
            max_ol = band.get("max_optimal_length")
            min_rm = band.get("min_robots_moved")
            max_rm = band.get("max_robots_moved")
            # Use separate iterator per band
            it = self._get_or_build_iterator(
                spec_key,
                min_ol,
                max_ol,
                min_rm,
                max_rm,
                int(rng.integers(0, 2**31 - 1)) if random_seed is None else random_seed,
            )
            band_samples: List[PuzzleMetadata] = []
            if it is not None:
                for _ in range(req):
                    data = self._next_row(it)
                    if data is None:
                        break
                    band_samples.append(PuzzleMetadata.from_dict(data))
            out.extend(band_samples)
            actually_sampled += len(band_samples)
            plan_details.append({
                "band": {
                    "min_optimal_length": min_ol,
                    "max_optimal_length": max_ol,
                    "min_robots_moved": min_rm,
                    "max_robots_moved": max_rm,
                    "weight": float(band.get("weight", 1.0)),
                },
                "requested": req,
                "sampled": len(band_samples),
            })

        # Monitoring snapshot for external logging
        self._last_stratified_plan = {
            "total_requested": int(total_count),
            "total_sampled": int(actually_sampled),
            "bands": plan_details,
        }
        return out

    def get_last_stratified_plan(self) -> Optional[Dict[str, Any]]:
        """Return details of the last stratified sampling plan and outcomes."""
        return self._last_stratified_plan


def compute_board_hash(board: Board) -> bytes:
    """Compute deterministic binary hash of a board state.
    
    This avoids JSON serialization entirely and instead hashes a compact
    binary representation composed of geometry, walls, robots, goal and target.
    """
    buf = bytearray()
    # Dimensions
    buf += struct.pack('<II', int(board.height), int(board.width))
    # Walls (raw bytes, deterministic layout)
    buf += board.h_walls.tobytes()
    buf += board.v_walls.tobytes()
    # Robots: sorted by robot id
    for rid in sorted(board.robot_positions.keys()):
        r, c = board.robot_positions[rid]
        buf += struct.pack('<III', int(rid), int(r), int(c))
    # Goal and target
    gr, gc = board.goal_position
    buf += struct.pack('<III', int(board.target_robot), int(gr), int(gc))
    # Compute hash (use SHA-256 as fallback if blake3 not available)
    try:
        return hashlib.blake3(buf).digest()
    except AttributeError:
        return hashlib.sha256(buf).digest()


def compute_layout_hash(board: Board) -> bytes:
    """Compute deterministic binary hash of the board layout excluding target_robot.

    Includes dimensions, walls, robots positions, and goal position, but excludes
    which robot is target to allow multi-round episodes on same layout.
    """
    buf = bytearray()
    buf += struct.pack('<II', int(board.height), int(board.width))
    buf += board.h_walls.tobytes()
    buf += board.v_walls.tobytes()
    for rid in sorted(board.robot_positions.keys()):
        r, c = board.robot_positions[rid]
        buf += struct.pack('<III', int(rid), int(r), int(c))
    gr, gc = board.goal_position
    buf += struct.pack('<II', int(gr), int(gc))
    try:
        return hashlib.blake3(buf).digest()
    except AttributeError:
        return hashlib.sha256(buf).digest()


def count_robots_moved(actions_encoded: List[int]) -> int:
    """Count number of distinct robots moved in action sequence.
    
    Args:
        actions_encoded: List of encoded actions (robot_id * 4 + direction)
        
    Returns:
        Number of distinct robots moved
    """
    robot_ids = set()
    for action in actions_encoded:
        robot_id = action // 4
        robot_ids.add(robot_id)
    return len(robot_ids)


def create_fixed_layout_from_metadata(metadata: PuzzleMetadata) -> FixedLayout:
    """Create FixedLayout from puzzle metadata.
    
    This requires the layout_blob to be present in the metadata.
    
    Args:
        metadata: Puzzle metadata with layout_blob
        
    Returns:
        FixedLayout for use with environment
        
    Raises:
        ValueError: If layout_blob is not available
    """
    if metadata.layout_blob is None:
        raise ValueError("Cannot create FixedLayout without layout_blob")
    return deserialize_layout_blob(metadata.layout_blob)


def create_fixed_layout_from_seed(metadata: PuzzleMetadata) -> FixedLayout:
    """Create FixedLayout by regenerating board from seed.
    
    Args:
        metadata: Puzzle metadata with seed and spec_key
        
    Returns:
        FixedLayout for use with environment
    """
    # Generate board directly from spec and seed without constructing an environment
    seed_to_use = int(metadata.board_seed) if getattr(metadata, 'board_seed', None) is not None else int(metadata.seed)
    board = generate_board_from_spec_with_seed(
        height=metadata.spec_key.height,
        width=metadata.spec_key.width,
        num_robots=metadata.spec_key.num_robots,
        edge_t_per_quadrant=metadata.spec_key.edge_t_per_quadrant,
        central_l_per_quadrant=metadata.spec_key.central_l_per_quadrant,
        seed=seed_to_use,
    )
    
    # Validate hash matches
    computed_hash = compute_board_hash(board)
    if computed_hash != metadata.board_hash:
        # If layout_hash is available, provide additional diagnostics and try layout-level check
        if getattr(metadata, 'layout_hash', None) is not None:
            comp_layout = compute_layout_hash(board)
            if comp_layout == metadata.layout_hash:
                raise ValueError(f"Board target mismatch for seed {seed_to_use}: layout matches but target_robot differs")
        raise ValueError(f"Board hash mismatch for seed {seed_to_use}")
    
    # Create FixedLayout
    return FixedLayout(
        height=board.height,
        width=board.width,
        h_walls=board.h_walls,
        v_walls=board.v_walls,
        robot_positions=board.robot_positions,
        goal_position=board.goal_position,
        target_robot=board.target_robot
    )


def serialize_layout_to_blob(board: Board) -> bytes:
    """Serialize a Board layout into a compact binary blob.

    Schema (little-endian):
    - uint32 height, uint32 width, uint32 num_robots
    - h_walls bytes ( (H+1)*W, bool packed as uint8 )
    - v_walls bytes ( H*(W+1), bool packed as uint8 )
    - for rid in sorted(robots): uint32 rid, uint32 r, uint32 c
    - uint32 goal_r, uint32 goal_c
    - uint32 target_robot
    """
    buf = bytearray()
    h = int(board.height)
    w = int(board.width)
    buf += struct.pack('<III', h, w, int(board.num_robots))
    buf += board.h_walls.astype(np.uint8).tobytes()
    buf += board.v_walls.astype(np.uint8).tobytes()
    for rid in sorted(board.robot_positions.keys()):
        r, c = board.robot_positions[rid]
        buf += struct.pack('<III', int(rid), int(r), int(c))
    gr, gc = board.goal_position
    buf += struct.pack('<II', int(gr), int(gc))
    buf += struct.pack('<I', int(board.target_robot))
    return bytes(buf)


def deserialize_layout_blob(blob: bytes) -> FixedLayout:
    """Deserialize a layout blob into a FixedLayout."""
    offset = 0
    def read(fmt: str):
        nonlocal offset
        size = struct.calcsize(fmt)
        vals = struct.unpack_from(fmt, blob, offset)
        offset += size
        return vals
    h, w, nrobots = read('<III')
    hw_size = (h + 1) * w
    vw_size = h * (w + 1)
    h_walls = np.frombuffer(blob, dtype=np.uint8, count=hw_size, offset=offset).copy().astype(bool).reshape(h + 1, w)
    offset += hw_size
    v_walls = np.frombuffer(blob, dtype=np.uint8, count=vw_size, offset=offset).copy().astype(bool).reshape(h, w + 1)
    offset += vw_size
    robot_positions: Dict[int, Tuple[int, int]] = {}
    for _ in range(int(nrobots)):
        rid, r, c = read('<III')
        robot_positions[int(rid)] = (int(r), int(c))
    gr, gc = read('<II')
    (target_robot,) = read('<I')
    return FixedLayout(
        height=int(h),
        width=int(w),
        h_walls=h_walls,
        v_walls=v_walls,
        robot_positions=robot_positions,
        goal_position=(int(gr), int(gc)),
        target_robot=int(target_robot),
    )

```

I'll update the precompute pipeline to serialize layouts and record solver metadata, add an LRU cache and reallocation to `BankSampler`, make the fallback behavior in `CriteriaFilteredEnv` optional (default disabled), rename the solver depth override parameter, and then record progress in `WorkInProgress.md`.

```python
*** Begin Patch
*** Update File: /Users/sidbaines/Documents/Code/20250911_RLNew/env/precompute_pipeline.py
@@
-from .puzzle_bank import (
-    PuzzleBank, PuzzleMetadata, SpecKey, SolverKey, 
-    compute_board_hash, compute_layout_hash, count_robots_moved
-)
+from .puzzle_bank import (
+    PuzzleBank, PuzzleMetadata, SpecKey, SolverKey, 
+    compute_board_hash, compute_layout_hash, count_robots_moved,
+    serialize_layout_to_blob
+)
-from .solver import solve_bfs, solve_astar
+from .solver import (
+    solve_bfs_with_metadata, solve_astar_with_metadata,
+)
@@
-        self.solver_config = solver_config or {"solver_type": "bfs", "max_depth": 10, "max_nodes": 100000}
+        self.solver_config = solver_config or {"solver_type": "bfs", "max_depth": 10, "max_nodes": 100000}
@@
-        self,
-        spec_key: SpecKey,
-        num_puzzles: int,
-        seed_range: Optional[Tuple[int, int]] = None,
-        batch_size: int = 100,
-        criteria: Optional[Dict[str, int]] = None
+        self,
+        spec_key: SpecKey,
+        num_puzzles: int,
+        seed_range: Optional[Tuple[int, int]] = None,
+        batch_size: int = 100,
+        criteria: Optional[Dict[str, int]] = None
@@
-        # Set criteria for filtering
-        self.criteria = criteria
+        # Set criteria for filtering
+        self.criteria = criteria
@@
-                    max_depth_override = None
-                    if criteria is not None and isinstance(criteria, dict):
-                        max_depth_override = criteria.get('max_depth', None)
-                    puzzle = self._generate_single_puzzle(spec_key, seed, max_depth=max_depth_override)
+                    max_depth_override = None
+                    if criteria is not None and isinstance(criteria, dict):
+                        # Renamed for clarity: solver_max_depth_override
+                        max_depth_override = criteria.get('solver_max_depth_override', criteria.get('max_depth', None))
+                    puzzle = self._generate_single_puzzle(spec_key, seed, max_depth=max_depth_override)
@@
-        self,
-        spec_key: SpecKey,
-        seed: int,
-        max_depth: Optional[int] = None,
-    ) -> Optional[PuzzleMetadata]:
+        self,
+        spec_key: SpecKey,
+        seed: int,
+        max_depth: Optional[int] = None,
+    ) -> Optional[PuzzleMetadata]:
@@
-        # Compute board hash
+        # Compute board hash
         board_hash = compute_board_hash(board)
         # Compute layout hash (target-agnostic) for multi-round reproducibility
         layout_hash = compute_layout_hash(board)
+        # Serialize layout for robust replay across generator versions
+        layout_blob = serialize_layout_to_blob(board)
@@
-        try:
-            solver_type = self.solver_config.get("solver_type", "bfs")
-            max_depth_val = max_depth or self.solver_config['max_depth']
-            max_nodes_val = self.solver_config['max_nodes']
-            
-            if solver_type == "bfs":
-                solution = solve_bfs(
-                    board,
-                    max_depth=max_depth_val,
-                    max_nodes=max_nodes_val
-                )
-            elif solver_type == "astar_zero":
-                solution = solve_astar(
-                    board,
-                    max_depth=max_depth_val,
-                    max_nodes=max_nodes_val,
-                    h_mode="admissible_zero"
-                )
-            elif solver_type == "astar_one":
-                solution = solve_astar(
-                    board,
-                    max_depth=max_depth_val,
-                    max_nodes=max_nodes_val,
-                    h_mode="admissible_one"
-                )
-            else:
-                raise ValueError(f"Unknown solver type: {solver_type}")
-                
-            solve_time = (time.time() - solve_start) * 1000  # Convert to ms
-        except Exception as e:
-            if self.verbose:
-                print(f"  Solver exception: {e}")
-            return None
+        try:
+            solver_type = self.solver_config.get("solver_type", "bfs")
+            max_depth_val = max_depth or self.solver_config['max_depth']
+            max_nodes_val = self.solver_config['max_nodes']
+
+            if solver_type == "bfs":
+                result = solve_bfs_with_metadata(
+                    board,
+                    max_depth=max_depth_val,
+                    max_nodes=max_nodes_val,
+                )
+            elif solver_type == "astar_zero":
+                result = solve_astar_with_metadata(
+                    board,
+                    max_depth=max_depth_val,
+                    max_nodes=max_nodes_val,
+                    h_mode="admissible_zero",
+                )
+            elif solver_type == "astar_one":
+                result = solve_astar_with_metadata(
+                    board,
+                    max_depth=max_depth_val,
+                    max_nodes=max_nodes_val,
+                    h_mode="admissible_one",
+                )
+            else:
+                raise ValueError(f"Unknown solver type: {solver_type}")
+
+            solve_time = (time.time() - solve_start) * 1000  # Convert to ms
+        except Exception as e:
+            if self.verbose:
+                print(f"  Solver exception: {e}")
+            return None
@@
-        if solution is None:
+        if not result.found:
             if self.verbose:
                 print(f"  No solution found")
             return None
@@
-        optimal_length = len(solution)
+        optimal_length = int(result.depth)
         if self.verbose:
             print(f"  Solution found: length={optimal_length}")
@@
-        for act in solution:
+        for act in result.actions:
             if isinstance(act, (list, tuple)) and len(act) == 2:
                 rid, direction = act
                 actions_encoded.append(int(rid) * 4 + int(direction))
             elif isinstance(act, int):
                 actions_encoded.append(int(act))
             else:
                 # Unknown format; skip this puzzle
                 if self.verbose:
                     print(f"  Unknown action format: {act}")
                 return None
@@
-        solved_within_limits = True  # If we got here, it was solved
-        hit_depth_limit = optimal_length >= self.solver_config['max_depth']
-        hit_node_limit = False  # TODO: Track nodes expanded in solver
+        solved_within_limits = True  # We only keep found solutions
+        hit_depth_limit = optimal_length >= self.solver_config['max_depth']
+        hit_node_limit = result.nodes_expanded >= self.solver_config['max_nodes']
@@
         # Create metadata
         metadata = PuzzleMetadata(
             seed=seed,
             spec_key=spec_key,
             solver_key=solver_key,
             board_hash=board_hash,
             board_seed=board_seed,
             layout_hash=layout_hash,
+            layout_blob=layout_blob,
             optimal_length=optimal_length,
             robots_moved=robots_moved,
-            nodes_expanded=0,  # TODO: Track in solver
+            nodes_expanded=int(result.nodes_expanded),
             solve_time_ms=int(solve_time),
             solved_within_limits=solved_within_limits,
             hit_depth_limit=hit_depth_limit,
             hit_node_limit=hit_node_limit,
             actions_encoded=actions_encoded,
-            layout_blob=None,  # TODO: Implement layout blob storage
             created_at=time.time(),
             run_id=self.bank.run_id
         )
         
         return metadata
@@
-                print(f"[controller] Generating chunk for spec {spec_key.to_partition_key()} ({chunk_puzzles_per_spec})")
+                print(f"[controller] Generating chunk for spec {spec_key.to_partition_key()} ({chunk_puzzles_per_spec})")
             stats = generator.generate_puzzles_for_spec(
                 spec_key=spec_key,
                 num_puzzles=chunk_puzzles_per_spec,
                 criteria={
-                    "max_depth": max_depth,
+                    "solver_max_depth_override": max_depth,
                 }
             )
*** End Patch
```

```python
*** Begin Patch
*** Update File: /Users/sidbaines/Documents/Code/20250911_RLNew/env/precompute_pipeline.py
@@
-from .puzzle_bank import (
-    PuzzleBank, PuzzleMetadata, SpecKey, SolverKey, 
-    compute_board_hash, compute_layout_hash, count_robots_moved
-)
+from .puzzle_bank import (
+    PuzzleBank, PuzzleMetadata, SpecKey, SolverKey, 
+    compute_board_hash, compute_layout_hash, count_robots_moved,
+    serialize_layout_to_blob
+)
@@
-from .solver import solve_bfs, solve_astar
+from .solver import solve_bfs_with_metadata, solve_astar_with_metadata
@@
-                    max_depth_override = None
-                    if criteria is not None and isinstance(criteria, dict):
-                        max_depth_override = criteria.get('max_depth', None)
+                    max_depth_override = None
+                    if criteria is not None and isinstance(criteria, dict):
+                        max_depth_override = criteria.get('solver_max_depth_override', criteria.get('max_depth', None))
@@
-        # Compute board hash
+        # Compute board hash
         board_hash = compute_board_hash(board)
         # Compute layout hash (target-agnostic) for multi-round reproducibility
         layout_hash = compute_layout_hash(board)
+        # Serialize layout for robust replay
+        layout_blob = serialize_layout_to_blob(board)
@@
-            solver_type = self.solver_config.get("solver_type", "bfs")
+            solver_type = self.solver_config.get("solver_type", "bfs")
             max_depth_val = max_depth or self.solver_config['max_depth']
             max_nodes_val = self.solver_config['max_nodes']
             
             if solver_type == "bfs":
-                solution = solve_bfs(
-                    board,
-                    max_depth=max_depth_val,
-                    max_nodes=max_nodes_val
-                )
+                result = solve_bfs_with_metadata(board, max_depth=max_depth_val, max_nodes=max_nodes_val)
             elif solver_type == "astar_zero":
-                solution = solve_astar(
-                    board,
-                    max_depth=max_depth_val,
-                    max_nodes=max_nodes_val,
-                    h_mode="admissible_zero"
-                )
+                result = solve_astar_with_metadata(board, max_depth=max_depth_val, max_nodes=max_nodes_val, h_mode="admissible_zero")
             elif solver_type == "astar_one":
-                solution = solve_astar(
-                    board,
-                    max_depth=max_depth_val,
-                    max_nodes=max_nodes_val,
-                    h_mode="admissible_one"
-                )
+                result = solve_astar_with_metadata(board, max_depth=max_depth_val, max_nodes=max_nodes_val, h_mode="admissible_one")
             else:
                 raise ValueError(f"Unknown solver type: {solver_type}")
             
             solve_time = (time.time() - solve_start) * 1000  # Convert to ms
@@
-        if solution is None:
+        if not result or not result.found:
             if self.verbose:
                 print(f"  No solution found")
             return None
@@
-        optimal_length = len(solution)
+        optimal_length = int(result.depth)
         if self.verbose:
             print(f"  Solution found: length={optimal_length}")
@@
-        for act in solution:
+        for act in result.actions:
             if isinstance(act, (list, tuple)) and len(act) == 2:
                 rid, direction = act
                 actions_encoded.append(int(rid) * 4 + int(direction))
             elif isinstance(act, int):
                 actions_encoded.append(int(act))
@@
-        hit_node_limit = False  # TODO: Track nodes expanded in solver
+        hit_node_limit = result.nodes_expanded >= self.solver_config['max_nodes']
@@
         metadata = PuzzleMetadata(
@@
             board_seed=board_seed,
             layout_hash=layout_hash,
+            layout_blob=layout_blob,
             optimal_length=optimal_length,
             robots_moved=robots_moved,
-            nodes_expanded=0,  # TODO: Track nodes expanded in solver
+            nodes_expanded=int(result.nodes_expanded),
             solve_time_ms=int(solve_time),
             solved_within_limits=solved_within_limits,
             hit_depth_limit=hit_depth_limit,
             hit_node_limit=hit_node_limit,
             actions_encoded=actions_encoded,
-            layout_blob=None,  # TODO: Implement layout blob storage
             created_at=time.time(),
             run_id=self.bank.run_id
         )
@@
-                criteria={
-                    "max_depth": max_depth,
-                }
+                criteria={
+                    "solver_max_depth_override": max_depth,
+                }
             )
*** End Patch
```

```python
*** Begin Patch
*** Update File: /Users/sidbaines/Documents/Code/20250911_RLNew/env/precompute_pipeline.py
@@
-from .puzzle_bank import (
-    PuzzleBank, PuzzleMetadata, SpecKey, SolverKey, 
-    compute_board_hash, compute_layout_hash, count_robots_moved
-)
+from .puzzle_bank import (
+    PuzzleBank, PuzzleMetadata, SpecKey, SolverKey, 
+    compute_board_hash, compute_layout_hash, count_robots_moved, serialize_layout_to_blob
+)
@@
-from .solver import solve_bfs, solve_astar
+from .solver import solve_bfs_with_metadata, solve_astar_with_metadata
@@
-                    if criteria is not None and isinstance(criteria, dict):
-                        max_depth_override = criteria.get('max_depth', None)
+                    if criteria is not None and isinstance(criteria, dict):
+                        max_depth_override = criteria.get('solver_max_depth_override', criteria.get('max_depth', None))
@@
-        # Compute layout hash (target-agnostic) for multi-round reproducibility
+        # Compute layout hash (target-agnostic) for multi-round reproducibility
         layout_hash = compute_layout_hash(board)
+        # Serialize layout for robust replay
+        layout_blob = serialize_layout_to_blob(board)
@@
-            if solver_type == "bfs":
-                solution = solve_bfs(
-                    board,
-                    max_depth=max_depth_val,
-                    max_nodes=max_nodes_val
-                )
+            if solver_type == "bfs":
+                result = solve_bfs_with_metadata(board, max_depth=max_depth_val, max_nodes=max_nodes_val)
             elif solver_type == "astar_zero":
-                solution = solve_astar(
-                    board,
-                    max_depth=max_depth_val,
-                    max_nodes=max_nodes_val,
-                    h_mode="admissible_zero"
-                )
+                result = solve_astar_with_metadata(board, max_depth=max_depth_val, max_nodes=max_nodes_val, h_mode="admissible_zero")
             elif solver_type == "astar_one":
-                solution = solve_astar(
-                    board,
-                    max_depth=max_depth_val,
-                    max_nodes=max_nodes_val,
-                    h_mode="admissible_one"
-                )
+                result = solve_astar_with_metadata(board, max_depth=max_depth_val, max_nodes=max_nodes_val, h_mode="admissible_one")
             else:
                 raise ValueError(f"Unknown solver type: {solver_type}")
@@
-        if solution is None:
+        if result is None or not result.found:
             if self.verbose:
                 print(f"  No solution found")
             return None
@@
-        optimal_length = len(solution)
+        optimal_length = int(result.depth)
@@
-        for act in solution:
+        for act in result.actions:
             if isinstance(act, (list, tuple)) and len(act) == 2:
                 rid, direction = act
                 actions_encoded.append(int(rid) * 4 + int(direction))
             elif isinstance(act, int):
                 actions_encoded.append(int(act))
             else:
                 # Unknown format; skip this puzzle
                 if self.verbose:
                     print(f"  Unknown action format: {act}")
                 return None
@@
-        hit_node_limit = False  # TODO: Track nodes expanded in solver
+        hit_node_limit = result.nodes_expanded >= self.solver_config['max_nodes']
@@
         metadata = PuzzleMetadata(
             seed=seed,
             spec_key=spec_key,
             solver_key=solver_key,
             board_hash=board_hash,
             board_seed=board_seed,
             layout_hash=layout_hash,
+            layout_blob=layout_blob,
             optimal_length=optimal_length,
             robots_moved=robots_moved,
-            nodes_expanded=0,  # TODO: Track nodes expanded in solver
+            nodes_expanded=int(result.nodes_expanded),
             solve_time_ms=int(solve_time),
             solved_within_limits=solved_within_limits,
             hit_depth_limit=hit_depth_limit,
             hit_node_limit=hit_node_limit,
             actions_encoded=actions_encoded,
-            layout_blob=None,  # TODO: Implement layout blob storage
             created_at=time.time(),
             run_id=self.bank.run_id
         )
*** End Patch
```

```python
"""
Precomputation Pipeline for Puzzle Bank

This module provides tools for generating and storing puzzles in the bank,
including solver integration and batch processing.
"""

from __future__ import annotations

import time
from pathlib import Path
from typing import List, Optional, Dict, Any, Tuple
import numpy as np
try:
    from tqdm import tqdm
except ImportError:
    # Fallback if tqdm not available
    def tqdm(iterable, *args, **kwargs):
        return iterable

from .puzzle_bank import (
    PuzzleBank, PuzzleMetadata, SpecKey, SolverKey, 
    compute_board_hash, compute_layout_hash, count_robots_moved, serialize_layout_to_blob
)
from .ricochet_env import RicochetRobotsEnv
from .solver import solve_bfs_with_metadata, solve_astar_with_metadata
from .curriculum_config import get_curriculum_specs_for_precomputation


class PuzzleGenerator:
    """Generates puzzles and their solutions for the bank."""
    
    def __init__(
        self,
        bank: PuzzleBank,
        solver_config: Optional[Dict[str, Any]] = None,
        verbose: bool = True
    ):
        """Initialize puzzle generator.
        
        Args:
            bank: Puzzle bank to store results
            solver_config: Solver configuration dictionary
            verbose: Whether to print progress
        """
        self.bank = bank
        self.solver_config = solver_config or {"solver_type": "bfs", "max_depth": 10, "max_nodes": 100000}
        self.verbose = verbose
    
    def generate_puzzles_for_spec(
        self,
        spec_key: SpecKey,
        num_puzzles: int,
        seed_range: Optional[Tuple[int, int]] = None,
        batch_size: int = 100,
        criteria: Optional[Dict[str, int]] = None
    ) -> Dict[str, Any]:
        """Generate puzzles for a specific spec.
        
        Args:
            spec_key: Generation specification
            num_puzzles: Number of puzzles to generate
            seed_range: Optional (min, max) seed range
            batch_size: Number of puzzles to process in each batch
            
        Returns:
            Statistics about generation
        """
        if seed_range is None:
            seed_range = (0, 2**31 - 1)
        
        # Set criteria for filtering
        self.criteria = criteria
        
        start_time = time.time()
        generated = 0
        solved = 0
        failed = 0
        
        if self.verbose:
            print(f"Generating {num_puzzles} puzzles for spec {spec_key.to_partition_key()}")
        
        # Process in batches
        for batch_start in range(0, num_puzzles, batch_size):
            batch_end = min(batch_start + batch_size, num_puzzles)
            batch_size_actual = batch_end - batch_start
            
            # Generate batch
            batch_puzzles = []
            for i in range(batch_size_actual):
                seed = np.random.randint(seed_range[0], seed_range[1])
                
                try:
                    # criteria may be None when called by controller; guard accordingly
                    max_depth_override = None
                    if criteria is not None and isinstance(criteria, dict):
                        max_depth_override = criteria.get('solver_max_depth_override', criteria.get('max_depth', None))
                    puzzle = self._generate_single_puzzle(spec_key, seed, max_depth=max_depth_override)
                    if puzzle is not None:
                        batch_puzzles.append(puzzle)
                        solved += 1
                    else:
                        failed += 1
                except Exception as e:
                    if self.verbose:
                        print(f"Error generating puzzle with seed {seed}: {e}")
                    failed += 1
                
                generated += 1
            
            # Store batch
            if batch_puzzles:
                self.bank.add_puzzles(batch_puzzles)
            
            if self.verbose:
                print(f"Batch {batch_start//batch_size + 1}: {len(batch_puzzles)} puzzles stored")
        
        elapsed = time.time() - start_time
        
        stats = {
            "spec_key": spec_key.to_partition_key(),
            "requested": num_puzzles,
            "generated": generated,
            "solved": solved,
            "failed": failed,
            "success_rate": solved / generated if generated > 0 else 0,
            "elapsed_time": elapsed,
            "puzzles_per_second": generated / elapsed if elapsed > 0 else 0
        }
        
        if self.verbose:
            print(f"Generation complete: {solved}/{generated} solved ({stats['success_rate']:.2%})")
            print(f"Time: {elapsed:.2f}s ({stats['puzzles_per_second']:.1f} puzzles/s)")
        
        return stats
    
    def _generate_single_puzzle(
        self,
        spec_key: SpecKey,
        seed: int,
        max_depth: Optional[int] = None,
    ) -> Optional[PuzzleMetadata]:
        """Generate a single puzzle and its solution.
        
        Args:
            spec_key: Generation specification
            seed: Random seed
            
        Returns:
            PuzzleMetadata if successful, None otherwise
        """
        # Create environment
        env = RicochetRobotsEnv(
            height=spec_key.height,
            width=spec_key.width,
            num_robots=spec_key.num_robots,
            edge_t_per_quadrant=spec_key.edge_t_per_quadrant,
            central_l_per_quadrant=spec_key.central_l_per_quadrant,
            ensure_solvable=False,  # We'll solve manually
            obs_mode="image"
        )
        
        # Generate board
        obs, info = env.reset(seed=seed)
        board = env.get_board()
        env.close()  # Clean up
        
        # Compute board hash
        board_hash = compute_board_hash(board)
        # Compute layout hash (target-agnostic) for multi-round reproducibility
        layout_hash = compute_layout_hash(board)
        # Serialize layout for robust replay
        layout_blob = serialize_layout_to_blob(board)
        # Extract the exact board_seed used by the env if exposed; fallback to used episode seed
        board_seed: Optional[int] = None
        try:
            info_dict = info if isinstance(info, dict) else {}
            bs = info_dict.get("board_seed")
            if bs is not None:
                board_seed = int(bs)
            else:
                es = info_dict.get("episode_seed")
                if es is not None:
                    board_seed = int(es)
        except Exception:
            board_seed = None
        
        # Solve puzzle
        solve_start = time.time()
        try:
            solver_type = self.solver_config.get("solver_type", "bfs")
            max_depth_val = max_depth or self.solver_config['max_depth']
            max_nodes_val = self.solver_config['max_nodes']
            
            if solver_type == "bfs":
                result = solve_bfs_with_metadata(board, max_depth=max_depth_val, max_nodes=max_nodes_val)
            elif solver_type == "astar_zero":
                result = solve_astar_with_metadata(board, max_depth=max_depth_val, max_nodes=max_nodes_val, h_mode="admissible_zero")
            elif solver_type == "astar_one":
                result = solve_astar_with_metadata(board, max_depth=max_depth_val, max_nodes=max_nodes_val, h_mode="admissible_one")
            else:
                raise ValueError(f"Unknown solver type: {solver_type}")
                
            solve_time = (time.time() - solve_start) * 1000  # Convert to ms
        except Exception as e:
            if self.verbose:
                print(f"  Solver exception: {e}")
            return None
        
        if result is None or not result.found:
            if self.verbose:
                print(f"  No solution found")
            return None
        
        # Extract solution metadata
        optimal_length = int(result.depth)
        if self.verbose:
            print(f"  Solution found: length={optimal_length}")
        
        # Normalize solution to encoded integer actions robot_id*4 + direction
        actions_encoded: List[int] = []
        for act in result.actions:
            if isinstance(act, (list, tuple)) and len(act) == 2:
                rid, direction = act
                actions_encoded.append(int(rid) * 4 + int(direction))
            elif isinstance(act, int):
                actions_encoded.append(int(act))
            else:
                # Unknown format; skip this puzzle
                if self.verbose:
                    print(f"  Unknown action format: {act}")
                return None
        robots_moved = count_robots_moved(actions_encoded)
        if self.verbose:
            print(f"  Robots moved: {robots_moved}")
        
        # Check if solution meets criteria (if provided)
        if hasattr(self, 'criteria') and self.criteria:
            if 'min_optimal_length' in self.criteria:
                if not (self.criteria['min_optimal_length'] <= optimal_length <= self.criteria['max_optimal_length']):
                    if self.verbose:
                        print(f"  Rejected: optimal_length {optimal_length} not in range [{self.criteria['min_optimal_length']}, {self.criteria['max_optimal_length']}]")
                    return None
                if not (self.criteria['min_robots_moved'] <= robots_moved <= self.criteria['max_robots_moved']):
                    if self.verbose:
                        print(f"  Rejected: robots_moved {robots_moved} not in range [{self.criteria['min_robots_moved']}, {self.criteria['max_robots_moved']}]")
                    return None
        
        # Determine flags
        solved_within_limits = True  # If we got here, it was solved
        hit_depth_limit = optimal_length >= self.solver_config['max_depth']
        hit_node_limit = result.nodes_expanded >= self.solver_config['max_nodes']
        
        # Map solver type to algorithm name
        solver_type = self.solver_config.get("solver_type", "bfs")
        if solver_type == "bfs":
            algorithm = "BFS"
        elif solver_type == "astar_zero":
            algorithm = "A*_zero"
        elif solver_type == "astar_one":
            algorithm = "A*_one"
        else:
            algorithm = "BFS"  # fallback
        
        # Create solver key with correct algorithm name
        solver_key = SolverKey(
            algorithm=algorithm,
            max_depth=self.solver_config['max_depth'],
            max_nodes=self.solver_config['max_nodes']
        )
        
        # Create metadata
        metadata = PuzzleMetadata(
            seed=seed,
            spec_key=spec_key,
            solver_key=solver_key,
            board_hash=board_hash,
            board_seed=board_seed,
            layout_hash=layout_hash,
            layout_blob=layout_blob,
            optimal_length=optimal_length,
            robots_moved=robots_moved,
            nodes_expanded=int(result.nodes_expanded),
            solve_time_ms=int(solve_time),
            solved_within_limits=solved_within_limits,
            hit_depth_limit=hit_depth_limit,
            hit_node_limit=hit_node_limit,
            actions_encoded=actions_encoded,
            created_at=time.time(),
            run_id=self.bank.run_id
        )
        
        return metadata


class CurriculumSpecGenerator:
    """Generates curriculum specifications for different difficulty levels."""
    
    @staticmethod
    def create_curriculum_specs() -> List[Dict[str, Any]]:
        """Create curriculum specifications.
        
        Returns:
            List of curriculum level specifications
        """
        from .curriculum_config import get_curriculum_specs_for_precomputation
        return get_curriculum_specs_for_precomputation()
    
    @staticmethod
    def create_band_keeping_specs() -> List[Dict[str, Any]]:
        """Create band-keeping specifications for balanced difficulty distribution.
        
        Returns:
            List of band specifications
        """
        # Define bands by optimal length and robots moved
        bands = []
        
        # Short puzzles, single robot
        for length in [1, 2, 3, 4, 5]:
            bands.append({
                "name": f"Short-{length}",
                "spec_key": SpecKey(
                    height=8, width=8, num_robots=1,
                    edge_t_per_quadrant=1, central_l_per_quadrant=1
                ),
                "min_optimal_length": length,
                "max_optimal_length": length,
                "min_robots_moved": 1,
                "max_robots_moved": 1,
                "target_fraction": 0.1  # 10% of training data
            })
        
        # Medium puzzles, single robot
        for length in [6, 7, 8, 9, 10]:
            bands.append({
                "name": f"Medium-{length}",
                "spec_key": SpecKey(
                    height=10, width=10, num_robots=1,
                    edge_t_per_quadrant=2, central_l_per_quadrant=2
                ),
                "min_optimal_length": length,
                "max_optimal_length": length,
                "min_robots_moved": 1,
                "max_robots_moved": 1,
                "target_fraction": 0.08  # 8% of training data
            })
        
        # Multi-robot puzzles
        for length in [4, 6, 8, 10, 12]:
            for robots in [2, 3]:
                bands.append({
                    "name": f"Multi-{robots}R-{length}",
                    "spec_key": SpecKey(
                        height=12, width=12, num_robots=robots,
                        edge_t_per_quadrant=2, central_l_per_quadrant=2
                    ),
                    "min_optimal_length": length,
                    "max_optimal_length": length,
                    "min_robots_moved": 1,
                    "max_robots_moved": robots,
                    "target_fraction": 0.05  # 5% of training data
                })
        
        return bands


def _speckey_dict_equals(a: dict, b: dict) -> bool:
    """Shallow equality for SpecKey dicts from manifest vs object-asdict."""
    return (
        a.get("height") == b.get("height") and
        a.get("width") == b.get("width") and
        a.get("num_robots") == b.get("num_robots") and
        a.get("edge_t_per_quadrant") == b.get("edge_t_per_quadrant") and
        a.get("central_l_per_quadrant") == b.get("central_l_per_quadrant") and
        a.get("generator_rules_version", "1.0") == b.get("generator_rules_version", "1.0")
    )


def compute_level_coverage_from_histograms(
    bank: PuzzleBank,
    curriculum_levels: "list[dict]",
) -> "dict[int, int]":
    """Compute how many puzzles are available per level using manifest histograms.

    Returns a map: level_index -> available_count satisfying that level's (optimal_length, robots_moved) bounds
    and matching the level's SpecKey.
    """
    coverage: dict[int, int] = {}
    partitions = bank.manifest.get("partitions", {})
    for level in curriculum_levels:
        level_idx = int(level["level"])
        sk_obj: SpecKey = level["spec_key"] if isinstance(level["spec_key"], SpecKey) else level["spec_key"]
        # Normalize to dict for comparison
        if isinstance(sk_obj, SpecKey):
            sk_dict = {
                "height": sk_obj.height,
                "width": sk_obj.width,
                "num_robots": sk_obj.num_robots,
                "edge_t_per_quadrant": sk_obj.edge_t_per_quadrant,
                "central_l_per_quadrant": sk_obj.central_l_per_quadrant,
                "generator_rules_version": sk_obj.generator_rules_version,
            }
        else:
            sk_dict = sk_obj

        min_ol = int(level["min_optimal_length"]) if level.get("min_optimal_length") is not None else None
        max_ol = int(level["max_optimal_length"]) if level.get("max_optimal_length") is not None else None
        min_rm = int(level["min_robots_moved"]) if level.get("min_robots_moved") is not None else None
        max_rm = int(level["max_robots_moved"]) if level.get("max_robots_moved") is not None else None

        count = 0
        for part_key, part in partitions.items():
            part_spec = part.get("spec_key", {})
            if not _speckey_dict_equals(part_spec, sk_dict):
                continue
            hist = (
                part.get("histogram", {})
                .get("by_optimal_length_and_robots_moved", {})
            )
            for k, v in hist.items():
                # k format: "ol={ol}|rm={rm}"
                try:
                    segs = dict(seg.split("=") for seg in k.split("|"))
                    ol = int(segs.get("ol", -1))
                    rm = int(segs.get("rm", -1))
                except Exception:
                    continue
                if (min_ol is not None and ol < min_ol) or (max_ol is not None and ol > max_ol):
                    continue
                if (min_rm is not None and rm < min_rm) or (max_rm is not None and rm > max_rm):
                    continue
                count += int(v)
        coverage[level_idx] = count
    return coverage


def run_band_first_precompute_controller(
    bank_dir: str,
    target_per_level: int = 1000,
    max_puzzles_global: int = 200000,
    chunk_puzzles_per_spec: int = 200,
    solver_config: Optional[Dict[str, Any]] = None,
    verbose: bool = True,
    curriculum_levels: Optional[List[Dict[str, Any]]] = None,
) -> Dict[str, Any]:
    """Outline controller: generate in chunks and stop when each level has enough coverage.

    This function uses manifest histograms to approximate available counts per level.
    It generates additional puzzles spec-by-spec in chunks until coverage thresholds
    are met or the global budget is exhausted.
    """
    if solver_config is None:
        solver_config = {"solver_type": "bfs", "max_depth": 10, "max_nodes": 100000}

    bank = PuzzleBank(bank_dir)
    generator = PuzzleGenerator(bank, solver_config=solver_config, verbose=False)
    levels = curriculum_levels if curriculum_levels is not None else get_curriculum_specs_for_precomputation()
    max_depth = max([level["max_optimal_length"] for level in levels])

    total_generated = 0
    loop_iter = 0
    while True:
        loop_iter += 1
        coverage = compute_level_coverage_from_histograms(bank, levels)
        if verbose:
            print(f"[controller] iteration {loop_iter} coverage: {coverage}")
        if coverage and all(c >= target_per_level for c in coverage.values()):
            if verbose:
                print("[controller] All levels meet target coverage. Stopping.")
            break
        elif coverage:
            remaining_levels = [level for level in levels if coverage[int(level["level"])] < target_per_level]
            max_depth = max([level["max_optimal_length"] for level in remaining_levels])
            if verbose:
                print(f"Tried to generate {total_generated} puzzles; total budget is {max_puzzles_global}")
                print(f"[controller] Remaining levels: {[level['level'] for level in remaining_levels]}")
                print(f"[controller] Max depth: {max_depth}")
        if total_generated >= max_puzzles_global:
            if verbose:
                print("[controller] Reached global max puzzle budget. Stopping.")
            break

        # Generate one small chunk per spec (round-robin)
        for spec in levels:
            if total_generated >= max_puzzles_global:
                break
            spec_key: SpecKey = spec["spec_key"]
            if verbose:
                print(f"[controller] Generating chunk for spec {spec_key.to_partition_key()} ({chunk_puzzles_per_spec})")
            stats = generator.generate_puzzles_for_spec(
                spec_key=spec_key,
                num_puzzles=chunk_puzzles_per_spec,
                criteria={
                    "solver_max_depth_override": max_depth,
                }
            )
            total_generated += stats.get("generated", 0)

    return {
        "total_generated": total_generated,
        "final_coverage": compute_level_coverage_from_histograms(bank, levels),
        "target_per_level": target_per_level,
        "max_puzzles_global": max_puzzles_global,
    }

def run_precomputation(
    bank_dir: str,
    curriculum_specs: Optional[List[Dict[str, Any]]] = None,
    solver_config: Optional[Dict[str, Any]] = None,
    verbose: bool = True
) -> Dict[str, Any]:
    """Run full precomputation pipeline.
    
    Args:
        bank_dir: Directory to store puzzle bank
        curriculum_specs: Curriculum specifications to generate
        solver_config: Solver configuration dictionary
        verbose: Whether to print progress
        
    Returns:
        Overall statistics
    """
    if curriculum_specs is None:
        curriculum_specs = CurriculumSpecGenerator.create_curriculum_specs()
    
    # Initialize bank and generator
    bank = PuzzleBank(bank_dir)
    generator = PuzzleGenerator(bank, solver_config, verbose)
    
    overall_stats = {
        "total_requested": 0,
        "total_generated": 0,
        "total_solved": 0,
        "total_failed": 0,
        "level_stats": []
    }
    
    if verbose:
        print(f"Starting precomputation for {len(curriculum_specs)} curriculum levels")
        print(f"Bank directory: {bank_dir}")
    
    # Generate puzzles for each level
    for spec in curriculum_specs:
        if verbose:
            print(f"\nGenerating Level {spec['level']}: {spec['name']}")
        
        level_stats = generator.generate_puzzles_for_spec(
            spec_key=spec["spec_key"],
            num_puzzles=spec["num_puzzles"]
        )
        
        overall_stats["total_requested"] += level_stats["requested"]
        overall_stats["total_generated"] += level_stats["generated"]
        overall_stats["total_solved"] += level_stats["solved"]
        overall_stats["total_failed"] += level_stats["failed"]
        overall_stats["level_stats"].append(level_stats)
    
    # Final statistics
    overall_stats["overall_success_rate"] = (
        overall_stats["total_solved"] / overall_stats["total_generated"]
        if overall_stats["total_generated"] > 0 else 0
    )
    
    if verbose:
        print(f"\nPrecomputation complete!")
        print(f"Total: {overall_stats['total_solved']}/{overall_stats['total_generated']} solved")
        print(f"Success rate: {overall_stats['overall_success_rate']:.2%}")
        print(f"Bank stats: {bank.get_stats()}")
    
    return overall_stats


if __name__ == "__main__":
    # Example usage
    import argparse
    
    parser = argparse.ArgumentParser(description="Precompute puzzle bank")
    parser.add_argument("--bank_dir", type=str, default="./puzzle_bank", help="Bank directory")
    parser.add_argument("--verbose", action="store_true", help="Verbose output")
    args = parser.parse_args()
    
    stats = run_precomputation(
        bank_dir=args.bank_dir,
        verbose=args.verbose
    )
    
    print(f"Precomputation completed with {stats['total_solved']} puzzles generated")

```

```python
"""
Criteria-Filtered Environment for Bank-Based Curriculum Learning

This module provides an environment wrapper that uses the puzzle bank to provide
puzzles matching specific criteria without requiring online solving.
"""

from __future__ import annotations

import numpy as np
from typing import Dict, List, Optional, Any, Tuple
from dataclasses import dataclass

from .puzzle_bank import (
    PuzzleBank, BankSampler, PuzzleMetadata, SpecKey, 
    create_fixed_layout_from_seed
)
from .ricochet_env import RicochetRobotsEnv, FixedLayout, GymEnvBase


@dataclass
class PuzzleCriteria:
    """Criteria for filtering puzzles from the bank."""
    spec_key: SpecKey
    min_optimal_length: Optional[int] = None
    max_optimal_length: Optional[int] = None
    min_robots_moved: Optional[int] = None
    max_robots_moved: Optional[int] = None
    # Optional stratified bands: list of dicts with ol/rm ranges and optional weight
    bands: Optional[List[Dict[str, Any]]] = None
    
    def matches(self, metadata: PuzzleMetadata) -> bool:
        """Check if puzzle metadata matches criteria.
        
        Args:
            metadata: Puzzle metadata to check
            
        Returns:
            True if matches criteria
        """
        # Check spec key
        if metadata.spec_key != self.spec_key:
            return False
        
        # Check optimal length
        if self.min_optimal_length is not None:
            if metadata.optimal_length < self.min_optimal_length:
                return False
        if self.max_optimal_length is not None:
            if metadata.optimal_length > self.max_optimal_length:
                return False
        
        # Check robots moved
        if self.min_robots_moved is not None:
            if metadata.robots_moved < self.min_robots_moved:
                return False
        if self.max_robots_moved is not None:
            if metadata.robots_moved > self.max_robots_moved:
                return False
        
        return True


class CriteriaFilteredEnv(GymEnvBase):
    """
    Environment wrapper that provides puzzles from the bank matching specific criteria.
    
    This environment uses the puzzle bank to provide puzzles without requiring
    online solving, enabling tight criteria and stable performance.
    """
    
    def __init__(
        self,
        bank: PuzzleBank,
        criteria: PuzzleCriteria,
        obs_mode: str = "rgb_image",
        channels_first: bool = True,
        include_noop: bool = False,
        render_mode: Optional[str] = None,
        verbose: bool = False,
        allow_fallback: bool = False,
    ):
        """Initialize criteria-filtered environment.
        
        Args:
            bank: Puzzle bank to sample from
            criteria: Criteria for puzzle selection
            obs_mode: Observation mode ("image", "rgb_image", "symbolic")
            channels_first: Whether to use channels-first format
            render_mode: Rendering mode
            verbose: Whether to print debug information
        """
        self.bank = bank
        self.criteria = criteria
        self.obs_mode = obs_mode
        self.channels_first = channels_first
        self.include_noop = include_noop
        self.verbose = verbose
        self.allow_fallback = allow_fallback
        
        # Initialize sampler
        self.sampler = BankSampler(bank, sampling_strategy="uniform")
        
        # Current puzzle metadata
        self._current_metadata: Optional[PuzzleMetadata] = None
        self._current_env: Optional[RicochetRobotsEnv] = None
        
        # Statistics
        self._total_episodes = 0
        self._successful_episodes = 0
        self._failed_samples = 0
        # Episode step counter (for optimality gap without online solving)
        self._episode_steps: int = 0
        
        # Create initial environment
        self._create_current_env()
        
        # Set up render mode
        self.render_mode = render_mode
        if self.render_mode is not None and self.render_mode not in ["ascii", "rgb"]:
            raise ValueError(f"Unsupported render_mode {self.render_mode}")
    
    def _create_current_env(self) -> None:
        """Create current environment from bank sample."""
        # Sample puzzle from bank (optionally stratified)
        if self.criteria.bands:
            samples = self.sampler.sample_puzzles_stratified(
                spec_key=self.criteria.spec_key,
                total_count=1,
                bands=self.criteria.bands,
                random_seed=None,
            )
            metadata = samples[0] if samples else None
        else:
            metadata = self.sampler.sample_puzzle(
                spec_key=self.criteria.spec_key,
                min_optimal_length=self.criteria.min_optimal_length,
                max_optimal_length=self.criteria.max_optimal_length,
                min_robots_moved=self.criteria.min_robots_moved,
                max_robots_moved=self.criteria.max_robots_moved
            )
        
        if metadata is None:
            if self.verbose:
                print(f"Warning: No puzzle found matching criteria {self.criteria}")
            self._failed_samples += 1
            # Fallback only if explicitly allowed
            if self.allow_fallback:
                self._create_fallback_env()
            else:
                raise RuntimeError("No puzzle found matching criteria and fallback disabled. Please increase bank coverage or relax criteria.")
            return
        
        self._current_metadata = metadata
        
        try:
            # Create FixedLayout from metadata
            fixed_layout = create_fixed_layout_from_seed(metadata)
            
            # Create environment with fixed layout
            self._current_env = RicochetRobotsEnv(
                fixed_layout=fixed_layout,
                obs_mode=self.obs_mode,
                channels_first=self.channels_first,
                include_noop=self.include_noop,
                render_mode=self.render_mode
            )
            
        except Exception as e:
            if self.verbose:
                print(f"Error creating environment from metadata: {e}")
            self._failed_samples += 1
            if self.allow_fallback:
                self._create_fallback_env()
            else:
                raise
    
    def _create_fallback_env(self) -> None:
        """Create fallback environment when bank sampling fails."""
        if self.verbose:
            print("Creating fallback environment")
        
        # Create a simple environment with the spec
        self._current_env = RicochetRobotsEnv(
            height=self.criteria.spec_key.height,
            width=self.criteria.spec_key.width,
            num_robots=self.criteria.spec_key.num_robots,
            edge_t_per_quadrant=self.criteria.spec_key.edge_t_per_quadrant,
            central_l_per_quadrant=self.criteria.spec_key.central_l_per_quadrant,
            ensure_solvable=True,  # Fallback to solvable generation
            obs_mode=self.obs_mode,
            channels_first=self.channels_first,
            include_noop=self.include_noop,
            render_mode=self.render_mode
        )
        
        self._current_metadata = None
    
    def reset(self, *, seed: Optional[int] = None, options: Optional[dict] = None):
        """Reset environment and return initial observation.
        
        Args:
            seed: Random seed (ignored for bank-based sampling)
            options: Additional options (ignored)
            
        Returns:
            (observation, info) tuple
        """
        # Create new environment for this episode
        self._create_current_env()
        
        # Reset environment
        obs, info = self._current_env.reset()
        # Reset step counter at start of episode
        self._episode_steps = 0
        
        # Add bank-specific info
        info.update({
            "bank_metadata": self._current_metadata,
            "criteria": {
                "spec_key": {
                    "height": self.criteria.spec_key.height,
                    "width": self.criteria.spec_key.width,
                    "num_robots": self.criteria.spec_key.num_robots,
                    "edge_t_per_quadrant": self.criteria.spec_key.edge_t_per_quadrant,
                    "central_l_per_quadrant": self.criteria.spec_key.central_l_per_quadrant
                },
                "min_optimal_length": self.criteria.min_optimal_length,
                "max_optimal_length": self.criteria.max_optimal_length,
                "min_robots_moved": self.criteria.min_robots_moved,
                "max_robots_moved": self.criteria.max_robots_moved,
                "bands": self.criteria.bands
            },
            "bank_stats": {
                "total_episodes": self._total_episodes,
                "successful_episodes": self._successful_episodes,
                "failed_samples": self._failed_samples
            }
        })

        # Attach stratified sampling plan if used
        if self.criteria.bands:
            plan = self.sampler.get_last_stratified_plan()
            if plan is not None:
                info["stratified_plan"] = plan
        
        if self._current_metadata is not None:
            info.update({
                "optimal_length": self._current_metadata.optimal_length,
                "robots_moved": self._current_metadata.robots_moved,
                "puzzle_seed": self._current_metadata.seed
            })
        
        return obs, info
    
    def step(self, action):
        """Step environment and return (obs, reward, terminated, truncated, info).
        
        Args:
            action: Action to take
            
        Returns:
            (observation, reward, terminated, truncated, info) tuple
        """
        obs, reward, terminated, truncated, info = self._current_env.step(action)
        # Count steps taken in the current episode
        self._episode_steps += 1
        
        # Track episode completion
        if terminated or truncated:
            self._total_episodes += 1
            if info.get("is_success", False) and not truncated:
                self._successful_episodes += 1
            # If we have optimal solution length from bank metadata, compute gap
            if self._current_metadata is not None:
                optimal_len = int(self._current_metadata.optimal_length)
                # Report raw episode steps and optimality gap only on episode end
                info.setdefault("episode_steps", int(self._episode_steps))
                # Only meaningful if success and not truncated; otherwise it's just steps until termination
                if info.get("is_success", False) and not truncated:
                    info["optimality_gap"] = int(self._episode_steps - optimal_len)
                else:
                    # For failures, still attach gap as None to indicate unavailable
                    info.setdefault("optimality_gap", None)
            # Reset step counter for safety (env will usually be reset by caller)
            self._episode_steps = 0
        
        return obs, reward, terminated, truncated, info
    
    def render(self, *args, **kwargs):
        """Render environment."""
        if self._current_env is None:
            return None
        return self._current_env.render(*args, **kwargs)

    def get_board(self):
        """Forward underlying board for visualization utilities."""
        if self._current_env is None:
            raise RuntimeError("Environment not initialized")
        return self._current_env.get_board()
    
    def close(self, *args, **kwargs):
        """Close environment."""
        if self._current_env is not None:
            self._current_env.close(*args, **kwargs)
    
    @property
    def action_space(self):
        """Action space."""
        if self._current_env is None:
            raise RuntimeError("Environment not initialized")
        return self._current_env.action_space
    
    @property
    def observation_space(self):
        """Observation space."""
        if self._current_env is None:
            raise RuntimeError("Environment not initialized")
        return self._current_env.observation_space
    
    @property
    def metadata(self):
        """Environment metadata."""
        if self._current_env is None:
            return {"render_modes": ["ascii", "rgb"], "render_fps": 4}
        return self._current_env.metadata
    
    def get_criteria(self) -> PuzzleCriteria:
        """Get current criteria."""
        return self.criteria
    
    def get_bank_stats(self) -> Dict[str, Any]:
        """Get bank statistics."""
        return {
            "total_episodes": self._total_episodes,
            "successful_episodes": self._successful_episodes,
            "failed_samples": self._failed_samples,
            "success_rate": (
                self._successful_episodes / self._total_episodes
                if self._total_episodes > 0 else 0
            )
        }
    
    def update_criteria(self, new_criteria: PuzzleCriteria) -> None:
        """Update puzzle criteria.
        
        Args:
            new_criteria: New criteria to use
        """
        self.criteria = new_criteria
        # Environment will be recreated on next reset


class BankCurriculumManager:
    """Curriculum manager that uses the puzzle bank for level progression."""
    
    def __init__(
        self,
        bank: PuzzleBank,
        curriculum_levels: List[Dict[str, Any]],
        success_rate_threshold: float = 0.8,
        min_episodes_per_level: int = 100,
        success_rate_window_size: int = 200,
        advancement_check_frequency: int = 50,
        verbose: bool = True
    ):
        """Initialize bank-based curriculum manager.
        
        Args:
            bank: Puzzle bank to sample from
            curriculum_levels: List of curriculum level specifications
            success_rate_threshold: Success rate required to advance
            min_episodes_per_level: Minimum episodes before advancement check
            success_rate_window_size: Window size for success rate calculation
            advancement_check_frequency: How often to check for advancement
            verbose: Whether to print progress
        """
        self.bank = bank
        self.curriculum_levels = curriculum_levels
        self.success_rate_threshold = success_rate_threshold
        self.min_episodes_per_level = min_episodes_per_level
        self.success_rate_window_size = success_rate_window_size
        self.advancement_check_frequency = advancement_check_frequency
        self.verbose = verbose
        
        # Current state
        self.current_level = 0
        self.success_rate_window = []
        self.episode_count = 0
        self.level_start_episode = 0
        
        if self.verbose:
            print(f"Bank curriculum manager initialized at level {self.current_level}")
    
    def get_current_criteria(self) -> PuzzleCriteria:
        """Get criteria for current level.
        
        Returns:
            PuzzleCriteria for current level
        """
        level_spec = self.curriculum_levels[self.current_level]
        # Build simple bands across discrete optimal lengths within range
        bands: Optional[List[Dict[str, Any]]] = None
        if (
            level_spec.get("min_optimal_length") is not None and
            level_spec.get("max_optimal_length") is not None
        ):
            try:
                min_ol = int(level_spec.get("min_optimal_length"))
                max_ol = int(level_spec.get("max_optimal_length"))
                min_rm = level_spec.get("min_robots_moved")
                max_rm = level_spec.get("max_robots_moved")
                bands = [
                    {
                        "min_optimal_length": ol,
                        "max_optimal_length": ol,
                        "min_robots_moved": min_rm,
                        "max_robots_moved": max_rm,
                        "weight": 1.0,
                    }
                    for ol in range(min_ol, max_ol + 1)
                ]
            except Exception:
                bands = None

        return PuzzleCriteria(
            spec_key=level_spec["spec_key"],
            min_optimal_length=level_spec.get("min_optimal_length"),
            max_optimal_length=level_spec.get("max_optimal_length"),
            min_robots_moved=level_spec.get("min_robots_moved"),
            max_robots_moved=level_spec.get("max_robots_moved"),
            bands=bands
        )
    
    def record_episode_result(self, success: bool) -> None:
        """Record episode result.
        
        Args:
            success: Whether episode was successful
        """
        self.episode_count += 1
        self.success_rate_window.append(success)
        
        # Trim window if too large
        if len(self.success_rate_window) > self.success_rate_window_size:
            self.success_rate_window = self.success_rate_window[-self.success_rate_window_size:]
        
        # Check for advancement
        if self._should_check_advancement():
            self._check_advancement()
    
    def _should_check_advancement(self) -> bool:
        """Check if it's time to evaluate advancement."""
        if self.current_level >= len(self.curriculum_levels) - 1:
            return False  # Already at max level
        
        # Check if we have enough episodes at current level
        episodes_at_level = self.episode_count - self.level_start_episode
        if episodes_at_level < self.min_episodes_per_level:
            return False
        
        # Check if it's time for advancement check
        return episodes_at_level % self.advancement_check_frequency == 0
    
    def _check_advancement(self) -> None:
        """Check if agent performance warrants advancing to next level."""
        if len(self.success_rate_window) < self.success_rate_window_size:
            return  # Not enough data yet
        
        success_rate = np.mean(self.success_rate_window)
        
        if success_rate >= self.success_rate_threshold:
            self._advance_level()
    
    def _advance_level(self) -> None:
        """Advance to the next curriculum level."""
        old_level = self.current_level
        self.current_level += 1
        self.level_start_episode = self.episode_count
        
        # Clear success rate window for new level
        self.success_rate_window.clear()
        
        if self.verbose:
            level_spec = self.curriculum_levels[self.current_level]
            print(f"Curriculum advanced from level {old_level} to {self.current_level}: {level_spec['name']}")
    
    def get_current_level(self) -> int:
        """Get current curriculum level."""
        return self.current_level
    
    def get_success_rate(self) -> float:
        """Get current success rate."""
        if not self.success_rate_window:
            return 0.0
        return float(np.mean(self.success_rate_window))
    
    def get_episodes_at_level(self) -> int:
        """Get number of episodes at current level."""
        return self.episode_count - self.level_start_episode
    
    def get_stats(self) -> Dict[str, Any]:
        """Get comprehensive statistics."""
        level_spec = self.curriculum_levels[self.current_level]
        return {
            "current_level": self.current_level,
            "level_name": level_spec["name"],
            "success_rate": self.get_success_rate(),
            "episodes_at_level": self.get_episodes_at_level(),
            "total_episodes": self.episode_count,
            "window_size": len(self.success_rate_window),
            "criteria": self.get_current_criteria()
        }

```

```python
"""
Puzzle Bank System for Curriculum Learning

This module provides a storage and retrieval system for precomputed Ricochet Robots
puzzles with their solution metadata. The bank stores puzzles in a lightweight format
using seeds and generation specs, with solution traces for deriving additional features.
"""

from __future__ import annotations

import hashlib
import json
import time
from dataclasses import dataclass, asdict
import struct
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Iterator, Any, Union
import numpy as np
try:
    import pandas as pd
    import pyarrow as pa
    import pyarrow.parquet as pq
except ImportError:
    # Fallback for environments without pyarrow
    import pandas as pd
    pa = None
    pq = None
from collections import defaultdict

from .ricochet_core import Board
from .ricochet_env import FixedLayout, generate_board_from_spec_with_seed


@dataclass(frozen=True)
class SpecKey:
    """Specification key that fully determines board generation given a seed."""
    height: int
    width: int
    num_robots: int
    edge_t_per_quadrant: int
    central_l_per_quadrant: int
    generator_rules_version: str = "1.0"
    
    def to_partition_key(self) -> str:
        """Generate partition key for storage organization."""
        return f"size-{self.height}x{self.width}_numR{self.num_robots}_ET{self.edge_t_per_quadrant}_CL{self.central_l_per_quadrant}_gen{self.generator_rules_version.replace('.', '')}"


@dataclass(frozen=True)
class SolverKey:
    """Solver configuration key for solution canonicalization."""
    algorithm: str = "BFS"
    neighbor_order_id: str = "canonical"  # canonical ordering for deterministic results
    max_depth: int = 50
    max_nodes: int = 100000
    solver_version: str = "1.0"
    
    def to_string(self) -> str:
        """String representation for indexing."""
        return f"{self.algorithm}_{self.neighbor_order_id}_d{self.max_depth}_n{self.max_nodes}_v{self.solver_version.replace('.', '')}"


@dataclass
class PuzzleMetadata:
    """Metadata for a single puzzle in the bank."""
    # Core identifiers
    seed: int
    spec_key: SpecKey
    solver_key: SolverKey
    board_hash: bytes  # blake3 hash of full board state (including target_robot)
    
    # Solution metrics
    optimal_length: int
    robots_moved: int  # count of distinct robot IDs in optimal plan
    nodes_expanded: int
    solve_time_ms: int
    
    # Flags
    solved_within_limits: bool
    hit_depth_limit: bool
    hit_node_limit: bool
    
    # Plan trace (for deriving additional features)
    actions_encoded: List[int]  # action = robot_id * 4 + direction

    # Metadata
    created_at: float
    run_id: str

    # Extended reproducibility
    board_seed: Optional[int] = None  # explicit seed used to generate this board
    layout_hash: Optional[bytes] = None  # hash of layout excluding target_robot
    
    # Optional layout blob (for generator drift recovery)
    layout_blob: Optional[bytes] = None
    
    
    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary for Parquet storage."""
        data = asdict(self)
        # Convert SpecKey and SolverKey to JSON strings for proper storage
        data['spec_key'] = json.dumps(asdict(self.spec_key))
        data['solver_key'] = json.dumps(asdict(self.solver_key))
        # Convert bytes to hex for JSON serialization
        data['board_hash'] = self.board_hash.hex()
        if data.get('layout_hash') is not None:
            data['layout_hash'] = self.layout_hash.hex() if isinstance(self.layout_hash, (bytes, bytearray)) else data['layout_hash']
        if self.layout_blob is not None:
            data['layout_blob'] = self.layout_blob.hex()
        return data
    
    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> PuzzleMetadata:
        """Create from dictionary loaded from Parquet."""
        # Convert back from hex - handle potential NaN/None values
        board_hash = data.get('board_hash')
        if board_hash and isinstance(board_hash, str) and board_hash.strip() and board_hash != 'nan':
            try:
                data['board_hash'] = bytes.fromhex(board_hash)
            except ValueError:
                raise ValueError(f"Invalid board_hash hex string: {board_hash}")
        else:
            raise ValueError("board_hash is required and cannot be empty")
        
        # Handle layout_blob - may be None, NaN, or empty string
        layout_blob = data.get('layout_blob')
        if (layout_blob and isinstance(layout_blob, str) and 
            layout_blob.strip() and layout_blob != 'nan'):
            try:
                data['layout_blob'] = bytes.fromhex(layout_blob)
            except ValueError:
                # Invalid hex string, treat as None
                data['layout_blob'] = None
        else:
            data['layout_blob'] = None

        # Optional fields: board_seed, layout_hash
        if 'board_seed' in data and data['board_seed'] == '':
            data['board_seed'] = None
        if 'layout_hash' in data:
            lh = data.get('layout_hash')
            if lh and isinstance(lh, str) and lh.strip() and lh != 'nan':
                try:
                    data['layout_hash'] = bytes.fromhex(lh)
                except ValueError:
                    data['layout_hash'] = None
            else:
                data['layout_hash'] = None
        
        # Reconstruct SpecKey and SolverKey - handle both dict and string formats
        import ast
        spec_key_data = data['spec_key']
        if isinstance(spec_key_data, dict):
            # Already a dictionary, use as-is
            pass
        elif isinstance(spec_key_data, str):
            try:
                # Try JSON first (for properly formatted JSON strings)
                spec_key_data = json.loads(spec_key_data)
            except json.JSONDecodeError:
                try:
                    # Fallback to ast.literal_eval for Python dict string representation
                    spec_key_data = ast.literal_eval(spec_key_data)
                except (ValueError, SyntaxError) as e:
                    raise ValueError(f"Invalid spec_key format: {spec_key_data}") from e
        else:
            raise ValueError(f"spec_key must be dict or string, got {type(spec_key_data)}")
        data['spec_key'] = SpecKey(**spec_key_data)
        
        solver_key_data = data['solver_key']
        if isinstance(solver_key_data, dict):
            # Already a dictionary, use as-is
            pass
        elif isinstance(solver_key_data, str):
            try:
                # Try JSON first (for properly formatted JSON strings)
                solver_key_data = json.loads(solver_key_data)
            except json.JSONDecodeError:
                try:
                    # Fallback to ast.literal_eval for Python dict string representation
                    solver_key_data = ast.literal_eval(solver_key_data)
                except (ValueError, SyntaxError) as e:
                    raise ValueError(f"Invalid solver_key format: {solver_key_data}") from e
        else:
            raise ValueError(f"solver_key must be dict or string, got {type(solver_key_data)}")
        data['solver_key'] = SolverKey(**solver_key_data)
        
        return cls(**data)


class PuzzleBank:
    """Storage and retrieval system for precomputed puzzles."""
    
    def __init__(self, bank_dir: Union[str, Path], run_id: Optional[str] = None):
        """Initialize puzzle bank.
        
        Args:
            bank_dir: Directory to store the puzzle bank
            run_id: Unique identifier for this computation run
        """
        self.bank_dir = Path(bank_dir)
        self.bank_dir.mkdir(parents=True, exist_ok=True)
        self.run_id = run_id or f"run_{int(time.time())}"
        # In-memory cache of partition DataFrames keyed by partition path
        # This avoids re-reading Parquet/CSV on every query during training
        self._partition_df_cache: Dict[Path, "pd.DataFrame"] = {}
        
        # Create manifest file for tracking partitions
        self.manifest_path = self.bank_dir / "manifest.json"
        self._load_manifest()
    
    def _load_manifest(self) -> None:
        """Load or create manifest file."""
        if self.manifest_path.exists():
            with open(self.manifest_path, 'r', encoding='utf-8') as f:
                self.manifest = json.load(f)
            # Backfill new fields for schema v1.1
            if "version" not in self.manifest or self.manifest.get("version") in {"1.0", 1.0}:
                # Upgrade to 1.1 while preserving existing data
                self.manifest["version"] = "1.1"
            # Ensure partitions dict exists
            self.manifest.setdefault("partitions", {})
            # Ensure new per-partition fields
            for part_key, part_info in self.manifest["partitions"].items():
                part_info.setdefault("histogram", {
                    "by_optimal_length_and_robots_moved": {}
                })
                part_info.setdefault("fingerprints", {})
                part_info.setdefault("dedup", {
                    "method": "none",
                    "bloom": None
                })
        else:
            self.manifest = {
                "version": "1.1",
                "partitions": {},
                "created_at": time.time()
            }
            self._save_manifest()
    
    def _save_manifest(self) -> None:
        """Save manifest file."""
        with open(self.manifest_path, 'w', encoding='utf-8') as f:
            json.dump(self.manifest, f, indent=2)
    
    def _get_partition_path(self, spec_key: SpecKey) -> Path:
        """Get path for a partition."""
        partition_key = spec_key.to_partition_key()
        return self.bank_dir / f"{partition_key}.parquet"
    
    def _load_partition_dataframe(self, partition_path: Path) -> "pd.DataFrame":
        """Load a partition into a pandas DataFrame, using an in-memory cache."""
        # Fast path: cached
        if partition_path in self._partition_df_cache:
            return self._partition_df_cache[partition_path]
        # Load from disk
        if not partition_path.exists():
            df = pd.DataFrame([])
            self._partition_df_cache[partition_path] = df
            return df
        if pa is not None and pq is not None:
            table = pq.read_table(partition_path)
            df = table.to_pandas()
        else:
            df = pd.read_csv(partition_path)
        # Clean NaNs to consistent blanks for string fields
        df = df.fillna('')
        self._partition_df_cache[partition_path] = df
        return df
    
    def get_partition_dataframe(self, spec_key: SpecKey) -> "pd.DataFrame":
        """Public helper to access a cached DataFrame for a spec partition."""
        partition_path = self._get_partition_path(spec_key)
        return self._load_partition_dataframe(partition_path)
    
    def add_puzzles(self, puzzles: List[PuzzleMetadata]) -> None:
        """Add puzzles to the bank.
        
        Args:
            puzzles: List of puzzle metadata to add
        """
        if not puzzles:
            return
        
        # Group by spec_key for partitioning
        by_spec = defaultdict(list)
        for puzzle in puzzles:
            by_spec[puzzle.spec_key].append(puzzle)
        
        for spec_key, spec_puzzles in by_spec.items():
            partition_path = self._get_partition_path(spec_key)
            
            # Convert to DataFrame
            data = [puzzle.to_dict() for puzzle in spec_puzzles]
            df = pd.DataFrame(data)
            
            if pa is not None and pq is not None:
                # Create Arrow table
                table = pa.Table.from_pandas(df)
                
                # Write to Parquet (append if exists)
                if partition_path.exists():
                    existing_table = pq.read_table(partition_path)
                    combined_table = pa.concat_tables([existing_table, table])
                    pq.write_table(combined_table, partition_path)
                else:
                    pq.write_table(table, partition_path)
            else:
                # Fallback to CSV if pyarrow not available
                if partition_path.exists():
                    existing_df = pd.read_csv(partition_path)
                    combined_df = pd.concat([existing_df, df], ignore_index=True)
                    combined_df.to_csv(partition_path, index=False)
                else:
                    df.to_csv(partition_path, index=False)
            
            # Update manifest
            partition_key = spec_key.to_partition_key()
            if partition_key not in self.manifest["partitions"]:
                self.manifest["partitions"][partition_key] = {
                    "spec_key": asdict(spec_key),
                    "created_at": time.time(),
                    "puzzle_count": 0,
                    "histogram": {
                        "by_optimal_length_and_robots_moved": {}
                    },
                    "fingerprints": {},
                    "dedup": {
                        "method": "none",
                        "bloom": None
                    }
                }
            self.manifest["partitions"][partition_key]["puzzle_count"] += len(spec_puzzles)
            self.manifest["partitions"][partition_key]["last_updated"] = time.time()

            # Increment histograms for (optimal_length, robots_moved)
            hist = self.manifest["partitions"][partition_key]["histogram"]["by_optimal_length_and_robots_moved"]
            for puzzle in spec_puzzles:
                ol = int(getattr(puzzle, "optimal_length", -1))
                rm = int(getattr(puzzle, "robots_moved", -1))
                key = f"ol={ol}|rm={rm}"
                hist[key] = int(hist.get(key, 0)) + 1
        
        self._save_manifest()
    
    def query_puzzles(
        self,
        spec_key: Optional[SpecKey] = None,
        min_optimal_length: Optional[int] = None,
        max_optimal_length: Optional[int] = None,
        min_robots_moved: Optional[int] = None,
        max_robots_moved: Optional[int] = None,
        limit: Optional[int] = None,
        random_seed: Optional[int] = None
    ) -> Iterator[PuzzleMetadata]:
        """Query puzzles matching criteria.
        
        Args:
            spec_key: Filter by generation spec
            min_optimal_length: Minimum optimal solution length
            max_optimal_length: Maximum optimal solution length
            min_robots_moved: Minimum number of robots moved
            max_robots_moved: Maximum number of robots moved
            limit: Maximum number of results
            random_seed: Random seed for reproducible sampling (None for random)
            
        Yields:
            PuzzleMetadata matching criteria
        """
        # Determine which partitions to search
        if spec_key is not None:
            partition_paths = [self._get_partition_path(spec_key)]
        else:
            partition_paths = [
                self.bank_dir / f"{partition_key}.parquet"
                for partition_key in self.manifest["partitions"].keys()
            ]
        
        count = 0
        for partition_path in partition_paths:
            # Load via cache (empty DataFrame if not present)
            df = self._load_partition_dataframe(partition_path)
            
            # Apply filters
            if len(df) == 0:
                continue
            mask = pd.Series([True] * len(df))
            
            if min_optimal_length is not None:
                mask &= df['optimal_length'] >= min_optimal_length
            if max_optimal_length is not None:
                mask &= df['optimal_length'] <= max_optimal_length
            if min_robots_moved is not None:
                mask &= df['robots_moved'] >= min_robots_moved
            if max_robots_moved is not None:
                mask &= df['robots_moved'] <= max_robots_moved
            
            # Convert filtered rows to PuzzleMetadata
            filtered_df = df[mask]
            
            # Shuffle the filtered data for random sampling
            if len(filtered_df) > 0:
                filtered_df = filtered_df.sample(frac=1.0, random_state=random_seed).reset_index(drop=True)
            
            for _, row in filtered_df.iterrows():
                if limit is not None and count >= limit:
                    return
                
                puzzle = PuzzleMetadata.from_dict(row.to_dict())
                yield puzzle
                count += 1
    
    def get_puzzle_count(self, spec_key: Optional[SpecKey] = None) -> int:
        """Get total number of puzzles in bank.
        
        Args:
            spec_key: If provided, count only for this spec
            
        Returns:
            Number of puzzles
        """
        if spec_key is not None:
            partition_key = spec_key.to_partition_key()
            return self.manifest["partitions"].get(partition_key, {}).get("puzzle_count", 0)
        else:
            return sum(
                partition.get("puzzle_count", 0)
                for partition in self.manifest["partitions"].values()
            )
    
    def get_stats(self) -> Dict[str, Any]:
        """Get bank statistics.
        
        Returns:
            Dictionary with bank statistics
        """
        total_puzzles = self.get_puzzle_count()
        partition_count = len(self.manifest["partitions"])
        
        # Get distribution stats
        all_puzzles = list(self.query_puzzles(limit=10000))  # Sample for stats
        if all_puzzles:
            optimal_lengths = [p.optimal_length for p in all_puzzles]
            robots_moved = [p.robots_moved for p in all_puzzles]
            
            stats = {
                "total_puzzles": total_puzzles,
                "partition_count": partition_count,
                "optimal_length_stats": {
                    "min": min(optimal_lengths),
                    "max": max(optimal_lengths),
                    "mean": np.mean(optimal_lengths),
                    "std": np.std(optimal_lengths)
                },
                "robots_moved_stats": {
                    "min": min(robots_moved),
                    "max": max(robots_moved),
                    "mean": np.mean(robots_moved),
                    "std": np.std(robots_moved)
                }
            }
        else:
            stats = {
                "total_puzzles": total_puzzles,
                "partition_count": partition_count,
                "optimal_length_stats": None,
                "robots_moved_stats": None
            }
        
        return stats


class BankSampler:
    """Sampler for retrieving puzzles from the bank during training."""
    
    def __init__(self, bank: PuzzleBank, sampling_strategy: str = "uniform"):
        """Initialize bank sampler.
        
        Args:
            bank: Puzzle bank to sample from
            sampling_strategy: Strategy for sampling ("uniform", "stratified")
        """
        self.bank = bank
        self.sampling_strategy = sampling_strategy
        # Cache of pre-filtered, shuffled views for fast per-episode sampling
        # key -> { 'df': DataFrame, 'order': np.ndarray[int], 'pos': int, 'rng': np.random.Generator }
        self._criteria_cache: Dict[Tuple[str, int, int, int, int], Dict[str, Any]] = {}
        # Monitoring
        self._last_stratified_plan: Optional[Dict[str, Any]] = None

    def _criteria_key(
        self,
        spec_key: SpecKey,
        min_optimal_length: Optional[int],
        max_optimal_length: Optional[int],
        min_robots_moved: Optional[int],
        max_robots_moved: Optional[int],
    ) -> Tuple[str, int, int, int, int]:
        # Use partition string plus numeric bounds (None -> sentinel -1)
        part = spec_key.to_partition_key()
        def _n(v: Optional[int]) -> int:
            return -1 if v is None else int(v)
        return (part, _n(min_optimal_length), _n(max_optimal_length), _n(min_robots_moved), _n(max_robots_moved))

    def _get_or_build_iterator(
        self,
        spec_key: SpecKey,
        min_optimal_length: Optional[int],
        max_optimal_length: Optional[int],
        min_robots_moved: Optional[int],
        max_robots_moved: Optional[int],
        random_seed: Optional[int],
    ) -> Optional[Dict[str, Any]]:
        key = self._criteria_key(spec_key, min_optimal_length, max_optimal_length, min_robots_moved, max_robots_moved)
        it = self._criteria_cache.get(key)
        if it is None:
            # Build filtered DataFrame from cached partition
            df = self.bank.get_partition_dataframe(spec_key)
            if len(df) == 0:
                return None
            mask = pd.Series([True] * len(df))
            if min_optimal_length is not None:
                mask &= df['optimal_length'] >= min_optimal_length
            if max_optimal_length is not None:
                mask &= df['optimal_length'] <= max_optimal_length
            if min_robots_moved is not None:
                mask &= df['robots_moved'] >= min_robots_moved
            if max_robots_moved is not None:
                mask &= df['robots_moved'] <= max_robots_moved
            filtered = df[mask]
            if len(filtered) == 0:
                return None
            # Initialize RNG and shuffled order
            rng = np.random.default_rng(random_seed)
            order = np.arange(len(filtered))
            rng.shuffle(order)
            it = {"df": filtered.reset_index(drop=True), "order": order, "pos": 0, "rng": rng}
            self._criteria_cache[key] = it
        return it
    
    def _next_row(self, it: Dict[str, Any]) -> Optional[Dict[str, Any]]:
        df = it["df"]
        order = it["order"]
        pos = it["pos"]
        if pos >= len(order):
            # reshuffle and reset
            it["rng"].shuffle(order)
            pos = 0
        if len(order) == 0:
            return None
        idx = int(order[pos])
        it["pos"] = pos + 1
        row = df.iloc[idx]
        return row.to_dict()
    
    def sample_puzzle(
        self,
        spec_key: SpecKey,
        min_optimal_length: Optional[int] = None,
        max_optimal_length: Optional[int] = None,
        min_robots_moved: Optional[int] = None,
        max_robots_moved: Optional[int] = None,
        random_seed: Optional[int] = None
    ) -> Optional[PuzzleMetadata]:
        """Sample a single puzzle matching criteria using a cached iterator."""
        it = self._get_or_build_iterator(
            spec_key,
            min_optimal_length,
            max_optimal_length,
            min_robots_moved,
            max_robots_moved,
            random_seed,
        )
        if it is None:
            return None
        data = self._next_row(it)
        if data is None:
            return None
        return PuzzleMetadata.from_dict(data)
    
    def sample_puzzles(
        self,
        spec_key: SpecKey,
        count: int,
        min_optimal_length: Optional[int] = None,
        max_optimal_length: Optional[int] = None,
        min_robots_moved: Optional[int] = None,
        max_robots_moved: Optional[int] = None,
        random_seed: Optional[int] = None
    ) -> List[PuzzleMetadata]:
        """Sample multiple puzzles matching criteria using the cached iterator."""
        it = self._get_or_build_iterator(
            spec_key,
            min_optimal_length,
            max_optimal_length,
            min_robots_moved,
            max_robots_moved,
            random_seed,
        )
        if it is None:
            return []
        out: List[PuzzleMetadata] = []
        for _ in range(int(max(0, count))):
            data = self._next_row(it)
            if data is None:
                break
            out.append(PuzzleMetadata.from_dict(data))
        # Future: implement stratified strategy here
        return out

    def sample_puzzles_stratified(
        self,
        spec_key: SpecKey,
        total_count: int,
        bands: List[Dict[str, Any]],
        random_seed: Optional[int] = None,
    ) -> List[PuzzleMetadata]:
        """Stratified sampling across bands defined by (ol, rm) ranges with weights.

        Args:
            spec_key: Partition to sample from
            total_count: Total number of puzzles requested
            bands: List of dicts with keys:
                - min_optimal_length, max_optimal_length
                - min_robots_moved, max_robots_moved
                - weight (float) optional; defaults to 1.0
            random_seed: Seed for deterministic allocation and iterators

        Returns:
            List of PuzzleMetadata sampled approximately according to weights
        """
        if total_count <= 0 or len(bands) == 0:
            return []
        rng = np.random.default_rng(random_seed)
        weights = np.array([float(b.get("weight", 1.0)) for b in bands], dtype=float)
        weights = np.maximum(weights, 0.0)
        if weights.sum() <= 0:
            weights = np.ones(len(bands), dtype=float)
        weights = weights / weights.sum()

        # Initial integral allocation via floor, then distribute remainder by largest fractional parts
        raw_alloc = weights * int(total_count)
        base_alloc = np.floor(raw_alloc).astype(int)
        remainder = int(total_count) - int(base_alloc.sum())
        frac = raw_alloc - base_alloc
        if remainder > 0:
            order = np.argsort(-frac)
            for idx in order[:remainder]:
                base_alloc[idx] += 1

        plan_details: List[Dict[str, Any]] = []
        out: List[PuzzleMetadata] = []
        actually_sampled = 0
        for i, band in enumerate(bands):
            req = int(base_alloc[i])
            if req <= 0:
                continue
            min_ol = band.get("min_optimal_length")
            max_ol = band.get("max_optimal_length")
            min_rm = band.get("min_robots_moved")
            max_rm = band.get("max_robots_moved")
            # Use separate iterator per band
            it = self._get_or_build_iterator(
                spec_key,
                min_ol,
                max_ol,
                min_rm,
                max_rm,
                int(rng.integers(0, 2**31 - 1)) if random_seed is None else random_seed,
            )
            band_samples: List[PuzzleMetadata] = []
            if it is not None:
                for _ in range(req):
                    data = self._next_row(it)
                    if data is None:
                        break
                    band_samples.append(PuzzleMetadata.from_dict(data))
            out.extend(band_samples)
            actually_sampled += len(band_samples)
            plan_details.append({
                "band": {
                    "min_optimal_length": min_ol,
                    "max_optimal_length": max_ol,
                    "min_robots_moved": min_rm,
                    "max_robots_moved": max_rm,
                    "weight": float(band.get("weight", 1.0)),
                },
                "requested": req,
                "sampled": len(band_samples),
            })

        # Monitoring snapshot for external logging
        self._last_stratified_plan = {
            "total_requested": int(total_count),
            "total_sampled": int(actually_sampled),
            "bands": plan_details,
        }
        return out

    def get_last_stratified_plan(self) -> Optional[Dict[str, Any]]:
        """Return details of the last stratified sampling plan and outcomes."""
        return self._last_stratified_plan


def compute_board_hash(board: Board) -> bytes:
    """Compute deterministic binary hash of a board state.
    
    This avoids JSON serialization entirely and instead hashes a compact
    binary representation composed of geometry, walls, robots, goal and target.
    """
    buf = bytearray()
    # Dimensions
    buf += struct.pack('<II', int(board.height), int(board.width))
    # Walls (raw bytes, deterministic layout)
    buf += board.h_walls.tobytes()
    buf += board.v_walls.tobytes()
    # Robots: sorted by robot id
    for rid in sorted(board.robot_positions.keys()):
        r, c = board.robot_positions[rid]
        buf += struct.pack('<III', int(rid), int(r), int(c))
    # Goal and target
    gr, gc = board.goal_position
    buf += struct.pack('<III', int(board.target_robot), int(gr), int(gc))
    # Compute hash (use SHA-256 as fallback if blake3 not available)
    try:
        return hashlib.blake3(buf).digest()
    except AttributeError:
        return hashlib.sha256(buf).digest()


def compute_layout_hash(board: Board) -> bytes:
    """Compute deterministic binary hash of the board layout excluding target_robot.

    Includes dimensions, walls, robots positions, and goal position, but excludes
    which robot is target to allow multi-round episodes on same layout.
    """
    buf = bytearray()
    buf += struct.pack('<II', int(board.height), int(board.width))
    buf += board.h_walls.tobytes()
    buf += board.v_walls.tobytes()
    for rid in sorted(board.robot_positions.keys()):
        r, c = board.robot_positions[rid]
        buf += struct.pack('<III', int(rid), int(r), int(c))
    gr, gc = board.goal_position
    buf += struct.pack('<II', int(gr), int(gc))
    try:
        return hashlib.blake3(buf).digest()
    except AttributeError:
        return hashlib.sha256(buf).digest()


def count_robots_moved(actions_encoded: List[int]) -> int:
    """Count number of distinct robots moved in action sequence.
    
    Args:
        actions_encoded: List of encoded actions (robot_id * 4 + direction)
        
    Returns:
        Number of distinct robots moved
    """
    robot_ids = set()
    for action in actions_encoded:
        robot_id = action // 4
        robot_ids.add(robot_id)
    return len(robot_ids)


def create_fixed_layout_from_metadata(metadata: PuzzleMetadata) -> FixedLayout:
    """Create FixedLayout from puzzle metadata.
    
    This requires the layout_blob to be present in the metadata.
    
    Args:
        metadata: Puzzle metadata with layout_blob
        
    Returns:
        FixedLayout for use with environment
        
    Raises:
        ValueError: If layout_blob is not available
    """
    if metadata.layout_blob is None:
        raise ValueError("Cannot create FixedLayout without layout_blob")
    return deserialize_layout_blob(metadata.layout_blob)


def create_fixed_layout_from_seed(metadata: PuzzleMetadata) -> FixedLayout:
    """Create FixedLayout by regenerating board from seed.
    
    Args:
        metadata: Puzzle metadata with seed and spec_key
        
    Returns:
        FixedLayout for use with environment
    """
    # Generate board directly from spec and seed without constructing an environment
    seed_to_use = int(metadata.board_seed) if getattr(metadata, 'board_seed', None) is not None else int(metadata.seed)
    board = generate_board_from_spec_with_seed(
        height=metadata.spec_key.height,
        width=metadata.spec_key.width,
        num_robots=metadata.spec_key.num_robots,
        edge_t_per_quadrant=metadata.spec_key.edge_t_per_quadrant,
        central_l_per_quadrant=metadata.spec_key.central_l_per_quadrant,
        seed=seed_to_use,
    )
    
    # Validate hash matches
    computed_hash = compute_board_hash(board)
    if computed_hash != metadata.board_hash:
        # If layout_hash is available, provide additional diagnostics and try layout-level check
        if getattr(metadata, 'layout_hash', None) is not None:
            comp_layout = compute_layout_hash(board)
            if comp_layout == metadata.layout_hash:
                raise ValueError(f"Board target mismatch for seed {seed_to_use}: layout matches but target_robot differs")
        raise ValueError(f"Board hash mismatch for seed {seed_to_use}")
    
    # Create FixedLayout
    return FixedLayout(
        height=board.height,
        width=board.width,
        h_walls=board.h_walls,
        v_walls=board.v_walls,
        robot_positions=board.robot_positions,
        goal_position=board.goal_position,
        target_robot=board.target_robot
    )


def serialize_layout_to_blob(board: Board) -> bytes:
    """Serialize a Board layout into a compact binary blob.

    Schema (little-endian):
    - uint32 height, uint32 width, uint32 num_robots
    - h_walls bytes ( (H+1)*W, bool packed as uint8 )
    - v_walls bytes ( H*(W+1), bool packed as uint8 )
    - for rid in sorted(robots): uint32 rid, uint32 r, uint32 c
    - uint32 goal_r, uint32 goal_c
    - uint32 target_robot
    """
    buf = bytearray()
    h = int(board.height)
    w = int(board.width)
    buf += struct.pack('<III', h, w, int(board.num_robots))
    buf += board.h_walls.astype(np.uint8).tobytes()
    buf += board.v_walls.astype(np.uint8).tobytes()
    for rid in sorted(board.robot_positions.keys()):
        r, c = board.robot_positions[rid]
        buf += struct.pack('<III', int(rid), int(r), int(c))
    gr, gc = board.goal_position
    buf += struct.pack('<II', int(gr), int(gc))
    buf += struct.pack('<I', int(board.target_robot))
    return bytes(buf)


def deserialize_layout_blob(blob: bytes) -> FixedLayout:
    """Deserialize a layout blob into a FixedLayout."""
    offset = 0
    def read(fmt: str):
        nonlocal offset
        size = struct.calcsize(fmt)
        vals = struct.unpack_from(fmt, blob, offset)
        offset += size
        return vals
    h, w, nrobots = read('<III')
    hw_size = (h + 1) * w
    vw_size = h * (w + 1)
    h_walls = np.frombuffer(blob, dtype=np.uint8, count=hw_size, offset=offset).copy().astype(bool).reshape(h + 1, w)
    offset += hw_size
    v_walls = np.frombuffer(blob, dtype=np.uint8, count=vw_size, offset=offset).copy().astype(bool).reshape(h, w + 1)
    offset += vw_size
    robot_positions: Dict[int, Tuple[int, int]] = {}
    for _ in range(int(nrobots)):
        rid, r, c = read('<III')
        robot_positions[int(rid)] = (int(r), int(c))
    gr, gc = read('<II')
    (target_robot,) = read('<I')
    return FixedLayout(
        height=int(h),
        width=int(w),
        h_walls=h_walls,
        v_walls=v_walls,
        robot_positions=robot_positions,
        goal_position=(int(gr), int(gc)),
        target_robot=int(target_robot),
    )

```

```python
"""
Puzzle Bank System for Curriculum Learning

This module provides a storage and retrieval system for precomputed Ricochet Robots
puzzles with their solution metadata. The bank stores puzzles in a lightweight format
using seeds and generation specs, with solution traces for deriving additional features.
"""

from __future__ import annotations

import hashlib
import json
import time
from dataclasses import dataclass, asdict
import struct
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Iterator, Any, Union
import numpy as np
try:
    import pandas as pd
    import pyarrow as pa
    import pyarrow.parquet as pq
except ImportError:
    # Fallback for environments without pyarrow
    import pandas as pd
    pa = None
    pq = None
from collections import defaultdict

from .ricochet_core import Board
from .ricochet_env import FixedLayout, generate_board_from_spec_with_seed


@dataclass(frozen=True)
class SpecKey:
    """Specification key that fully determines board generation given a seed."""
    height: int
    width: int
    num_robots: int
    edge_t_per_quadrant: int
    central_l_per_quadrant: int
    generator_rules_version: str = "1.0"
    
    def to_partition_key(self) -> str:
        """Generate partition key for storage organization."""
        return f"size-{self.height}x{self.width}_numR{self.num_robots}_ET{self.edge_t_per_quadrant}_CL{self.central_l_per_quadrant}_gen{self.generator_rules_version.replace('.', '')}"


@dataclass(frozen=True)
class SolverKey:
    """Solver configuration key for solution canonicalization."""
    algorithm: str = "BFS"
    neighbor_order_id: str = "canonical"  # canonical ordering for deterministic results
    max_depth: int = 50
    max_nodes: int = 100000
    solver_version: str = "1.0"
    
    def to_string(self) -> str:
        """String representation for indexing."""
        return f"{self.algorithm}_{self.neighbor_order_id}_d{self.max_depth}_n{self.max_nodes}_v{self.solver_version.replace('.', '')}"


@dataclass
class PuzzleMetadata:
    """Metadata for a single puzzle in the bank."""
    # Core identifiers
    seed: int
    spec_key: SpecKey
    solver_key: SolverKey
    board_hash: bytes  # blake3 hash of full board state (including target_robot)
    
    # Solution metrics
    optimal_length: int
    robots_moved: int  # count of distinct robot IDs in optimal plan
    nodes_expanded: int
    solve_time_ms: int
    
    # Flags
    solved_within_limits: bool
    hit_depth_limit: bool
    hit_node_limit: bool
    
    # Plan trace (for deriving additional features)
    actions_encoded: List[int]  # action = robot_id * 4 + direction

    # Metadata
    created_at: float
    run_id: str

    # Extended reproducibility
    board_seed: Optional[int] = None  # explicit seed used to generate this board
    layout_hash: Optional[bytes] = None  # hash of layout excluding target_robot
    
    # Optional layout blob (for generator drift recovery)
    layout_blob: Optional[bytes] = None
    
    
    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary for Parquet storage."""
        data = asdict(self)
        # Convert SpecKey and SolverKey to JSON strings for proper storage
        data['spec_key'] = json.dumps(asdict(self.spec_key))
        data['solver_key'] = json.dumps(asdict(self.solver_key))
        # Convert bytes to hex for JSON serialization
        data['board_hash'] = self.board_hash.hex()
        if data.get('layout_hash') is not None:
            data['layout_hash'] = self.layout_hash.hex() if isinstance(self.layout_hash, (bytes, bytearray)) else data['layout_hash']
        if self.layout_blob is not None:
            data['layout_blob'] = self.layout_blob.hex()
        return data
    
    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> PuzzleMetadata:
        """Create from dictionary loaded from Parquet."""
        # Convert back from hex - handle potential NaN/None values
        board_hash = data.get('board_hash')
        if board_hash and isinstance(board_hash, str) and board_hash.strip() and board_hash != 'nan':
            try:
                data['board_hash'] = bytes.fromhex(board_hash)
            except ValueError:
                raise ValueError(f"Invalid board_hash hex string: {board_hash}")
        else:
            raise ValueError("board_hash is required and cannot be empty")
        
        # Handle layout_blob - may be None, NaN, or empty string
        layout_blob = data.get('layout_blob')
        if (layout_blob and isinstance(layout_blob, str) and 
            layout_blob.strip() and layout_blob != 'nan'):
            try:
                data['layout_blob'] = bytes.fromhex(layout_blob)
            except ValueError:
                # Invalid hex string, treat as None
                data['layout_blob'] = None
        else:
            data['layout_blob'] = None

        # Optional fields: board_seed, layout_hash
        if 'board_seed' in data and data['board_seed'] == '':
            data['board_seed'] = None
        if 'layout_hash' in data:
            lh = data.get('layout_hash')
            if lh and isinstance(lh, str) and lh.strip() and lh != 'nan':
                try:
                    data['layout_hash'] = bytes.fromhex(lh)
                except ValueError:
                    data['layout_hash'] = None
            else:
                data['layout_hash'] = None
        
        # Reconstruct SpecKey and SolverKey - handle both dict and string formats
        import ast
        spec_key_data = data['spec_key']
        if isinstance(spec_key_data, dict):
            # Already a dictionary, use as-is
            pass
        elif isinstance(spec_key_data, str):
            try:
                # Try JSON first (for properly formatted JSON strings)
                spec_key_data = json.loads(spec_key_data)
            except json.JSONDecodeError:
                try:
                    # Fallback to ast.literal_eval for Python dict string representation
                    spec_key_data = ast.literal_eval(spec_key_data)
                except (ValueError, SyntaxError) as e:
                    raise ValueError(f"Invalid spec_key format: {spec_key_data}") from e
        else:
            raise ValueError(f"spec_key must be dict or string, got {type(spec_key_data)}")
        data['spec_key'] = SpecKey(**spec_key_data)
        
        solver_key_data = data['solver_key']
        if isinstance(solver_key_data, dict):
            # Already a dictionary, use as-is
            pass
        elif isinstance(solver_key_data, str):
            try:
                # Try JSON first (for properly formatted JSON strings)
                solver_key_data = json.loads(solver_key_data)
            except json.JSONDecodeError:
                try:
                    # Fallback to ast.literal_eval for Python dict string representation
                    solver_key_data = ast.literal_eval(solver_key_data)
                except (ValueError, SyntaxError) as e:
                    raise ValueError(f"Invalid solver_key format: {solver_key_data}") from e
        else:
            raise ValueError(f"solver_key must be dict or string, got {type(solver_key_data)}")
        data['solver_key'] = SolverKey(**solver_key_data)
        
        return cls(**data)


class PuzzleBank:
    """Storage and retrieval system for precomputed puzzles."""
    
    def __init__(self, bank_dir: Union[str, Path], run_id: Optional[str] = None):
        """Initialize puzzle bank.
        
        Args:
            bank_dir: Directory to store the puzzle bank
            run_id: Unique identifier for this computation run
        """
        self.bank_dir = Path(bank_dir)
        self.bank_dir.mkdir(parents=True, exist_ok=True)
        self.run_id = run_id or f"run_{int(time.time())}"
        # In-memory cache of partition DataFrames keyed by partition path
        # This avoids re-reading Parquet/CSV on every query during training
        self._partition_df_cache: Dict[Path, "pd.DataFrame"] = {}
        
        # Create manifest file for tracking partitions
        self.manifest_path = self.bank_dir / "manifest.json"
        self._load_manifest()
    
    def _load_manifest(self) -> None:
        """Load or create manifest file."""
        if self.manifest_path.exists():
            with open(self.manifest_path, 'r', encoding='utf-8') as f:
                self.manifest = json.load(f)
            # Backfill new fields for schema v1.1
            if "version" not in self.manifest or self.manifest.get("version") in {"1.0", 1.0}:
                # Upgrade to 1.1 while preserving existing data
                self.manifest["version"] = "1.1"
            # Ensure partitions dict exists
            self.manifest.setdefault("partitions", {})
            # Ensure new per-partition fields
            for part_key, part_info in self.manifest["partitions"].items():
                part_info.setdefault("histogram", {
                    "by_optimal_length_and_robots_moved": {}
                })
                part_info.setdefault("fingerprints", {})
                part_info.setdefault("dedup", {
                    "method": "none",
                    "bloom": None
                })
        else:
            self.manifest = {
                "version": "1.1",
                "partitions": {},
                "created_at": time.time()
            }
            self._save_manifest()
    
    def _save_manifest(self) -> None:
        """Save manifest file."""
        with open(self.manifest_path, 'w', encoding='utf-8') as f:
            json.dump(self.manifest, f, indent=2)
    
    def _get_partition_path(self, spec_key: SpecKey) -> Path:
        """Get path for a partition."""
        partition_key = spec_key.to_partition_key()
        return self.bank_dir / f"{partition_key}.parquet"
    
    def _load_partition_dataframe(self, partition_path: Path) -> "pd.DataFrame":
        """Load a partition into a pandas DataFrame, using an in-memory cache."""
        # Fast path: cached
        if partition_path in self._partition_df_cache:
            return self._partition_df_cache[partition_path]
        # Load from disk
        if not partition_path.exists():
            df = pd.DataFrame([])
            self._partition_df_cache[partition_path] = df
            return df
        if pa is not None and pq is not None:
            table = pq.read_table(partition_path)
            df = table.to_pandas()
        else:
            df = pd.read_csv(partition_path)
        # Clean NaNs to consistent blanks for string fields
        df = df.fillna('')
        self._partition_df_cache[partition_path] = df
        return df
    
    def get_partition_dataframe(self, spec_key: SpecKey) -> "pd.DataFrame":
        """Public helper to access a cached DataFrame for a spec partition."""
        partition_path = self._get_partition_path(spec_key)
        return self._load_partition_dataframe(partition_path)
    
    def add_puzzles(self, puzzles: List[PuzzleMetadata]) -> None:
        """Add puzzles to the bank.
        
        Args:
            puzzles: List of puzzle metadata to add
        """
        if not puzzles:
            return
        
        # Group by spec_key for partitioning
        by_spec = defaultdict(list)
        for puzzle in puzzles:
            by_spec[puzzle.spec_key].append(puzzle)
        
        for spec_key, spec_puzzles in by_spec.items():
            partition_path = self._get_partition_path(spec_key)
            
            # Convert to DataFrame
            data = [puzzle.to_dict() for puzzle in spec_puzzles]
            df = pd.DataFrame(data)
            
            if pa is not None and pq is not None:
                # Create Arrow table
                table = pa.Table.from_pandas(df)
                
                # Write to Parquet (append if exists)
                if partition_path.exists():
                    existing_table = pq.read_table(partition_path)
                    combined_table = pa.concat_tables([existing_table, table])
                    pq.write_table(combined_table, partition_path)
                else:
                    pq.write_table(table, partition_path)
            else:
                # Fallback to CSV if pyarrow not available
                if partition_path.exists():
                    existing_df = pd.read_csv(partition_path)
                    combined_df = pd.concat([existing_df, df], ignore_index=True)
                    combined_df.to_csv(partition_path, index=False)
                else:
                    df.to_csv(partition_path, index=False)
            
            # Update manifest
            partition_key = spec_key.to_partition_key()
            if partition_key not in self.manifest["partitions"]:
                self.manifest["partitions"][partition_key] = {
                    "spec_key": asdict(spec_key),
                    "created_at": time.time(),
                    "puzzle_count": 0,
                    "histogram": {
                        "by_optimal_length_and_robots_moved": {}
                    },
                    "fingerprints": {},
                    "dedup": {
                        "method": "none",
                        "bloom": None
                    }
                }
            self.manifest["partitions"][partition_key]["puzzle_count"] += len(spec_puzzles)
            self.manifest["partitions"][partition_key]["last_updated"] = time.time()

            # Increment histograms for (optimal_length, robots_moved)
            hist = self.manifest["partitions"][partition_key]["histogram"]["by_optimal_length_and_robots_moved"]
            for puzzle in spec_puzzles:
                ol = int(getattr(puzzle, "optimal_length", -1))
                rm = int(getattr(puzzle, "robots_moved", -1))
                key = f"ol={ol}|rm={rm}"
                hist[key] = int(hist.get(key, 0)) + 1
        
        self._save_manifest()
    
    def query_puzzles(
        self,
        spec_key: Optional[SpecKey] = None,
        min_optimal_length: Optional[int] = None,
        max_optimal_length: Optional[int] = None,
        min_robots_moved: Optional[int] = None,
        max_robots_moved: Optional[int] = None,
        limit: Optional[int] = None,
        random_seed: Optional[int] = None
    ) -> Iterator[PuzzleMetadata]:
        """Query puzzles matching criteria.
        
        Args:
            spec_key: Filter by generation spec
            min_optimal_length: Minimum optimal solution length
            max_optimal_length: Maximum optimal solution length
            min_robots_moved: Minimum number of robots moved
            max_robots_moved: Maximum number of robots moved
            limit: Maximum number of results
            random_seed: Random seed for reproducible sampling (None for random)
            
        Yields:
            PuzzleMetadata matching criteria
        """
        # Determine which partitions to search
        if spec_key is not None:
            partition_paths = [self._get_partition_path(spec_key)]
        else:
            partition_paths = [
                self.bank_dir / f"{partition_key}.parquet"
                for partition_key in self.manifest["partitions"].keys()
            ]
        
        count = 0
        for partition_path in partition_paths:
            # Load via cache (empty DataFrame if not present)
            df = self._load_partition_dataframe(partition_path)
            
            # Apply filters
            if len(df) == 0:
                continue
            mask = pd.Series([True] * len(df))
            
            if min_optimal_length is not None:
                mask &= df['optimal_length'] >= min_optimal_length
            if max_optimal_length is not None:
                mask &= df['optimal_length'] <= max_optimal_length
            if min_robots_moved is not None:
                mask &= df['robots_moved'] >= min_robots_moved
            if max_robots_moved is not None:
                mask &= df['robots_moved'] <= max_robots_moved
            
            # Convert filtered rows to PuzzleMetadata
            filtered_df = df[mask]
            
            # Shuffle the filtered data for random sampling
            if len(filtered_df) > 0:
                filtered_df = filtered_df.sample(frac=1.0, random_state=random_seed).reset_index(drop=True)
            
            for _, row in filtered_df.iterrows():
                if limit is not None and count >= limit:
                    return
                
                puzzle = PuzzleMetadata.from_dict(row.to_dict())
                yield puzzle
                count += 1
    
    def get_puzzle_count(self, spec_key: Optional[SpecKey] = None) -> int:
        """Get total number of puzzles in bank.
        
        Args:
            spec_key: If provided, count only for this spec
            
        Returns:
            Number of puzzles
        """
        if spec_key is not None:
            partition_key = spec_key.to_partition_key()
            return self.manifest["partitions"].get(partition_key, {}).get("puzzle_count", 0)
        else:
            return sum(
                partition.get("puzzle_count", 0)
                for partition in self.manifest["partitions"].values()
            )
    
    def get_stats(self) -> Dict[str, Any]:
        """Get bank statistics.
        
        Returns:
            Dictionary with bank statistics
        """
        total_puzzles = self.get_puzzle_count()
        partition_count = len(self.manifest["partitions"])
        
        # Get distribution stats
        all_puzzles = list(self.query_puzzles(limit=10000))  # Sample for stats
        if all_puzzles:
            optimal_lengths = [p.optimal_length for p in all_puzzles]
            robots_moved = [p.robots_moved for p in all_puzzles]
            
            stats = {
                "total_puzzles": total_puzzles,
                "partition_count": partition_count,
                "optimal_length_stats": {
                    "min": min(optimal_lengths),
                    "max": max(optimal_lengths),
                    "mean": np.mean(optimal_lengths),
                    "std": np.std(optimal_lengths)
                },
                "robots_moved_stats": {
                    "min": min(robots_moved),
                    "max": max(robots_moved),
                    "mean": np.mean(robots_moved),
                    "std": np.std(robots_moved)
                }
            }
        else:
            stats = {
                "total_puzzles": total_puzzles,
                "partition_count": partition_count,
                "optimal_length_stats": None,
                "robots_moved_stats": None
            }
        
        return stats


class BankSampler:
    """Sampler for retrieving puzzles from the bank during training."""
    
    def __init__(self, bank: PuzzleBank, sampling_strategy: str = "uniform"):
        """Initialize bank sampler.
        
        Args:
            bank: Puzzle bank to sample from
            sampling_strategy: Strategy for sampling ("uniform", "stratified")
        """
        self.bank = bank
        self.sampling_strategy = sampling_strategy
        # Cache of pre-filtered, shuffled views for fast per-episode sampling
        # key -> { 'df': DataFrame, 'order': np.ndarray[int], 'pos': int, 'rng': np.random.Generator }
        self._criteria_cache: Dict[Tuple[str, int, int, int, int], Dict[str, Any]] = {}
        self._cache_order: List[Tuple[str, int, int, int, int]] = []
        self._max_cache_entries: int = 128
        # Monitoring
        self._last_stratified_plan: Optional[Dict[str, Any]] = None

    def _criteria_key(
        self,
        spec_key: SpecKey,
        min_optimal_length: Optional[int],
        max_optimal_length: Optional[int],
        min_robots_moved: Optional[int],
        max_robots_moved: Optional[int],
    ) -> Tuple[str, int, int, int, int]:
        # Use partition string plus numeric bounds (None -> sentinel -1)
        part = spec_key.to_partition_key()
        def _n(v: Optional[int]) -> int:
            return -1 if v is None else int(v)
        return (part, _n(min_optimal_length), _n(max_optimal_length), _n(min_robots_moved), _n(max_robots_moved))

    def _get_or_build_iterator(
        self,
        spec_key: SpecKey,
        min_optimal_length: Optional[int],
        max_optimal_length: Optional[int],
        min_robots_moved: Optional[int],
        max_robots_moved: Optional[int],
        random_seed: Optional[int],
    ) -> Optional[Dict[str, Any]]:
        key = self._criteria_key(spec_key, min_optimal_length, max_optimal_length, min_robots_moved, max_robots_moved)
        it = self._criteria_cache.get(key)
        if it is None:
            # Build filtered DataFrame from cached partition
            df = self.bank.get_partition_dataframe(spec_key)
            if len(df) == 0:
                return None
            mask = pd.Series([True] * len(df))
            if min_optimal_length is not None:
                mask &= df['optimal_length'] >= min_optimal_length
            if max_optimal_length is not None:
                mask &= df['optimal_length'] <= max_optimal_length
            if min_robots_moved is not None:
                mask &= df['robots_moved'] >= min_robots_moved
            if max_robots_moved is not None:
                mask &= df['robots_moved'] <= max_robots_moved
            filtered = df[mask]
            if len(filtered) == 0:
                return None
            # Initialize RNG and shuffled order
            rng = np.random.default_rng(random_seed)
            order = np.arange(len(filtered))
            rng.shuffle(order)
            it = {"df": filtered.reset_index(drop=True), "order": order, "pos": 0, "rng": rng}
            # Evict LRU if over capacity
            self._criteria_cache[key] = it
            self._cache_order.append(key)
            if len(self._criteria_cache) > self._max_cache_entries:
                old_key = self._cache_order.pop(0)
                if old_key in self._criteria_cache and old_key != key:
                    self._criteria_cache.pop(old_key, None)
        return it
    
    def _next_row(self, it: Dict[str, Any]) -> Optional[Dict[str, Any]]:
        df = it["df"]
        order = it["order"]
        pos = it["pos"]
        if pos >= len(order):
            # reshuffle and reset
            it["rng"].shuffle(order)
            pos = 0
        if len(order) == 0:
            return None
        idx = int(order[pos])
        it["pos"] = pos + 1
        row = df.iloc[idx]
        return row.to_dict()
    
    def sample_puzzle(
        self,
        spec_key: SpecKey,
        min_optimal_length: Optional[int] = None,
        max_optimal_length: Optional[int] = None,
        min_robots_moved: Optional[int] = None,
        max_robots_moved: Optional[int] = None,
        random_seed: Optional[int] = None
    ) -> Optional[PuzzleMetadata]:
        """Sample a single puzzle matching criteria using a cached iterator."""
        it = self._get_or_build_iterator(
            spec_key,
            min_optimal_length,
            max_optimal_length,
            min_robots_moved,
            max_robots_moved,
            random_seed,
        )
        if it is None:
            return None
        data = self._next_row(it)
        if data is None:
            return None
        return PuzzleMetadata.from_dict(data)
    
    def sample_puzzles(
        self,
        spec_key: SpecKey,
        count: int,
        min_optimal_length: Optional[int] = None,
        max_optimal_length: Optional[int] = None,
        min_robots_moved: Optional[int] = None,
        max_robots_moved: Optional[int] = None,
        random_seed: Optional[int] = None
    ) -> List[PuzzleMetadata]:
        """Sample multiple puzzles matching criteria using the cached iterator."""
        it = self._get_or_build_iterator(
            spec_key,
            min_optimal_length,
            max_optimal_length,
            min_robots_moved,
            max_robots_moved,
            random_seed,
        )
        if it is None:
            return []
        out: List[PuzzleMetadata] = []
        for _ in range(int(max(0, count))):
            data = self._next_row(it)
            if data is None:
                break
            out.append(PuzzleMetadata.from_dict(data))
        # Future: implement stratified strategy here
        return out

    def sample_puzzles_stratified(
        self,
        spec_key: SpecKey,
        total_count: int,
        bands: List[Dict[str, Any]],
        random_seed: Optional[int] = None,
    ) -> List[PuzzleMetadata]:
        """Stratified sampling across bands defined by (ol, rm) ranges with weights.

        Args:
            spec_key: Partition to sample from
            total_count: Total number of puzzles requested
            bands: List of dicts with keys:
                - min_optimal_length, max_optimal_length
                - min_robots_moved, max_robots_moved
                - weight (float) optional; defaults to 1.0
            random_seed: Seed for deterministic allocation and iterators

        Returns:
            List of PuzzleMetadata sampled approximately according to weights
        """
        if total_count <= 0 or len(bands) == 0:
            return []
        rng = np.random.default_rng(random_seed)
        weights = np.array([float(b.get("weight", 1.0)) for b in bands], dtype=float)
        weights = np.maximum(weights, 0.0)
        if weights.sum() <= 0:
            weights = np.ones(len(bands), dtype=float)
        weights = weights / weights.sum()

        # Initial integral allocation via floor, then distribute remainder by largest fractional parts
        raw_alloc = weights * int(total_count)
        base_alloc = np.floor(raw_alloc).astype(int)
        remainder = int(total_count) - int(base_alloc.sum())
        frac = raw_alloc - base_alloc
        if remainder > 0:
            order = np.argsort(-frac)
            for idx in order[:remainder]:
                base_alloc[idx] += 1

        plan_details: List[Dict[str, Any]] = []
        out: List[PuzzleMetadata] = []
        actually_sampled = 0
        band_iters: List[Optional[Dict[str, Any]]] = []
        allocations: List[int] = []
        for i, band in enumerate(bands):
            req = int(base_alloc[i])
            if req <= 0:
                band_iters.append(None)
                allocations.append(0)
                continue
            min_ol = band.get("min_optimal_length")
            max_ol = band.get("max_optimal_length")
            min_rm = band.get("min_robots_moved")
            max_rm = band.get("max_robots_moved")
            # Use separate iterator per band
            it = self._get_or_build_iterator(
                spec_key,
                min_ol,
                max_ol,
                min_rm,
                max_rm,
                int(rng.integers(0, 2**31 - 1)) if random_seed is None else random_seed,
            )
            band_samples: List[PuzzleMetadata] = []
            if it is not None:
                for _ in range(req):
                    data = self._next_row(it)
                    if data is None:
                        break
                    band_samples.append(PuzzleMetadata.from_dict(data))
            out.extend(band_samples)
            actually_sampled += len(band_samples)
            band_iters.append(it)
            allocations.append(req)
            plan_details.append({
                "band": {
                    "min_optimal_length": min_ol,
                    "max_optimal_length": max_ol,
                    "min_robots_moved": min_rm,
                    "max_robots_moved": max_rm,
                    "weight": float(band.get("weight", 1.0)),
                },
                "requested": req,
                "sampled": len(band_samples),
            })

        # Reallocate leftover to bands with remaining availability
        leftover = int(total_count) - int(actually_sampled)
        if leftover > 0:
            for i, it in enumerate(band_iters):
                if leftover <= 0:
                    break
                if it is None:
                    continue
                # Try to draw more from this band until exhausted
                taken = 0
                while leftover > 0:
                    data = self._next_row(it)
                    if data is None:
                        break
                    out.append(PuzzleMetadata.from_dict(data))
                    actually_sampled += 1
                    taken += 1
                    leftover -= 1
                if taken > 0:
                    plan_details[i]["extra_sampled"] = int(plan_details[i].get("extra_sampled", 0) + taken)

        # Monitoring snapshot for external logging
        self._last_stratified_plan = {
            "total_requested": int(total_count),
            "total_sampled": int(actually_sampled),
            "bands": plan_details,
        }
        return out

    def get_last_stratified_plan(self) -> Optional[Dict[str, Any]]:
        """Return details of the last stratified sampling plan and outcomes."""
        return self._last_stratified_plan


def compute_board_hash(board: Board) -> bytes:
    """Compute deterministic binary hash of a board state.
    
    This avoids JSON serialization entirely and instead hashes a compact
    binary representation composed of geometry, walls, robots, goal and target.
    """
    buf = bytearray()
    # Dimensions
    buf += struct.pack('<II', int(board.height), int(board.width))
    # Walls (raw bytes, deterministic layout)
    buf += board.h_walls.tobytes()
    buf += board.v_walls.tobytes()
    # Robots: sorted by robot id
    for rid in sorted(board.robot_positions.keys()):
        r, c = board.robot_positions[rid]
        buf += struct.pack('<III', int(rid), int(r), int(c))
    # Goal and target
    gr, gc = board.goal_position
    buf += struct.pack('<III', int(board.target_robot), int(gr), int(gc))
    # Compute hash (use SHA-256 as fallback if blake3 not available)
    try:
        return hashlib.blake3(buf).digest()
    except AttributeError:
        return hashlib.sha256(buf).digest()


def compute_layout_hash(board: Board) -> bytes:
    """Compute deterministic binary hash of the board layout excluding target_robot.

    Includes dimensions, walls, robots positions, and goal position, but excludes
    which robot is target to allow multi-round episodes on same layout.
    """
    buf = bytearray()
    buf += struct.pack('<II', int(board.height), int(board.width))
    buf += board.h_walls.tobytes()
    buf += board.v_walls.tobytes()
    for rid in sorted(board.robot_positions.keys()):
        r, c = board.robot_positions[rid]
        buf += struct.pack('<III', int(rid), int(r), int(c))
    gr, gc = board.goal_position
    buf += struct.pack('<II', int(gr), int(gc))
    try:
        return hashlib.blake3(buf).digest()
    except AttributeError:
        return hashlib.sha256(buf).digest()


def count_robots_moved(actions_encoded: List[int]) -> int:
    """Count number of distinct robots moved in action sequence.
    
    Args:
        actions_encoded: List of encoded actions (robot_id * 4 + direction)
        
    Returns:
        Number of distinct robots moved
    """
    robot_ids = set()
    for action in actions_encoded:
        robot_id = action // 4
        robot_ids.add(robot_id)
    return len(robot_ids)


def create_fixed_layout_from_metadata(metadata: PuzzleMetadata) -> FixedLayout:
    """Create FixedLayout from puzzle metadata.
    
    This requires the layout_blob to be present in the metadata.
    
    Args:
        metadata: Puzzle metadata with layout_blob
        
    Returns:
        FixedLayout for use with environment
        
    Raises:
        ValueError: If layout_blob is not available
    """
    if metadata.layout_blob is None:
        raise ValueError("Cannot create FixedLayout without layout_blob")
    return deserialize_layout_blob(metadata.layout_blob)


def create_fixed_layout_from_seed(metadata: PuzzleMetadata) -> FixedLayout:
    """Create FixedLayout by regenerating board from seed.
    
    Args:
        metadata: Puzzle metadata with seed and spec_key
        
    Returns:
        FixedLayout for use with environment
    """
    # Generate board directly from spec and seed without constructing an environment
    seed_to_use = int(metadata.board_seed) if getattr(metadata, 'board_seed', None) is not None else int(metadata.seed)
    board = generate_board_from_spec_with_seed(
        height=metadata.spec_key.height,
        width=metadata.spec_key.width,
        num_robots=metadata.spec_key.num_robots,
        edge_t_per_quadrant=metadata.spec_key.edge_t_per_quadrant,
        central_l_per_quadrant=metadata.spec_key.central_l_per_quadrant,
        seed=seed_to_use,
    )
    
    # Validate hash matches
    computed_hash = compute_board_hash(board)
    if computed_hash != metadata.board_hash:
        # If layout_hash is available, provide additional diagnostics and try layout-level check
        if getattr(metadata, 'layout_hash', None) is not None:
            comp_layout = compute_layout_hash(board)
            if comp_layout == metadata.layout_hash:
                raise ValueError(f"Board target mismatch for seed {seed_to_use}: layout matches but target_robot differs")
        raise ValueError(f"Board hash mismatch for seed {seed_to_use}")
    
    # Create FixedLayout
    return FixedLayout(
        height=board.height,
        width=board.width,
        h_walls=board.h_walls,
        v_walls=board.v_walls,
        robot_positions=board.robot_positions,
        goal_position=board.goal_position,
        target_robot=board.target_robot
    )


def serialize_layout_to_blob(board: Board) -> bytes:
    """Serialize a Board layout into a compact binary blob.

    Schema (little-endian):
    - uint32 height, uint32 width, uint32 num_robots
    - h_walls bytes ( (H+1)*W, bool packed as uint8 )
    - v_walls bytes ( H*(W+1), bool packed as uint8 )
    - for rid in sorted(robots): uint32 rid, uint32 r, uint32 c
    - uint32 goal_r, uint32 goal_c
    - uint32 target_robot
    """
    buf = bytearray()
    h = int(board.height)
    w = int(board.width)
    buf += struct.pack('<III', h, w, int(board.num_robots))
    buf += board.h_walls.astype(np.uint8).tobytes()
    buf += board.v_walls.astype(np.uint8).tobytes()
    for rid in sorted(board.robot_positions.keys()):
        r, c = board.robot_positions[rid]
        buf += struct.pack('<III', int(rid), int(r), int(c))
    gr, gc = board.goal_position
    buf += struct.pack('<II', int(gr), int(gc))
    buf += struct.pack('<I', int(board.target_robot))
    return bytes(buf)


def deserialize_layout_blob(blob: bytes) -> FixedLayout:
    """Deserialize a layout blob into a FixedLayout."""
    offset = 0
    def read(fmt: str):
        nonlocal offset
        size = struct.calcsize(fmt)
        vals = struct.unpack_from(fmt, blob, offset)
        offset += size
        return vals
    h, w, nrobots = read('<III')
    hw_size = (h + 1) * w
    vw_size = h * (w + 1)
    h_walls = np.frombuffer(blob, dtype=np.uint8, count=hw_size, offset=offset).copy().astype(bool).reshape(h + 1, w)
    offset += hw_size
    v_walls = np.frombuffer(blob, dtype=np.uint8, count=vw_size, offset=offset).copy().astype(bool).reshape(h, w + 1)
    offset += vw_size
    robot_positions: Dict[int, Tuple[int, int]] = {}
    for _ in range(int(nrobots)):
        rid, r, c = read('<III')
        robot_positions[int(rid)] = (int(r), int(c))
    gr, gc = read('<II')
    (target_robot,) = read('<I')
    return FixedLayout(
        height=int(h),
        width=int(w),
        h_walls=h_walls,
        v_walls=v_walls,
        robot_positions=robot_positions,
        goal_position=(int(gr), int(gc)),
        target_robot=int(target_robot),
    )

```

```markdown
# Work in progress
Contains a description, detailed specifications, discussion, notes, questions and answers etc. all related to the current task

## Current task
Redesign the curriculum environment generation and storage to improve generation speed, reduce wasted compute and future-proof for relevant later changes.

## Plan 
### Scope and intent (quick summary)
- **Goal**: Make the bank-based curriculum the primary path; keep online generation only for smoke tests. Achieve faster precompute, zero online solving during training, robust sampling, and schema/versioning for future changes.

### Current state (from code scan)
- Two curriculum paths exist:
  - **Online filtered env**: `OptimalLengthFilteredEnv` within `CurriculumWrapper` regenerates until constraints met (risk: slow, loops, online solver calls).
  - **Bank-based**: `PuzzleBank` + `BankSampler` + `CriteriaFilteredEnv` via `BankCurriculumManager` (preferred; cached DataFrames, Parquet partitions by `SpecKey`).
- Precompute scripts: `generate_curriculum_bank.py` uses `PuzzleGenerator` from `env/precompute_pipeline.py` and shared specs in `env/curriculum_config.py`.
- Gaps/risks noticed:
  - `layout_blob` deserialization is not implemented; recovery relies on regen-by-seed + hash match.
  - Potential generator drift vs stored `board_hash`/`layout_hash` without explicit `generator_rules_version` discipline.
  - Criteria naming: `criteria['max_depth']` is passed to solver as max search depth; ensure not confused with optimal_length fields.
  - Bank sampling cache growth and memory usage; no eviction policy.

### Proposed phases
1) Requirements & targets
   - Lock target curriculum levels and bounds (optimal length, robots moved) for v1.1.
   - Decide minimum viable bank size per level and overall generation time budget.
   - Make bank-based curriculum default in training scripts and README.

2) Generation speedups (precompute)
   - Add parallel/multiprocess batching to `PuzzleGenerator.generate_puzzles_for_spec` with process-safe appends to Parquet (chunk files then merge).
   - Early rejection: light-weight heuristic before full solve (e.g., bounding-box distance lower bound) to skip hopeless boards.
   - Deduplicate by `layout_hash` during generation to avoid repeated layouts with different seeds.
   - Track and store solver nodes expanded; expose to criteria later.

3) Storage/schema hardening
   - Manifest v1.1: add schema version, generator hash, solver config fingerprint, counts per difficulty band.
   - Enforce `pyarrow` Parquet path; add basic indices (via sorting + metadata) on `optimal_length`, `robots_moved`.
   - Optional: split large partitions into shards (e.g., `...parquet/part-*.parquet`) for append-friendly writes.

4) Sampling and curriculum
   - Use `BankSampler` everywhere in `CriteriaFilteredEnv`; add graceful fallback if a level is underfilled (nearest-band backoff, with logging).
   - Add iterator eviction policy (LRU by criteria key, cap memory).
   - Curriculum advancement remains success-rate based; ensure `BankCurriculumManager.get_stats()` is surfaced in logs.

5) Validation and tests
   - Determinism: regen-from-seed -> `board_hash` and `layout_hash` equality tests.
   - Query filters: unit tests for `PuzzleBank.query_puzzles` and `BankSampler.sample_puzzles` across bounds.
   - End-to-end: precompute a tiny bank; train for N episodes; ensure zero online solve calls; curriculum advances.

6) Migration & tooling
   - Write a migration script to re-tag existing partitions with v1.1 manifest fields; verify hashes; compute missing stats.
   - Backup current bank directories and provide verification report.

7) Monitoring & ergonomics
   - Add concise CLI progress and final summaries; optionally wandb/TensorBoard counters for sampled difficulty bands.
   - Docs: update `README.md` quickstart and `docs/ProgressNotes` once implemented.

### Open questions / decisions needed
- Target bank size per level and acceptable precompute duration on current machine?
  - Answer: target 1000 per level for now - but note that this means 1000 actually solving this criteria; currently we track how many we try and then only a fraciton of this satisfy criteria. Ideally precompute takes less than 10 mins per level, but this may not be achievable.
- Keep both BFS and A* variants in bank or standardize on one? If both, how to encode in `SolverKey` for sampling?
  - Keep option to switch if we want, but default to whichever is faster. They should be producing the same minimum  solves (note that the actual solve found might not be the same, since different solves of same length can exist).
- Minimum/maximum `robots_moved` constraints we actually want during early curriculum?
  - In early curriculum, we will want max 1 robot moved, switch to allowing 2+ after a couple of levels.
- Do we require `layout_blob` serialization now, or is seed+hash regeneration sufficient for the next milestones?
  - I'm unsure on this, but I'm expecting seed+hash is good enough. Feel free to investigate this yourself if you think it's likely to be important for what you choose to do next.
- Memory cap for sampler cache (MB) and eviction policy preferences?
  - We have 16GB ram on CPU to work with, but note that some of this will be taken up by other processes.
- Any near-term changes to generator rules that require bumping `generator_rules_version`?
  - No

### Notes and risks
- Generator drift can invalidate stored hashes; strict versioning and manifest fingerprints mitigate this.
- Parallel Parquet writes can corrupt files if not sharded; prefer shard-per-worker then consolidated manifest.
- Overly strict criteria can starve levels; implement backoff and report.
- Large pandas DataFrames in cache can cause RAM spikes; cap and monitor.

### Next actions (initial)
- Align on level spec targets and bank size per level.
- Prototype small parallel generation on 1–2 specs; measure puzzles/sec and success rate.
- Add starvation backoff in `CriteriaFilteredEnv`/`BankSampler` and log warnings when underfilled.
- Draft manifest v1.1 fields and plan migration script.

## User Questions
- I think at the moment we generate N puzzles for each level and try to solve them, and only store puzzles of interest. I think this leads to a lot of wasted compute. I am wondering if it's somehow better to do something like:
  1) Just solve a load of puzzles, storing the metadata about each one, inculding optimal solve length, number of robots moved in optimal solve, hashes, all the rest of the meta data... They could be stored in whatever format is most useful, but I am wondering if storing by (optimal solve length, num robots moved) pair as a key is useful
  2) Keep track of (or maybe it's easy enough just to count, periodically) how many puzzles exist for each (optimal solve length, num robots moved) pair
  3) Periodically check, for each level in our curricula, whether there are sufficient puzzles which satisfy the criteria, by summing over the permissable (optimal solve length, num robots moved) pairs.
  4) Stop once *either* each level has sufficient puzzles satisfying the criteria *or* a maximum puzzle generation count is met.
  5) Ideally have the option to continue adding to an (already generated) puzzle bank, by skipping seeds which have already been calculated for
Please think about this, and let me know if you think it's possible/works well, or if you have improvements to suggest or think that an alternative method (including just continuing with your suggested implementation) is better. Feel free to push back against me on this, I want to come up with the best strategy so I would appreciate your honest input on how best to move forward.

### Answer / proposed approach
- Short answer: Yes—this “band-first” strategy is good and should reduce waste. We should solve-and-record broadly, then allocate to curriculum levels from an indexed pool rather than pre-filtering per level.

- Concretely:
  - Store all solved puzzles with full metadata (already supported by `PuzzleMetadata`), plus a per-partition histogram keyed by `(optimal_length, robots_moved)` and optionally by coarse wall density bins. Persist histogram into the manifest for O(1) availability.
  - Maintain a `seen_seeds` (or `seen_layout_hashes`) set per `SpecKey` in the manifest to support incremental runs and dedup across sessions. Prefer dedup by `layout_hash` to avoid trivial target-robot variations.
  - Generation stopping rule: run in chunks, after each chunk recompute coverage per curriculum level by summing across admissible bands; stop when all levels ≥ target counts or a global max budget is hit.
  - Selection for training: implement a quota-based sampler per level that draws proportionally from its admissible bands to prevent mode collapse (stratified within level constraints).

- Improvements to make it robust:
  - Sharded Parquet writes (per worker) and post-run consolidation (or leave sharded) to avoid writer contention.
  - Lightweight pre-solve filters (e.g., Manhattan lower bound to goal) to skip obviously-too-long boards and lift effective hit-rate.
  - Track solver nodes expanded and time to allow future cost-aware generation (prioritize specs/bands with best hit-rate per second).
  - Starvation backoff: if a level is underfilled, temporarily widen its admissible bands within a guardrail and record that deviation.

- Why this beats level-by-level pre-filtering:
  - Converts many hard rejections into useful inventory for other levels; reuses compute broadly.
  - Enables global stopping criteria and incremental topping-up without re-solving duplicates.

### Implementation notes (minimal changes needed)
- Add manifest fields: per-partition band histogram, `seen_layout_hashes` bloom or rolling shard, generator/solver fingerprints.
- Extend `PuzzleGenerator` to periodically flush histograms and seen-sets; add an outer controller loop to evaluate stop criteria.
- Extend `BankSampler` to accept per-level band quotas and to consult histograms for availability.

### Trade-offs / risks
- Histograms and seen-sets can grow; use compact structures (counts, approximate bloom) and shard by partition.
- Stratification slightly complicates the sampler but yields smoother training.
- If generator rules change, histograms become incomparable—guard with version pins already proposed.

### Additional questions (for alignment)
- Are we okay with approximate dedup (bloom on `layout_hash`) to bound memory, or do we prefer exact sets with periodic compaction?
  - I think approximation is okay
- Should we include a coarse wall-complexity bin in histograms now, or defer until we see coverage gaps?
  - I think include it for future-proofing but for now we'll be using 2-per quadrant for both the edge-T and central-L squares by default.
- For starvation backoff, what max widening factor of `(optimal_length, robots_moved)` bands is acceptable before we’d rather keep generating?
  - Actually I think initially we'll set this to 'none' (ie, always prefer generating rather than widening)

## Progress
- 2025-09-18: Wrote initial plan, answered alignment questions, and designed band-first generation strategy. Preparing manifest v1.1 changes (histograms, dedup fields) and sampler adjustments.
- 2025-09-18: Implemented manifest v1.1 upgrade path and per-partition histograms `(optimal_length, robots_moved)` in `env/puzzle_bank.py`; backfilled defaults on load and incremented counts in `add_puzzles`. Lint check passed.
- 2025-09-18: Added controller outline `run_band_first_precompute_controller` and `compute_level_coverage_from_histograms` in `env/precompute_pipeline.py` to drive chunked generation using manifest histograms. Lint check passed.
- 2025-09-18: Exposed controller via CLI in `generate_curriculum_bank.py` with flags `--use_controller`, `--target_per_level`, `--max_puzzles_global`, and `--chunk_per_spec`. Lint check passed.
- 2025-09-18: Added stratified sampling API `sample_puzzles_stratified` with monitoring in `BankSampler` (env/puzzle_bank.py) to support per-band quotas and capture outcomes.
- 2025-09-18: Integrated stratified quotas into `CriteriaFilteredEnv` and `BankCurriculumManager` (auto-banding per optimal length) and attached sampling plan to `info` for monitoring.
- 2025-09-18: Fixed None handling for `info` from `env.reset()` in `PuzzleGenerator` to prevent `'NoneType' object has no attribute 'get'` errors during precompute runs.
- 2025-09-18: Ensured the controller respects custom curriculum configs by passing parsed levels through; default `train_agent.py` supports bank curriculum with `--curriculum --use_bank_curriculum` and a precomputed bank.

## Next steps
- Implement manifest v1.1 structure in `env/puzzle_bank.py` (histograms, fingerprints, dedup scaffolding).
- Wire `add_puzzles` to increment histograms and persist.
- Sketch precompute controller loop for chunked generation + stop criteria (no parallel yet).
- Add logging of coverage per curriculum level using histograms.
- Expose controller via CLI (optional new script entrypoint or extend `generate_curriculum_bank.py`).
- Add sampler quotas/backoff logic (stratified within level constraints) and monitoring hooks.
- Integrate stratified quotas into `CriteriaFilteredEnv`/`BankCurriculumManager` and surface monitoring via logs.
- Add unit tests for new sampler method and controller coverage calculation; update README with CLI usage for controller.

## Review #1

- Concerns
  - Bank reproducibility depends on regeneration-from-seed; `layout_blob` deserialization is unimplemented (`create_fixed_layout_from_metadata` raises NotImplementedError). Any generator drift breaks replay despite stored hashes. Suggest prioritizing a compact, versioned layout serialization and round-trip tests.
  - Hash discipline: `SpecKey.generator_rules_version` defaults to "1.0" but there’s no enforcement that code changes bump it. Mismatch between stored `board_hash`/`layout_hash` and regenerated boards will silently fall back to exceptions only at runtime. Add an explicit generator fingerprint in manifest and assert on load.
  - Solver metadata gaps: `nodes_expanded` and node-limit hits are not tracked in `PuzzleGenerator` (always 0/False). This weakens difficulty diagnostics and bank QA. The solvers already support metadata variants; wire them in.
  - Criteria confusion: `precompute_pipeline.PuzzleGenerator.generate_puzzles_for_spec` treats `criteria['max_depth']` as a solver cutoff while curriculum criteria elsewhere refer to optimal_length and robots_moved. Risk of conflation in future changes; rename to `solver_max_depth_override` and keep types distinct.
  - Sampler cache growth: `BankSampler._criteria_cache` is unbounded and never evicted; long runs with varying bands/criteria can grow memory. Add LRU with max entries and metrics.
  - Manifest histograms: counts updated only via `add_puzzles`; no validation against file contents. If external edits occur or duplicates are appended, histograms can drift. Consider recompute/verify utility and duplicate prevention (e.g., unique `(spec_key, board_hash)` index or dedup pass).
  - Stratified sampling coverage: allocation by weights doesn’t check availability before planning, so many bands may underfill silently. Expose deficit and optionally reallocate leftover to other bands to keep `total_sampled` close to `total_count`.
  - Fallback path in `CriteriaFilteredEnv`: on failed sample, it creates a fresh random solvable env, which violates “no online solving during training”. At minimum, log and count separately; ideally, never fallback in training mode and instead surface a clear error/metric to fix bank coverage.
  - RGB obs consistency: `RicochetRobotsEnv` sets fixed pixel size 96 for rgb_image but comments reference 128 in places; ensure all constants are consistent and documented. Also verify channels_first path for rgb returns the exact Box shape.
  - Reward/termination semantics: step uses `step_penalty`, optional `noop_penalty`, and goal reward, but no per-move optimality shaping. That’s fine, but ensure tests cover truncation (`TimeLimit.truncated`) and that success is only set when not truncated (some wrappers rely on this, and `CriteriaFilteredEnv` assumes it).

- Questions/uncertainties
  - Should bank usage in training hard-fail when criteria underfill rather than fallback to online env? If not, what’s the desired policy for deficit reallocation among admissible bands?
  - Target coverage per level: README suggests 1000; do we want adaptive targets based on observed hit-rates per spec to balance total compute?
  - Multiple solvers: keep BFS default, but do we anticipate changing heuristics (e.g., `admissible_one`) during generation? If yes, we need solver fingerprints in manifest and possibly separate partitions or fields.
  - Curriculum advancement: current `BankCurriculumManager` uses windowed success rate. Do we also want an optimality-gap threshold to avoid advancing on sloppy solves when metadata is available?
  - Dedup granularity: prefer `layout_hash` or full `board_hash`? If we allow multi-round use of layouts with different target robots, layout-level dedup seems preferable; confirm.

- Suggested improvements/fixes (high priority)
  - Implement layout serialization/deserialization: define a minimal binary schema (dims, walls, robots, goal, target) with version header; store in `layout_blob` and validate round-trip against hashes. Add tests.
  - Wire metadata-rich solving in precompute: use `solve_bfs_with_metadata`/`solve_astar_with_metadata` to populate `nodes_expanded`, and set `hit_node_limit`/`hit_depth_limit` correctly.
  - Rename/clarify `criteria` usage in precompute controller: use explicit `solver_max_depth_override`; keep curriculum filters separate and enforced only at allocation/coverage.
  - Add LRU cache and limits to `BankSampler` iterators; expose cache size metrics and an API to clear per spec.
  - Enhance stratified sampler: compute availability per band from cached filtered views; reallocate unfilled quota to bands with surplus; record deficits in `plan` for monitoring.
  - Add a `verify_manifest` CLI to recompute histograms from Parquet, detect duplicates, and optionally repair manifest.
  - Make `CriteriaFilteredEnv`’s fallback optional and default to disabled in training; when disabled, raise with a helpful message including coverage stats from manifest.
  - Standardize RGB constants and document in `README.md`; add a unit test that asserts observation space shape and dtype for both orders.

- Nice-to-have / future work
  - Store per-partition Bloom filter for `layout_hash` to avoid regenerating duplicates during long runs; rotate shards to bound size.
  - Record solver time and nodes in manifest aggregates to guide spec prioritization in the controller (puzzles/sec, hit-rate per spec/band).
  - Add quota-based curriculum level sampler that draws proportionally from admissible bands and respects per-band caps to avoid mode collapse.
  - Provide a tiny demo bank in repo for tests/integration to decouple CI from heavy precompute.

- Evidence from tests vs code
  - Tests cover stratified weight allocation and histogram updates; current sampler meets allocation but doesn’t reallocate deficits (acceptable but noted).
  - Precompute controller coverage test only asserts non-negative coverage; expand tests to check exact counts and spec matching with mixed entries.
  - Env solvability tests validate `optimal_length` and solver limits; consistent with implementation.

### Progress
- Layout serialization/deserialization: Implemented compact binary layout blob and round-trip creation via `serialize_layout_to_blob` and `deserialize_layout_blob`. `create_fixed_layout_from_metadata` now uses the blob. Precompute stores `layout_blob` in `PuzzleMetadata`.
- Solver metadata wiring: Switched precompute to use `solve_bfs_with_metadata`/`solve_astar_with_metadata`, recording `nodes_expanded`, `depth`, and computing `hit_node_limit` accurately.
- Precompute controller arg clarity: Renamed internal override to `solver_max_depth_override` and updated controller to pass this key.
- Bank sampler cache: Added LRU eviction to `_criteria_cache` (max 128 entries) and tracked order.
- Stratified sampling: Added leftover reallocation to bands with remaining availability; plan now reports any `extra_sampled` per band.
- Criteria fallback: Added `allow_fallback` flag to `CriteriaFilteredEnv` (default False); when disabled and sampling fails, it raises with guidance instead of silently generating online.
- RGB rendering fix: Internal walls now render correctly via mask overwrite; background/overlay compositing corrected.

```

I implemented the highest-impact changes and recorded progress under Review #1.

- Implemented layout serialization/deserialization
  - Added `serialize_layout_to_blob` and `deserialize_layout_blob` in `env/puzzle_bank.py`.
  - `create_fixed_layout_from_metadata` now uses the blob.
  - Precompute stores `layout_blob` in `PuzzleMetadata`.

- Wired solver metadata into precompute
  - Switched to `solve_bfs_with_metadata` / `solve_astar_with_metadata`.
  - Populated `nodes_expanded`, used `result.depth` for `optimal_length`, set `hit_node_limit` correctly.
  - Kept solver-type mapping in `SolverKey`.

- Clarified solver override naming in controller
  - Controller now passes `solver_max_depth_override`.
  - `PuzzleGenerator.generate_puzzles_for_spec` reads that key.

- BankSampler improvements
  - Added LRU eviction (max 128 cached iterators).
  - Stratified sampler now reallocates leftover quota to bands with availability and reports `extra_sampled`.

- CriteriaFilteredEnv fallback control
  - Added `allow_fallback` (default False). If sampling fails and fallback is disabled, it raises instead of generating online.

- RGB rendering fix already applied earlier; walls now visible.

- Updated `WorkInProgress.md` Review #1 with a Progress subsection listing these edits and their effects.

If you want, I can proceed to add a manifest verify utility next (recompute histograms, detect duplicates) and expand tests for the new serializer and sampler behavior.

