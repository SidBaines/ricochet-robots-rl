# Review implementation of step 2
_Exported on 15/09/2025 at 17:16:13 BST from Cursor (1.4.5)_

---

**User**

You are a research engineer, working as part of a team to implement a research proposal. Please take the time to read the ResearchPlan.md. Your colleagues have already implemented and reviewed steps 1.1 - 1.4 of the plan, which have been checked and are deemed completed, so you do not need to worry about this. You can read the ProgressNotes left by your colleagues, and read the code they have written, in order to understand what you are working with.

Now, another colleague has implemented step 2 of the plan, detailed in ResearchPlan.md.
Your task is to review the implementation of step 2, comparing it to the specification and goals laid out in the ResearchPlan.md. It is unlikely that the plan has been implemented correctly, since this attempt was just a first pass, and you must provide feedback on what need to be changed in order to satisfy the specificaiton laid out in the plan. You should also feel free to think about the plan itself, and let me know if you think there are any issues with step 2 of the plan.

You are not to change any code, but instead to review the work and give a list of feedback, ordered roughly from highest importance to lowest. Be as critical as you can - it's really important that we get this step up to standard to set us up in a way which makes the rest of the research plan implementation as easy and smooth as possible. 

---

**Cursor**

I'll start by opening the research plan and the step 2 progress notes, plus the main training code and related modules, so I can compare the implementation against the spec.

I'll scan the training script, model implementations, evaluation script, tests, and key env pieces in parallel to compare the implemented Step 2 against the plan.

I’ve read the research plan and the step 2 notes, and I’ve reviewed the training script, ConvLSTM, SmallCNN, evaluation, and milestone tests. I’ll quickly scan the recurrent policy and curriculum/test files to ensure the step 2 implementation aligns end-to-end before giving feedback.

Status: I reviewed ResearchPlan step 2 and the current code (training loop, ConvLSTM, recurrent policy, CNN, evaluation, and tests). I’m ready to give prioritized feedback to bring the implementation in line with the spec.

### High-priority issues
- Recurrent PPO integration is not correct
  - The custom `RecurrentActorCriticPolicy` extends `BasePolicy`, but PPO in SB3 assumes specific rollout collection and sequence handling for RNNs (episode masking, sequence batching, truncation lengths for BPTT). In this code:
    - No sequence-aware rollout buffer or mask handling is implemented; the placeholder `RecurrentCallback` does nothing meaningful on `done`s.
    - Hidden state resets are not handled per-env boundary during rollout. This will cause hidden-state leakage across episodes and undefined gradients.
    - Recommendation: either switch to SB3-contrib’s `RecurrentPPO` and its `RecurrentActorCriticPolicy` contracts, or implement a proper sequence-aware rollout buffer with episode masks and truncated BPTT yourself. Don’t train recurrent models with vanilla PPO’s feedforward buffer.
- Action distribution bug in the recurrent policy
  - In `models/recurrent_policy.py` `_get_action_dist_from_latent`, it constructs `CategoricalDistribution` with logits instead of action_dim, then calls `proba_distribution`. Correct usage in SB3 is:
    - `dist = CategoricalDistribution(self.action_space.n)`, then `dist.proba_distribution(action_logits)`.
  - As written, it will be incorrect or crash under many SB3 versions. This must be fixed before any training works reliably.
- PPO policy API compliance risk
  - PPO often relies on attributes like `mlp_extractor` or `get_distribution`. Your policy implements `forward`, `evaluate_actions`, and `predict_values`, which may work if PPO uses the generic API, but this is not guaranteed. You should either subclass `ActorCriticPolicy`/`RecurrentActorCriticPolicy` from SB3(-contrib) with the expected interfaces or ensure all required methods and fields are implemented (e.g., `get_distribution`, consistent `distribution` module usage, optimizer setup integrated with SB3).
- Hidden state lifecycle and env vectorization
  - Hidden states are cached per-batch, but in vectorized envs, individual envs finish at different times. You need per-env hidden states and to reset just those that are done at each SB3 step. Right now, none of that is implemented, which will poison training signals.
- Evaluation script incompatible with recurrent models
  - `evaluate_agent.py` uses `PPO.load(...)` without ensuring the saved policy class matches. If trained with the custom recurrent policy, load must register/restore that class; otherwise, SB3 will attempt to instantiate its default policy. It also doesn’t set or reset hidden states during episode rollouts, which can affect RNN behavior.

### Medium-priority issues
- Missing proper recurrent training tests
  - There are no tests for: hidden state reset on `done`, correctness of per-env hidden-state management in `VecEnv`, RNN training stability on longer episodes, or memory usage. Add tests that:
    - Verify hidden states are zeroed on episode start and after `done` per env.
    - Assert identical behavior whether episodes end synchronously or asynchronously across envs.
- Curriculum learning alignment with solver-based difficulty
  - The plan calls for curriculum levels defined by optimal solution length (via the solver). The CLI and wrappers exist, but it’s unclear if `env.curriculum` advances difficulty using solver-measured move-length thresholds. If not, implement curriculum levels keyed to optimal depth (e.g., ≤3, ≤5, ≤7 moves) and advance based on success rate on the current tier.
- No-op “thinking” action not validated
  - The plan encourages exploring explicit no-op planning. It’s wired in the env and CLI (`--include-noop`, `noop_penalty`), but there’s no training or evaluation test validating no-op usage and cost. Add:
    - A unit test ensuring no-op repeats observations and only applies `noop_penalty`.
    - A training smoke test with no-op enabled that shows the agent can learn to occasionally use it on a simple layout.
- Baselines breadth and parity
  - The plan suggests a CNN/ResNet baseline. There is `SmallCNN` (good), but no ResNet-style option. Consider a small residual CNN extractor to better match literature comparisons and ensure similar feature capacity vs ConvLSTM.
- DRC forward pass details
  - The ConvLSTM stack includes pool-and-inject and a boundary channel (good). However:
    - There is spatial downsampling by 2 before DRC. This is fine for efficiency, but document the impact on grid alignment and ensure tests cover consistent action selection vs no-downsample on small boards.
    - Skip connections config is exposed but not used. If unused, remove it or actually wire skip connections between ConvLSTM layers to align more with DRC variants, or hide the flag to avoid confusion.
- Monitoring and fixed-set evaluation
  - The monitoring/evaluator hooks are great, but the core evaluation script does not compute optimality gap or success on a fixed test set. Provide a simple CLI path to run the fixed-set optimality gap evaluator without the training harness.

### Lower-priority issues and polish
- Image normalization consistency
  - You conditionally enable `normalize_images` only for `rgb_image`. For standard integer grid channels, you disable normalization (good). Ensure the env emits consistent dtype and scale across modes and document the expected range.
- CLI defaults and presets
  - The `research_plan` profile is a nice touch. Consider setting a default `--profile research_plan` for first-time users and emit a warning about long runtimes.
- Documentation and README
  - Add a brief section explaining recurrent-policy caveats: need for SB3-contrib’s recurrent algorithms or custom rollout buffer, how hidden states are managed per env, and known limitations.
- Serialization compatibility
  - Confirm that saving/loading with the custom policy works across processes: register the class and ensure any custom kwargs for `features_extractor` are serialized in SB3’s saved dict.

### What to change to meet Step 2 spec
- Use a supported recurrent PPO path:
  - Prefer SB3-contrib `RecurrentPPO` plus a compliant `RecurrentActorCriticPolicy` integrating your `ConvLSTMFeaturesExtractor` and hidden-state API; or
  - Implement a custom rollout buffer with sequence segments, masks, and truncated BPTT; handle per-env hidden state resets robustly.
- Fix distribution construction in the policy:
  - Instantiate `CategoricalDistribution(self.action_space.n)` and call `proba_distribution(action_logits)`.
- Add tests for recurrent internals:
  - Hidden state reset on `done`, multi-env async terminations, RNN behavior on long episodes, and memory footprint under typical batch sizes.
- Ensure curriculum is solver-informed:
  - Difficulty tiers defined by optimal length; advance via success rate thresholds as per plan.
- Validate “thinking steps”:
  - Add minimal tests and a tiny training run exercising no-op usage; log and plot no-op frequency.
- Add a ResNet-style baseline:
  - Small residual CNN feature extractor selectable via CLI to match plan’s “baseline CNN/ResNet” comparison.
- Tighten evaluation:
  - Provide a script or CLI option to evaluate on a fixed set with optimality gap and save trajectories, independent from the training callbacks.

### Potential plan-level considerations
- SB3 recurrent training vs custom loop
  - The plan assumes we can “just do PPO” with ConvLSTM. In practice, stable recurrent PPO requires either SB3-contrib’s implementations or a custom sequence-aware buffer; mixing a custom recurrent policy with vanilla PPO is brittle. I recommend the plan explicitly require SB3-contrib RecurrentPPO (or a custom loop), to avoid wasting time on framework impedance mismatches.
- No-op thinking steps vs internal repeats
  - The plan offers both approaches. With ConvLSTM repeats already implemented, prioritize internal iterations first for cleaner learning signals. Consider reserving explicit no-op for later experiments to reduce confounds early.
- Training budget realism
  - Single-GPU PPO on complex Ricochet Robots may need more than 5–10M steps to show consistent planning; plan should expect extended runs or stronger curricula, and ensure environment stepping performance is profiled.

Summary
- Critical: fix recurrent PPO integration (use SB3-contrib or implement sequence-aware buffer), correct the distribution API, and implement proper hidden state resets per env. Add solver-aligned curriculum and tests for recurrent behavior and no-op usage. Provide a ResNet baseline and strengthen evaluation tooling. These changes align the current first pass with the Step 2 specification and reduce technical debt before Step 3.

