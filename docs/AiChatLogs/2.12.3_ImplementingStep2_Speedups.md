# Review implementation of step 2
_Exported on 18/09/2025 at 10:54:03 BST from Cursor (1.4.5)_

---

**User**

You are a research engineer, working as part of a team to implement a research proposal. Please take the time to read the ResearchPlan.md. Your colleagues have already implemented steps 1-2 of the plan. Step 1 has been well-checked and are deemed completed, so you do not need to worry about this. You can read the ProgressNotes left by your colleagues, and read the code they have written, in order to understand what you are working with.

Your task is to review the implementation of step 2, comparing it to the specification and goals laid out in the ResearchPlan.md. It is unlikely that the plan has been implemented correctly, since this attempt was just a first pass, and you must provide feedback on what need to be changed in order to satisfy the specificaiton laid out in the plan. You should also feel free to think about the plan itself, and let me know if you think there are any issues with step 2 of the plan.

You are not to change any code, but instead to review the work and give a list of feedback, ordered roughly from highest importance to lowest. Be as critical as you can - it's really important that we get this step up to standard to set us up in a way which makes the rest of the research plan implementation as easy and smooth as possible. 

---

**Cursor**

I'll start by locating and reading the research plan and the key files changed for step 2 so I can compare the implementation to the spec. Then I'll skim relevant progress notes for context.

I'll read the research plan and the step 2 progress notes, plus the curriculum config and training/evaluation scripts to compare the implementation against the spec.

I'll skim the model definitions and the training/evaluation scripts to compare architecture and training flow against Step 2 requirements before drafting the feedback.

I'll skim the step 2 progress notes to see intended design decisions and compare them to the plan before writing feedback.

- I read the research plan (esp. Step 2) and the code/notes for the current Step 2 implementation. Now I’ll give a prioritized review of gaps and concrete fixes.

### High-priority issues

- Recurrent DRC training is not wired correctly to SB3’s rollout/optimization
  - What I see:
    - `RecurrentActorCriticPolicy` extends `BasePolicy` and manages hidden state internally, but SB3’s vanilla `PPO` does not handle recurrent sequence masks or pass `episode_starts` into the policy. There is no custom rollout buffer nor any callback that calls `set_episode_starts`. The comment “remove ad‑hoc recurrent callback” implies this was deliberately left out.
    - `_init_hidden_states` returns `None`, and the extractor is given `hidden_states`, but without proper reset/masking across episodes/minibatches, hidden state will leak between trajectories and batches.
  - Why it matters:
    - This makes training “recurrent” models effectively feed-forward or, worse, incorrect due to state leakage—contradicting Step 2’s aim to train a ConvLSTM/DRC agent properly.
  - What to change:
    - Either:
      - Use `sb3-contrib RecurrentPPO` end-to-end with a policy that conforms to its API (i.e., implement a `CnnLstmPolicy`-style custom policy whose feature extractor is DRC) so sequence handling and masks are correct; or
      - Implement a custom recurrent rollout buffer, teach the collector to track `episode_starts`, pass sequence lengths, and ensure masking/reset in both rollout and minibatch slicing. That’s substantial work—reusing `RecurrentPPO` is likely faster and safer.

- Online curriculum uses solver gating at reset, likely violating Step 2’s “no solver in sampling” guidance
  - What I see:
    - `OptimalLengthFilteredEnv.reset` loops generating puzzles and checks `info['optimal_length']` up to 100 attempts, only if `ensure_solvable` is True.
  - Why it matters:
    - Running the solver at episode generation during training is expensive and can throttle throughput, contrary to Step 2 recommendations which push solver use to offline/bank curation or evaluation.
  - What to change:
    - Prefer bank-based curriculum for Step 2 training runs. If an “online” curriculum must exist, gate difficulty using cheap proxies (board size, wall counts, robots) without invoking optimal-length checks, or cache pre-solved seeds offline and only sample from them online.

- DRC policy/extractor integration deviates from SB3’s expected signatures
  - What I see:
    - `ConvLSTMFeaturesExtractor.forward` returns `(features, new_hidden_states)` and accepts `hidden_states`, which is non-standard for `BaseFeaturesExtractor`. It only works because the custom policy calls the extractor directly. But SB3 expects `forward(observations) -> features` and uses it in other code paths (e.g., value prediction), increasing maintenance risk and corner-case breakage.
  - Why it matters:
    - This tight coupling to a bespoke policy is brittle and doesn’t integrate with SB3 tooling (e.g., evaluation callbacks, policies that expect standard extractors).
  - What to change:
    - If you move to `sb3-contrib RecurrentPPO`, port DRC logic into a policy class that matches its recurrent policy API (with `latent` recurrence handled in the policy, not via extractor). Alternatively, keep extractor pure (features only), and manage hidden state exclusively in the policy.

- Hidden-state resets and batching safety are incomplete
  - What I see:
    - `set_episode_starts` exists but is never called by training. `_apply_pending_episode_starts` zeroes `h/c` with a mask but falls back to dropping all states if mismatched—this will silently degrade to non-recurrent behavior in common vec-env + minibatch scenarios.
  - Why it matters:
    - Even if recurrence “sort of works,” masks must be correct per env index; otherwise training signals are noisy or wrong.
  - What to change:
    - Ensure episode start masks are plumbed from the rollout collector to the policy; or switch to `RecurrentPPO`, which already handles this.

### Medium-priority issues

- Curriculum mechanism split (online vs bank) lacks a clean, unified trainer flow
  - What I see:
    - There are two parallel systems: online `CurriculumWrapper` with solver-based “max_optimal_length” gating, and a bank-based `BankCurriculumWrapper`. The train script chooses either via flags, and eval logic has divergence between them.
  - Why it matters:
    - Step 2 asks for a coherent training framework with curriculum. The bank-based path aligns with speed/scale guidance; the online path harms throughput and adds complexity.
  - What to change:
    - Make bank curriculum the default path for Step 2; keep online curriculum behind an “experimental” flag with warnings. Consolidate eval so both paths share the same success-rate/level logging and frozen-level eval.

- Observation shape stability under bank curriculum is risky
  - What I see:
    - Bank levels’ `SpecKey` may vary grid sizes. Vectorized envs require fixed observation shapes. The script prints a note recommending `--obs-mode rgb_image` but does not enforce fixed-size rendering or resizing.
  - Why it matters:
    - If levels change H×W, the vec-env will break at level transitions or sampler boundaries unless `rgb_image` outputs a consistent image size (not guaranteed from the env code shown).
  - What to change:
    - Enforce `obs_mode=rgb_image` and implement consistent output size (fixed H×W via renderer) across all levels in bank curriculum; or constrain bank levels to same board size for training. Validate with a test that observation shapes never change along a run.

- Missing ResNet baseline called out in Step 2
  - What I see:
    - Baselines are MLP and SmallCNN. ResNet baseline (as in the referenced paper) is absent.
  - Why it matters:
    - Step 2 suggests multiple architectures for comparison. ResNet is a useful non-recurrent baseline for interpretability comparisons and sanity checks.
  - What to change:
    - Add a light ResNet feature extractor option and CLI flag (e.g., `--resnet-small`), with parity in logging and evaluation.

- No-op “thinking” steps exist but are under-specified and under-tested
  - What I see:
    - `include_noop` and `noop_penalty` exist; milestone env factories don’t pass `noop_penalty`, and there’s no curriculum phase toggling no-op availability as suggested in the plan.
  - Why it matters:
    - Step 2 mentions experimenting with explicit planning via no-op steps. If this is a research target, we need a measured setup and logging (usage rates, effect on success/length).
  - What to change:
    - Add tests and logging for no-op usage. Optionally stage a curriculum phase that first disallows no-op then enables it, measuring changes in episode length and success.

- Default curriculum levels have a slight non-monotonicity in max optimal length
  - What I see:
    - Level 2 `max_optimal_length=6`, Level 3 is 5, then Level 4 is 10. Your validator permits a 0.8x dip, but semantically Level 3 being “harder” with a lower max optimal length is odd.
  - Why it matters:
    - Off-by-one/semantics can cause counterintuitive gating and agent plateau.
  - What to change:
    - Revisit level specs so difficulty signals (robots, walls, max optimal length) align monotonically.

### Lower-priority issues and polish

- Policy normalization toggles are inconsistent across modes
  - What I see:
    - For `CnnPolicy`, `normalize_images` is set True only when `obs_mode=="rgb_image"`. For other modes, dtype/scale assumptions differ between custom policy and SB3 built-ins.
  - What to change:
    - Standardize: ensure uint8 inputs are normalized to [0,1] in all image modes, or explicitly convert env observations to float32 and document it.

- Documentation mismatch with implementation
  - What I see:
    - README advertises ConvLSTM as “DRC-lite” and recurrent planning, but in practice the only safe recurrent path is `--convlstm` using `sb3-contrib CnnLstmPolicy`. The `--drc` path likely trains incorrectly as explained above.
  - What to change:
    - Clarify docs: mark `--drc` experimental until it’s integrated with `RecurrentPPO`. Provide recommended commands for recurrent training that are known-good.

- Evaluation vs optimality integration is fragmented
  - What I see:
    - Monitoring hub hooks `optimality_gap` when available, but only bank env attaches it reliably. Step 2’s evaluation section expects solver-computed optimality on a fixed set.
  - What to change:
    - Make the fixed-set + solver optimality evaluation path first-class and documented, with a simple CLI example. Ensure it runs regardless of bank/online curriculum choice.

- Test coverage gaps for recurrent path and curriculum transitions
  - What to change:
    - Add tests that: (a) recurrent policy resets hidden state on episode boundaries across VecEnvs; (b) observation shapes remain constant across a bank curriculum level advance; (c) curriculum advancement occurs when success-rate threshold is met in a controlled mock env.

### Potential concerns with Step 2 plan itself

- Insisting on online, optimal-length-filtered curriculum during training is impractical
  - The plan suggests using solver-gated optimal-length during generation; for single-GPU training this is a major bottleneck. The implemented bank curriculum is the right direction. I’d recommend updating Step 2 to codify “offline curation (bank) only during training; solver reserved for evaluation and bank construction.”

- Recurrent training should standardize on `RecurrentPPO` or equivalent library support
  - Rolling your own recurrent PPO collector/minibatching is non-trivial and error-prone. The plan should explicitly prefer `sb3-contrib RecurrentPPO` for Step 2 to ensure correct sequence handling, and scope any custom recurrent training loop as future work.

- Observation consistency across curricula needs to be explicit
  - The plan should require a fixed observation tensor shape (e.g., RGB 64×64) irrespective of board size to avoid vec-env incompatibilities.

### Concrete next steps (implementation guidance)

- Replace `--drc` training path with a `RecurrentPPO`-compatible custom policy that embeds the DRC core, or temporarily disable `--drc` and recommend `--convlstm` until the policy is correctly integrated.
- Make bank curriculum the default when `--curriculum` is on; emit a warning when `--curriculum` is used without `--use_bank_curriculum`.
- Enforce fixed-size `rgb_image` observations (resize if needed) for bank curriculum; add a guard that fails fast if shapes vary.
- Add a small ResNet extractor and CLI flag.
- Add a fixed-set solver-backed evaluation command example and ensure it logs optimality metrics.
- Expand tests to cover recurrent resets, curriculum advancement, and obs-shape invariance.

- I’ve finished the review. Next, if you want, I can draft a minimal plan to refactor the recurrent training path onto `RecurrentPPO` while keeping your DRC core, and outline the tests needed to validate it.

- Key gaps to fix first:
  - Recurrent training correctness (switch to `RecurrentPPO` integration for DRC).
  - Remove solver from training-time sampling; default to bank curriculum.
  - Enforce stable observation shape under curriculum.

---

**User**

Thanks for the great work. Please can you make the edits to update the step 2 research plan according to your 'potential concerns' - your suggestions all seem great.

---

**Cursor**

I'll update `ResearchPlan.md` to reflect: standardizing on sb3-contrib RecurrentPPO for recurrent policies (including DRC integration), mandating bank-based curriculum with no solver during training-time sampling, and enforcing fixed-size RGB observations for curricula.

```markdown
# Detailed Research Plan for RL Agents on _Ricochet Robots_ with Mechanistic Interpretability

## Project Overview

**Objective:** Train a reinforcement learning agent to play the puzzle game _Ricochet Robots_ and then apply **mechanistic interpretability** techniques to understand _how the trained agent plans its moves_. The project will involve developing a custom RL environment for _Ricochet Robots_, implementing and training neural network agents (potentially using architectures like the Deep Repeating ConvLSTM from prior work[\[1\]](https://arxiv.org/html/2407.15421v3#:~:text=Guez%20et%C2%A0al.%20,encode%20the%20observation%20with%20filters)), and conducting a thorough interpretability analysis (e.g. probes, saliency maps, feature visualization, and causal interventions) on the trained models[\[2\]](https://ar5iv.labs.arxiv.org/html/2411.00867#:~:text=fundamental%20features%20like%20maze%20walls,for%20interactively%20exploring%20layer%20activations)[\[3\]](https://ar5iv.labs.arxiv.org/html/2411.00867#:~:text=specific%20pathways%20and%20decision,the%20behavior%20of%20RL%20models).

We will break the project into clear, sequential steps. Each step is detailed enough to be handed off to LLM-based coding agents to implement. We emphasize modular, well-documented code and intermediate testing at each stage to ensure correctness and flexibility for future modifications. We also include recommendations on frameworks (e.g. using **Python** with **PyTorch** and **OpenAI Gym/Gymnasium** style interfaces) and libraries for both the RL training (e.g. Stable-Baselines3 or a custom training loop) and interpretability (e.g. **Captum** for PyTorch, or custom analysis scripts). A single GPU will be used for training, so we favor efficient implementations and possibly simplified training algorithms (e.g. **PPO/A2C** instead of massively distributed methods) that can still achieve good results[\[4\]](http://proceedings.mlr.press/v97/guez19a/guez19a.pdf#:~:text=a%20distributed%20framework%20and%20the,2).

Below is a comprehensive step-by-step plan:

## Step 1: Implement the Ricochet Robots Environment

**1.1 Define Game Rules & State Representation:** Implement a custom environment simulating _Ricochet Robots_. This puzzle is played on a grid with walls and multiple colored robots. A target location (often associated with one robot’s color) is chosen, and the objective is to move the correct robot to the target using as few moves as possible. Each move consists of selecting a robot and a direction; the robot then _slides in a straight line until it hits a wall or another robot_. Key aspects to decide and implement:

- **Grid and Components:** Represent the board as a 2D grid. Include walls (some fixed layout or random generation) and a set of robots (each with a position and possibly an ID or color). Also include a goal location (and possibly an associated target robot).
- **State Representation Options:**
- _Image-based (RGB or multi-channel) representation:_ We can represent the board as an image array (e.g. a matrix with different values or channels for walls, robots, goal). This was the approach in related work on Sokoban puzzles, where the input to the ConvLSTM was an encoded image of the game board[\[5\]](https://arxiv.org/html/2407.15421v3#:~:text=Guez%20et%C2%A0al.%20,encode%20the%20observation%20with%20filters). Using an image observation will allow us to leverage convolutional neural networks and mirror the setup in the referenced paper (which trained on RGB arrays).
- _Symbolic representation:_ Alternatively, implement a simpler state representation (e.g. a tuple of robot coordinates, with a separate structure for walls). This could be useful for debugging or for models that take structured input. We will keep the code flexible to support either mode (perhaps via an environment parameter obs_mode that can be "image" or "symbolic").
- **Action Space:** Define discrete actions. One convenient encoding is to treat each (robot, direction) pair as an action. For example, if there are 4 robots and 4 directions (up, down, left, right), that would be 16 discrete actions. Another approach is a two-dimensional action space (choose robot index and choose direction separately), but many RL libraries assume a single discrete action integer, so encoding all choices as one integer is simplest. We’ll create a mapping for action index to (robot_id, direction). Additionally, we might include a “_no-op_” action that does nothing; this can be used to allow the agent to _think_ for a step without moving (which could be analogous to the “thinking steps” used by DRC agents[\[6\]](https://arxiv.org/html/2407.15421v3#:~:text=,steps)[\[7\]](https://arxiv.org/html/2407.15421v3#:~:text=With%20each%20internal%20iteration%2C%20the,See%20Section%204) – more on this in the training section).
- **Transition Dynamics (step function):** On each step, apply the chosen action: move the selected robot in the given direction until it hits an obstacle (wall or board edge or another robot). Update the robot’s position. Compute reward (see below) and check if the episode is done. Ensuring that the movement mechanics are correct is critical – we should add **unit tests** for this (e.g., if a robot has a wall two squares up, a move-up action should land the robot just below that wall).
- **Termination Condition:** The episode ends when the target robot reaches the goal location (success) or when a time/move limit is exceeded. _Ricochet Robots_ puzzles are typically single-goal episodes, so once solved, we terminate. We will include a maximum step limit per episode (to prevent infinite play; e.g. 100 or more moves, as puzzles shouldn’t need unlimited moves). Optionally, if using a no-op (think) action, we may also terminate if the agent does too many no-ops.
- **Reward Design:** Use a sparse primary reward signal: for example, +1 (or +10) when the goal is reached, and 0 otherwise. To encourage efficiency, we add a small step penalty (e.g. –0.01 per move or per time-step) so that shorter solutions yield higher cumulative reward. This balances the objective of solving the puzzle with the objective of doing so in minimal moves. (An alternative is giving a –1 reward each step until solved, which directly makes the agent learn to minimize steps.) Ensuring the reward encourages _short solutions_ is important because the agent could otherwise solve slowly without penalty. We will experiment with reward values; this can be adjusted if needed.
- **Observation and Rendering:** Implement reset() to initialize a new random puzzle board. Ensure that each episode generates a puzzle that is _solvable_. We can design a random board generator (placing random walls and robots) but we must verify solvability, which may be non-trivial. A practical approach is to generate a random configuration and run the solver (to be implemented below) to check if a solution exists within a reasonable depth. If not solvable, we regenerate or adjust. For reproducibility and curriculum (later steps), allow seeding of the random generator so that specific puzzles can be regenerated. Also implement a render() method for debugging, which can output a text or graphical representation of the board (e.g., ASCII or using matplotlib to draw the grid, robots, and goal). This will help in debugging the environment and in visualizing agent behavior during evaluation.

**1.2 Implement a Puzzle Solver for Curriculum & Validation:** To facilitate curriculum learning and to evaluate the agent’s performance, we will implement a solver for _Ricochet Robots_. Likely, we’ll implement an **A** _or_ _breadth-first search (BFS)_ _algorithm, since_ Ricochet Robots _is a deterministic puzzle well-suited for graph search. This solver will find the_ optimal solution (minimum moves)\* for a given board configuration if possible:

- Represent states for the solver as needed (e.g., positions of all robots). Use a hashable representation (like a tuple of coordinates) to keep track of visited states.
- We can use BFS for exact minimal solutions, though the branching factor is large (many robot+direction moves). We may need to incorporate some heuristics (for A\*, a possible heuristic is the Manhattan distance of the target robot to the goal, though walls make it non-trivial). There are known fast solvers for Ricochet Robots (even competitions for minimal solutions), but we can start with a straightforward search for small boards.
- Optimize the solver by pruning obviously redundant moves (e.g., if a robot would move and stop immediately because of a wall next to it, that move is pointless; or avoid cycles).
- Because solving can be expensive, we might restrict puzzle size or complexity for training. For example, start with a smaller grid (like 8x8 or 10x10 rather than the full 16x16 board) and fewer robots if needed, so that BFS or A\* can solve it reasonably fast during development. We can increase complexity gradually.
- Use the solver in two ways:
- **Curriculum generation:** We can generate puzzles of increasing difficulty (measured by the optimal solution length). For instance, start with puzzles that require 3-5 moves to solve, then 5-7, and so on. The solver will tell us the minimal moves for a puzzle, so we can filter puzzles by difficulty. This approach mirrors how a curriculum can be applied: solve easy puzzles first, then harder ones.
- **Validation & evaluation:** After training the agent, we’ll use the solver to compare the agent’s solutions to optimal. We can measure _solution optimality gap_ (e.g., agent needed 8 moves, optimal is 5) as a metric. We can also ensure our agent at least solves the puzzles (success rate) and see if it sometimes gets stuck or fails where a solver would succeed.
- We will include **tests for the solver**: e.g., feed it a known puzzle configuration (perhaps a trivial one where the robot is next to the target) and ensure it finds the correct minimal moves. Also test that the solver’s solution when applied to the environment indeed reaches the goal.

**1.3 Integration & Gym Interface:** Wrap the environment in the OpenAI Gym/Gymnasium API format for compatibility with RL libraries. This means implementing reset() (returns initial observation), step(action) (returns next_observation, reward, done, info). Define the observation space and action space properly (e.g., if image observation, spaces.Box with shape \[H,W,channels\]; if using discrete actions, spaces.Discrete(n_actions)). Use the gym registration mechanism if needed (so it can be created with gym.make). Make sure the environment is **deterministic given a random seed** for reproducibility – e.g., use a fixed np.random.RandomState seeded in reset() when needed, or allow passing specific board layouts.

**1.4 Initial Testing of Environment:** Before proceeding to training, test the environment manually or with simple agents: - Write a simple script or unit test to take random actions in the environment and ensure nothing crashes and the logic holds (e.g., a robot indeed stops at walls, etc.). - Use the solver on a few random boards and then simulate those moves in the environment to ensure the environment’s transitions match what the solver expects. - Perhaps implement a trivial agent (like always move a random robot up) to run a few episodes and ensure the episode ends when the goal is reached and the reward is given.

By the end of this step, we should have a robust and efficient _Ricochet Robots_ environment, along with a solver for optimal planning. This provides the foundation for training the RL agent. (If speed becomes an issue for environment stepping, we might consider optimizations such as vectorizing some computations or, in extreme cases, using a faster backend like C++ with Python bindings. However, initially a pure Python implementation is fine. Note that in the Sokoban project, they implemented the environment in C++ using EnvPool for speed[\[8\]](https://arxiv.org/html/2407.15421v3#:~:text=), but we can start simpler.)

## Step 2: Select and Implement the RL Training Framework & Model

**2.1 Choose an RL Training Framework:** Given that we have a single GPU and likely a single-machine setup, we will use a well-tested RL library to simplify training. Two good options are **Stable-Baselines3 (SB3)** or **Ray RLlib**: - _Stable-Baselines3:_ A user-friendly choice with reliable implementations of PPO, A2C, DQN, etc. We can use SB3’s **PPO** (proximal policy optimization) or **A2C** (synchronous advantage actor-critic) algorithms for training. PPO is on-policy and generally stable, making it a good default. SB3 also has a _VecEnv_ mechanism to run multiple environment instances in parallel (which helps utilize the GPU by feeding larger batches). We can start e.g. 8 parallel envs to collect data faster. - _RLlib:_ A more scalable framework that includes IMPALA, A3C, PPO, etc., and supports RNN policies easily. However, it can be more complex to set up. Since we’re not on a cluster and want to keep things straightforward, SB3 is likely the easier path. - We’ll proceed with **SB3 + PPO** for initial training. (We note that the referenced Sokoban paper used the IMPALA (V-trace) algorithm for massive-scale training[\[9\]](https://arxiv.org/html/2407.15421v3#:~:text=We%20train%20each%20network%20for,see%20Section%20A.1), but they also found that the same ConvLSTM architecture could be effectively trained with A3C on a smaller scale[\[4\]](http://proceedings.mlr.press/v97/guez19a/guez19a.pdf#:~:text=a%20distributed%20framework%20and%20the,2). So PPO/A2C on a single GPU should be feasible for our purposes, given we likely won’t reach billions of steps.)

**2.2 Design the Neural Network Architecture:** The agent’s policy-value network needs to handle the board observations and output actions (plus a value estimate for critic). We have a few architecture options, and we can implement multiple to compare:

- **Deep Repeating ConvLSTM (DRC) architecture:** This is the architecture used by Guez et al. (2019) and analyzed in the Sokoban interpretability paper[\[1\]](https://arxiv.org/html/2407.15421v3#:~:text=Guez%20et%C2%A0al.%20,encode%20the%20observation%20with%20filters). It is a _recurrent convolutional network_ specialized for planning tasks:
- It consists of convolutional layers feeding into a stack of ConvLSTM layers (convolutional LSTM units). In DRC(D,N) terminology, _D_ is the number of ConvLSTM layers stacked, and _N_ is the number of times they are “repeated” (iterated) per time-step[\[10\]](http://proceedings.mlr.press/v97/guez19a/guez19a.pdf#:~:text=action%20distribution%20%CF%80t%20and%20a,that%20these%20are%20not%20crucial)[\[11\]](http://proceedings.mlr.press/v97/guez19a/guez19a.pdf#:~:text=full%20network,ADDITIONAL%20DETAILS). For example, DRC(3,3) has 3 ConvLSTM layers and runs them for 3 internal iterations each environment step.
- Before the ConvLSTM core, there are a couple of standard conv layers to encode the raw observation into a spatial feature map (no non-linearity on the final encoding layer, as noted in the Sokoban analysis[\[5\]](https://arxiv.org/html/2407.15421v3#:~:text=Guez%20et%C2%A0al.%20,encode%20the%20observation%20with%20filters)).
- The ConvLSTM core itself can use 32 or 64 channels in each layer (e.g., 32 as used by Guez et al.). We will need to implement a ConvLSTM (PyTorch doesn’t have it built-in, but we can write one or use an existing open-source implementation). Key features from prior work: they used _pool-and-inject_ (feed global pooled info to each step) and a boundary padding channel[\[12\]](https://arxiv.org/html/2407.15421v3#:~:text=)[\[13\]](https://arxiv.org/html/2407.15421v3#:~:text=To%20avoid%20convolution%20edge%20effects,padded%2C%20maintaining%20the%20output%20size), but we might initially implement a basic ConvLSTM without those extensions, and consider adding them if needed.
- The output of the ConvLSTM (the hidden state of the last ConvLSTM layer after the N iterations for the time-step) goes through a small feedforward head. Typically, this means flattening or global pooling the spatial output and then using a fully-connected layer (or two) to produce: (a) the policy logits for each action, and (b) the value estimate. The Sokoban network used 256-unit FC and then separate heads for policy and value[\[14\]](https://arxiv.org/html/2407.15421v3#:~:text=).
- _Thinking steps:_ The DRC can effectively perform computation over multiple internal iterations per environment step. If we want to emulate that, we have two approaches:
    1. **Implicit (via model):** We can design the model to internally apply the ConvLSTM stack multiple times each step. (This is complicated to integrate with standard RL libraries, which usually call the network once per step. We might have to custom-implement the policy to loop internally N times. Alternatively, we could simply treat each LSTM pass as a time-step if we allow the environment to feed the same observation repeatedly – see next point.)
    2. **Explicit (via environment actions):** Introduce a _NOOP/thinking_ action (as noted earlier) that causes the environment to resend the same observation without changing state. The agent can learn to use this action to “think” for extra steps. In fact, prior research observed agents _deliberately taking no-op cycles to think_ when it was beneficial[\[15\]](https://arxiv.org/html/2407.15421v3#:~:text=the%20RNN%20has%20a%20causal,distribution%20Sokoban%20puzzles)[\[7\]](https://arxiv.org/html/2407.15421v3#:~:text=With%20each%20internal%20iteration%2C%20the,See%20Section%204). We might incorporate this by adding a no-op action with a slight negative reward (to discourage endless stalling but not as much as a normal move). This way, the agent has the option to spend a few steps to process the situation more with its ConvLSTM before making a real move. This explicit approach might be easier to implement than modifying the network’s internal loop. We should allow this only if needed – it's an advanced feature to explore planning behavior (Finding 3 from the Sokoban analysis was exactly that “pacing” behavior of delaying moves to refine the plan[\[16\]](https://arxiv.org/html/2407.15421v3#:~:text=With%20each%20internal%20iteration%2C%20the,See%20Section%204)).
 - We will implement the DRC-based model in PyTorch. For recurrent training we will standardize on the SB3-contrib _RecurrentPPO_ implementation and integrate the DRC core into a policy compatible with that API. This ensures correct sequence collection, masking, and optimization. We will not build a custom recurrent rollout/optimization loop in Step 2; any bespoke recurrent trainer would be considered future work.
- **Baseline CNN/ResNet architecture:** In addition to the recurrent model, we will implement a simpler feed-forward policy as a baseline. This helps for comparison and for debugging (since a simpler model might train faster initially).
- One baseline is a **ResNet-based CNN** (in fact, Guez et al. 2019 also trained a non-recurrent ResNet agent as a baseline[\[17\]](https://arxiv.org/html/2407.15421v3#:~:text=This%20is%20a%20convolutional%20residual,The%20ResNet%20has%20M%20parameters)). We can implement a small ResNet or just a multi-layer CNN that processes the board image and then flattens to output actions and value. It won’t have memory of past steps beyond what can be inferred from the single observation (which in a fully observable puzzle like this might be sufficient to solve anyway). The ResNet from literature had ~9 convolutional blocks[\[18\]](https://arxiv.org/html/2407.15421v3#:~:text=This%20is%20a%20convolutional%20residual,The%20ResNet%20has%20M%20parameters); we can use fewer if the observation size is smaller. We likely don’t need something extremely deep; a few conv layers + a couple of residual connections could suffice.
- Another baseline is an **MLP** (if using a symbolic state representation). If we encode the state as positions, an MLP could take those as input. This is less natural for image observations but could be tried for a low-dimensional state version.
- These baseline models will be easier to integrate with SB3 (since SB3 by default supports feed-forward policies). We can train one of these first to ensure the environment and reward are workable (e.g., see if a CNN can learn to solve simple puzzles), and then move to the ConvLSTM which is more complex.
- **Model Interfaces and Configurability:** We will design our code such that different models can be selected. Perhaps define a configuration or use command-line arguments to choose “conv_lstm” vs “cnn” vs “mlp” policy. The training script can then instantiate the appropriate policy network. By keeping them in a common interface (e.g., all are subclasses of a common PolicyNetwork that outputs action probabilities and value), the training loop can remain the same. This modularity ensures if we want to try a different architecture (say a Transformer or a larger model) later, we can plug it in without rewriting the whole pipeline.

**2.3 Training Hyperparameters & Strategy:** Set up the RL training loop (using PPO from SB3; for recurrent models use SB3-contrib RecurrentPPO): - **Hyperparameters:** We will likely start with defaults from SB3’s PPO for similar environments, but we will tune as needed. Key parameters: - _Learning Rate:_ ~3e-4 is a common starting point for PPO. Possibly use a scheduler to decay it over time. - _Discount factor (gamma):_ 0.99 (or maybe 0.995 if we expect puzzles to last many steps). - _PPO clip range:_ ~0.2 (default). - _GAE lambda:_ 0.95 (default). - _Entropy bonus:_ a small coefficient (e.g., 0.01) to encourage exploration. This was used in Sokoban training[\[19\]](https://arxiv.org/html/2407.15421v3#:~:text=transitions%20on%20256%20actors%20using,to%20at%20the%20end%20of) (they had an entropy coefficient ~0.01). - _Value loss coefficient:_ e.g., 0.5 (standard). - _Batching:_ If using parallel envs, say 8 envs \* 128 steps per update = 1024 samples per PPO batch. We can use minibatch size 256 and 4 optimization epochs per batch (for PPO). - _Recurrent settings:_ If using a recurrent policy (RecurrentPPO), set the rollout length for BPTT appropriate to episode lengths (e.g., truncate at 50 if needed for memory). - _Gradient clipping:_ PPO often uses grad norm clip ~0.5 or 1.0. - _Reward normalization:_ We likely disable reward normalization if using sparse rewards, to not mess up the scale too much (or at least be cautious). - All these can be adjusted based on preliminary experiments.

- **Training Duration:** We don’t have a strict timeframe, but we can estimate how many environment steps might be needed. The referenced Sokoban agent was trained with billions of frames for near-perfection[\[9\]](https://arxiv.org/html/2407.15421v3#:~:text=We%20train%20each%20network%20for,see%20Section%20A.1), but we won’t reach that on a single GPU. We might start with something like 5e6 to 1e7 time steps as a target, and see what performance is reached. We will monitor the learning curves to decide if more is needed. (This could take on the order of days of training on one GPU, depending on how fast each step is. We can optimize or reduce complexity if it’s too slow.)
- **Curriculum Learning Integration:** We will incorporate **curriculum learning** by using a bank-based curriculum constructed offline. Concretely:
  - We will not invoke the solver during training-time sampling. Instead, we will precompute a puzzle bank with metadata (e.g., optimal lengths) and sample from it during training according to curriculum criteria. This preserves throughput on a single GPU/CPU setup.
  - If an online curriculum is explored, it must avoid solver calls during reset and rely only on cheap proxies (e.g., board size, wall density, number of robots). Online solver-gated curricula are considered experimental and out-of-scope for Step 2.
  - Observation shape must remain constant across curriculum levels to be compatible with vectorized environments. We will standardize on fixed-size RGB images (e.g., `rgb_image` with a fixed H×W). If board sizes vary, observations will be resized/padded to a consistent resolution before reaching the policy.
  - We will plan curriculum steps (e.g., 1 robot → add walls → multiple robots → full complexity) and advance automatically when success rate exceeds a threshold, measured over a sliding window.
  - As an optional secondary axis, we may stage training phases that first disable the no-op “thinking” action and later enable it to study explicit planning behavior. This will be accompanied by logging to quantify no-op usage and its effect on solution length/success.
- **Algorithm Implementation:** We will write a training script train_agent.py. If using SB3, this will:
- Parse configuration (which algorithm, which model, environment settings).
- Instantiate the environment (possibly wrapped in a vector with multiple copies).
- If using curriculum, possibly create a wrapper or a custom callback that can adjust the environment difficulty.
- Instantiate the RL model (e.g., ppo = PPO(CustomPolicy, env, hyperparams…) if we have a custom policy class).
- Train for the desired number of timesteps, using SB3’s model.learn() mechanism. We can utilize callbacks for logging or for curriculum adjustments.
- Save the final model (and perhaps periodic checkpoints).
- We will also save training logs. SB3 by default can use TensorBoard for monitoring; we should integrate that to track metrics like episode reward, success rate (we can log success as part of info dict in env and have a callback to record it), loss values, etc. These logs will help ensure training is progressing.
- **Intermediate Checks:** It is important to frequently verify that training is working:
- We will monitor the average episode return and success rate. Initially, with random weights, the success rate will be near 0. We expect over time the agent starts solving simple puzzles.
- If we see no progress, we might need to adjust hyperparameters or reward structure.
- We can evaluate the current model every N timesteps on a set of puzzles (maybe using a callback that runs a fixed evaluation environment and computes success rate). This is similar to how one might monitor performance on a validation set.
- We can also print out example trajectories periodically: have the agent play one episode greedily and output the moves taken, to qualitatively see if it's learning reasonable behavior (e.g., does it move towards the goal or is it stuck in random moves?).
- If using curriculum, verify that at a given stage the agent indeed reaches high success before moving on.

**2.4 Single-GPU Considerations:** With one GPU, our training will be scaled-down compared to massive distributed runs. This means: - We might not reach the superhuman performance of the large-scale experiments, but we should aim for a reasonably competent agent that solves a good fraction of puzzles. - We should be mindful of GPU memory (especially if using ConvLSTM – the model might be somewhat large). But with a single environment step at a time, it should be fine; using 8 parallel envs and small batch should also fit in memory easily for a model with a few million parameters. - Training speed: The environment might become a bottleneck if written in pure Python. To mitigate this: - Do not call the solver during training-time sampling. Restrict solver usage to offline bank construction and evaluation/analysis. During training, the env just simulates moves and returns rewards. - Prefer bank-based curricula to avoid solver overheads, and enforce fixed observation size for compatibility with vectorized envs. - We can compile some parts with numpy or numba if needed to speed up the movement logic if profiling shows it’s slow. - Ensure vectorized env is used so that we utilize parallel CPU cores for multiple env instances.

By the end of Step 2, we should have: - A working training script that can launch the training of an agent. - The ability to choose architecture and difficulty level. - Logging to observe progress. - Possibly an initial model (from a short training run on an easy variant) to confirm everything works end-to-end.

## Step 3: Training the Agent (Execution & Monitoring)

**3.1 Run Training and Adjust if Needed:** With the code in place, execute the training process. Likely, we will run multiple experiments: - Start with a simpler scenario to sanity-check learning. For example, train a small CNN agent on a 1-robot, no-wall version (the robot just needs to go straight to a goal). This should be a trivial task and solve quickly, confirming our reward and action logic are correct. We expect the agent to learn to go directly to the goal in a few thousand steps for such a trivial case. - Gradually introduce full complexity (multiple robots, walls) and possibly use the curriculum plan. We may run the training in stages, as discussed.

**3.2 Monitor Learning Curves:** Use TensorBoard or logs to watch: - _Episode reward:_ should trend up over time (from near 0 towards closer to 1 per episode if success becomes common and especially if step penalties are small). - _Success rate:_ we can define success = reaching goal. We want this to go from 0 towards maybe 0.8 or higher on the training distribution. - _Episode length:_ ideally decreases as agent solves faster (if we penalize steps). - If using no-op actions, monitor their usage – does the agent actually use them meaningfully (e.g., maybe early on it won’t, but later it might if it finds benefit).

If the agent is not learning (flat curves), we will debug: - Check if rewards are too sparse – maybe increase the goal reward or add a small shaping (like a reward for moving closer to goal – though designing that for this puzzle might be complex and could interfere with learning). - Ensure the exploration is sufficient. If necessary, increase entropy bonus or try an epsilon-greedy exploration if using DQN (though PPO with entropy is usually fine). - Possibly adjust the curriculum to easier tasks. - Check if there’s a bug in the environment or action encoding.

**3.3 Save Models and Training Artifacts:** We will save periodic checkpoints (e.g., every 500k steps) so we can later analyze different stages of learning or recover from crashes. Ensure the random seeds for experiments are logged so results are reproducible.

**3.4 Final Model Selection:** After training for the planned number of steps (or when we see performance leveling off), we select the final policy model for interpretability analysis. If we trained multiple architectures (say ConvLSTM and CNN baseline), we might choose the better-performing one or analyze both for comparison. The plan in the original paper was to use the best-performing network (they specifically interpret the best network from Guez et al. 2019[\[20\]](https://arxiv.org/html/2407.15421v3#:~:text=In%20this%20work%2C%20we%20focus,for%20another%20DRC%20network%20and)). So likely we will focus on the ConvLSTM agent if it is working well. If not, a well-performing CNN agent can also be analyzed to see if it developed any form of implicit planning.

At this stage, we should have at least one trained RL agent that can solve _Ricochet Robots_ puzzles to a reasonable degree. We move on to evaluating its behavior and then digging into its internals.

## Step 4: Evaluate the Trained Agent and Ensure Correctness

Before jumping into interpretability, it’s important to quantitatively and qualitatively evaluate the agent’s behavior:

- **Success Rate and Optimality:** Using the solver from Step 1, measure the agent’s performance on a test set of puzzles:
- Create a set of, say, 100 random puzzles (of various difficulties). For each puzzle, have the agent (with no exploration noise, i.e., take greedy actions w.r.t. its policy) attempt to solve it. Record whether it reaches the goal and how many moves it uses.
- Use the solver to find the optimal moves for those puzzles (we might limit to puzzles solvable within a certain depth so the solver returns results promptly). Compare the agent’s moves to the optimal. We can compute statistics like average solution length vs optimal, and how often the agent finds the optimal or near-optimal solution.
- If the agent fails on some puzzles, examine if those are just very hard or if there’s a pattern (e.g., does it struggle when the solution requires moving a non-target robot out of the way first, etc.).
- **Generalization Tests:** Test the agent on some out-of-distribution setups if applicable:
- For example, if during training we never had a certain board size or configuration, try something new (maybe a larger board or a different wall pattern) to see if the agent can adapt or if it fails. The Sokoban analysis showed that a good planning agent (DRC) could generalize to larger boards beyond training distribution[\[15\]](https://arxiv.org/html/2407.15421v3#:~:text=the%20RNN%20has%20a%20causal,distribution%20Sokoban%20puzzles)[\[21\]](https://arxiv.org/html/2407.15421v3#:~:text=,DRC%20causally%20represents%20its%20plan). It might be interesting to see if ours does anything similar.
- We could also test “misgeneralization”: for instance, if in training the target was often a certain color or location, does the agent develop a bias? (The maze example showed a bias always moving top-right[\[2\]](https://ar5iv.labs.arxiv.org/html/2411.00867#:~:text=fundamental%20features%20like%20maze%20walls,for%20interactively%20exploring%20layer%20activations).) In our context, if a certain robot was often the target, maybe the agent habitually moves that robot even when it’s not the target. We can design a test to check that: e.g., sometimes deliberately make a different robot the target and see if it still erroneously moves the old favorite robot.
- If we included the no-op thinking action, check if the agent uses it in evaluation. Does it take a pause at the start of difficult puzzles to plan (like the “thinking for extra steps at beginning” behavior[\[22\]](https://arxiv.org/html/2407.15421v3#:~:text=Guez%20et%C2%A0al.%20,between%20plans%20and%20search%20algorithms)[\[7\]](https://arxiv.org/html/2407.15421v3#:~:text=With%20each%20internal%20iteration%2C%20the,See%20Section%204))? We can measure how many no-ops it does and whether that correlates with puzzle complexity.
- **Qualitative Analysis:** Observe some episodes:
- Use the environment’s render function or printouts to trace what the agent does in a few representative scenarios. Do the moves seem to follow a logical plan? For example, if the target is in a corner, does the agent navigate walls intelligently and maybe use other robots as anchors, etc.? This can give us hints about what strategy it has learned (and will guide what to look for in interpretability).
- Look for any weird behaviors (does it ever loop needlessly, does it get stuck circling, etc.). If such behaviors exist, note them as targets for interpretability analysis (we might ask: why does the agent sometimes do X? Is there an artifact in its network that causes that?).

If the agent’s performance is unsatisfactory (e.g., very low success), we might need to revisit training (adjust hyperparameters or train longer). Assuming we have a decently performing model, we proceed to the core interpretability work.

## Step 5: Mechanistic Interpretability Analysis of the Agent

This is the key research phase where we “open up” the trained model to understand its internal mechanisms for planning. We will undertake a series of analyses, each as a subtask. The overall goal is to identify _what concepts or features the network has learned (e.g., does it recognize walls, goals, etc.), how it represents plans or subgoals internally, and how those internal representations drive its decisions._ We draw inspiration from prior interpretability studies on RL agents: - Tristan et al. (2024) who identified features like walls and observed goal misgeneralization biases via saliency and feature visualization[\[2\]](https://ar5iv.labs.arxiv.org/html/2411.00867#:~:text=fundamental%20features%20like%20maze%20walls,for%20interactively%20exploring%20layer%20activations). - The Sokoban RNN analysis which found a _causal plan representation_ in hidden states and verified it by predicting future actions and intervening on hidden states[\[23\]](https://arxiv.org/html/2407.15421v3#:~:text=prior%20work%2C%20we%20analyze%20a,distribution%20Sokoban%20puzzles)[\[21\]](https://arxiv.org/html/2407.15421v3#:~:text=,DRC%20causally%20represents%20its%20plan). - Techniques like _activation patching_ to test causal roles of neurons[\[3\]](https://ar5iv.labs.arxiv.org/html/2411.00867#:~:text=specific%20pathways%20and%20decision,the%20behavior%20of%20RL%20models). - _Probing_ intermediate layers with simple classifiers to detect encoded information[\[24\]](https://arxiv.org/html/2407.15421v3#:~:text=,net%2Fforum%3Fid%3DPPjSKy40XUB). - Visualization of convolutional filters and their activations to see what they respond to (as done with maze walls, pathways, etc.[\[25\]](https://ar5iv.labs.arxiv.org/html/2411.00867#:~:text=To%20confirm%20our%20hypothesis%2C%20we,These%20are%20shown%20in%20Fig%C2%A03)).

We will structure the interpretability work into several steps, each potentially implemented as separate scripts or notebooks:

**5.1 Extract Activations and Trajectories:** First, set up the ability to run the trained model in a controlled way and record its internal activations. Specifically: - Modify the agent’s model forward pass to _hook_ into intermediate layers. In PyTorch, we can register forward hooks on layers of interest (e.g., the ConvLSTM hidden state at each step, or activations of specific conv layers). - Run a variety of episodes through the model and record: - The observation (board state) at each time step. - The action taken. - The model’s policy output (the probabilities or logits for actions) and value output. - Activations of key internal units (e.g., each ConvLSTM layer’s hidden state vector after each step, or specific neurons we want to track). - Store this data for analysis, possibly in a structured form (like a pandas DataFrame for high-level stats, and NumPy arrays or PyTorch tensors for the activations). - We should gather data on both puzzles the agent solves and perhaps some it fails (if any) to see differences. - Also include some known optimal plans from the solver for comparison: e.g., record the agent’s trajectory vs the optimal trajectory for the same puzzle.

This data collection is essential for the subsequent probing and analysis. We’ll likely integrate this into an analyze_agent.py script that can load a saved model and run it on a set of test levels while capturing internals.

**5.2 Feature Probing (Intermediate Representations):** We will apply **probe tasks** to the activations: - Train simple classifiers or regressors on the collected data to predict certain _known features_ of the state or solution from the model’s internal activations. This follows the idea of _linear classifier probes_[\[24\]](https://arxiv.org/html/2407.15421v3#:~:text=,net%2Fforum%3Fid%3DPPjSKy40XUB), which is a common way to test if a concept is encoded in a network’s layer. For example, we can try: - **Wall/Obstacle Detection:** Does any part of the network explicitly encode the layout of walls? For instance, take the conv layer outputs or LSTM hidden state, and see if you can linearly reconstruct a map of walls from them. (Tristan et al. found neurons that respond to walls in a maze[\[25\]](https://ar5iv.labs.arxiv.org/html/2411.00867#:~:text=To%20confirm%20our%20hypothesis%2C%20we,These%20are%20shown%20in%20Fig%C2%A03).) - **Goal & Robot Positions:** Test if the network’s hidden state encodes the position of the target and/or the key robot. Because the network sees the entire board, it might internally localize the goal. We can train a probe that takes the hidden state and predicts, say, the coordinates of the goal or the distance of the target robot to the goal. - **Plan/Next Moves:** A crucial probe is to see if the model’s hidden state contains a **plan for future moves**. The Sokoban RNN paper did this by training a probe to predict the agent’s future actions (sequence of moves) from the current state[\[23\]](https://arxiv.org/html/2407.15421v3#:~:text=prior%20work%2C%20we%20analyze%20a,distribution%20Sokoban%20puzzles). We can do something similar: from the hidden state at time _t_, try to predict the subsequent moves the agent will take (perhaps a few steps ahead). If the agent is truly planning, there should be some encoding of the intended path. For example, we could train a probe to predict whether the next move will be, say, “move red robot up”, or even the next _k_ moves. If a simple model (like a logistic regression or small MLP) can do this with high accuracy, it suggests the plan is _linearly present_ in the hidden state. - We can use labels either from the agent’s actual future actions or from the _optimal solver’s_ actions. Comparing both could be insightful (maybe the agent’s internal plan aligns with its own behavior, but is that plan also aligned with an optimal plan or is it doing something else?). - **Subgoals or Intermediate Objectives:** We might guess certain subgoals such as “robot X needs to reach position Y”. If we can derive some binary feature for subgoal (like is robot at half-way point), we could probe for that. This is more speculative, but if we notice in behavior that the agent seems to, for example, move a blocking robot aside as a subgoal, we can test if there’s a neuron that fires when a robot is in place or when a path is clear. - We will keep these probes simple (linear models or one-layer MLPs) to focus on linear separability. We’ll use a portion of our collected trajectories as training data for the probes and the rest for validation to avoid overfitting. - If a probe achieves good accuracy on a feature, that means the network’s activations encode that feature. For instance, if a probe can perfectly tell the Manhattan distance to the goal from the LSTM state, then the model likely computes that internally. - Particularly, if a “future moves” probe can predict several moves in advance with high accuracy, it indicates an explicit plan representation (which was exactly a finding for the Sokoban DRC: it predicted ~50 steps ahead with high accuracy[\[26\]](https://arxiv.org/html/2407.15421v3#:~:text=50%20steps%20in%20advance,Leveraging%20these%20insights%2C%20we%20extend)). We will document such findings.

**5.3 Saliency Mapping (Input Attribution):** Use **saliency and attribution techniques** to see _what parts of the input the network is focusing on_ when making decisions: - Techniques like **grad-CAM**, **integrated gradients**, or simple **gradient saliency** can be applied. Since our input is an image/grid, we can compute the gradient of the chosen action’s logit (or the value) with respect to the input pixels. The magnitude of this gradient can indicate which input cells most affect the decision. - We will use the **Captum library** or implement manually to get saliency maps for a set of states. For each such state (board configuration) and the action the agent took, produce a heatmap highlighting important cells. We expect that: - The area around the target and the robot being moved might light up. - Walls along the path might be highlighted if the agent is considering them (e.g., if it plans to bounce off a wall). - If we find irrelevant areas highlighted, that could indicate some misgeneralization or odd heuristic. (For example, if always the top-right corner cell is highlighted regardless of goal, that echoes the misgeneralization example where the agent was biased to go top-right[\[2\]](https://ar5iv.labs.arxiv.org/html/2411.00867#:~:text=fundamental%20features%20like%20maze%20walls,for%20interactively%20exploring%20layer%20activations).) - We can also do **occlusion tests**: systematically mask or remove parts of the input and see how the policy output changes. For instance, remove a wall and see if the chosen action changes – if yes, the wall was important for that decision. This can confirm if the agent is relying on specific obstacles in its planning. - Saliency maps will be saved as images (overlay on the board perhaps) as part of the deliverables. We will cite that using saliency mapping and feature mapping is a proven approach to visualize learned strategies[\[2\]](https://ar5iv.labs.arxiv.org/html/2411.00867#:~:text=fundamental%20features%20like%20maze%20walls,for%20interactively%20exploring%20layer%20activations).

**5.4 Neural Feature Visualization:** Investigate what individual neurons or channels in the network represent: - For convolutional layers: We can visualize their filters or activation patterns: - Take a conv layer activation (e.g., first conv layer) for various inputs and visualize the feature maps. Often, early conv filters learn edge detectors or game-specific patterns. Tristan et al. found first-layer conv filters that clearly respond to maze walls or corridors[\[25\]](https://ar5iv.labs.arxiv.org/html/2411.00867#:~:text=To%20confirm%20our%20hypothesis%2C%20we,These%20are%20shown%20in%20Fig%C2%A03). We might find analogous things: e.g., a filter that activates in presence of a vertical wall, or one that highlights the target robot. - We can create images highlighting where a particular filter has high activation. For example, overlay the activation on the grid to see what areas cause it to fire. - We could also perform **activation maximization**: attempt to generate an input pattern that maximizes a specific neuron’s activation (using gradient ascent). This is easier for conv filters in vision models to see what texture or pattern they like. However, since our input is a discrete grid, this might just result in some combination of walls/robots that triggers the neuron. It could still be insightful (e.g., maybe a neuron maximizes when a robot is in line with a target). - For LSTM units: It’s harder to visualize directly, but we can attempt to correlate particular LSTM hidden units with human-understandable features: - For each hidden dimension, see how its activation varies with some known variables (like distance to goal, or whether the goal is reached). Possibly plot a histogram or correlation. - Identify any “monosemantic” neurons – ones that clearly correspond to a single interpretable concept. If we find, say, one neuron’s activation is basically the negative of the move count remaining or something, that would be a great insight. - We could use techniques like **principal component analysis (PCA)** or **t-SNE** on the hidden state vectors to see if they cluster by certain scenario features (e.g., all states that are close to solving look similar in hidden space). - The Sokoban interpretability work also tried training **sparse autoencoders** on the last-layer hidden state to find a more interpretable basis[\[27\]](https://arxiv.org/html/2407.15421v3#:~:text=%5E%7B8%7D%5E%7B8%7D8https%3A%2F%2Fhuggingface.co%2FAlignmentResearch%2Flearned). We might consider a simpler version: apply a _dictionary learning_ or PCA to find principal directions in the hidden space and see if those directions correspond to something (similar to what has been done for interpreting language models with PCA or ICA). This is an exploratory step. - Document any notable neurons or features. For instance, “Neuron 12 in layer 2 seems to activate whenever the red robot is in the same row as the target” – we would support that claim with evidence (visualizations or correlations).

**5.5 Causal Interventions – _Activation Patching Experiments_:** To move from correlation to causation, we perform **intervention experiments** on the network’s activations[\[3\]](https://ar5iv.labs.arxiv.org/html/2411.00867#:~:text=specific%20pathways%20and%20decision,the%20behavior%20of%20RL%20models): - The idea is to verify that certain internal representations _cause_ certain behaviors. For example, if we believe the agent’s plan is encoded in some part of the hidden state, then _overwriting that part_ should change the agent’s actions in predictable ways[\[21\]](https://arxiv.org/html/2407.15421v3#:~:text=,DRC%20causally%20represents%20its%20plan). - Implementation: - Take two different scenarios (A and B) at a specific time step. For instance, scenario A has a certain board where the agent is about to move towards the goal, and B is another board where maybe the agent would do a different move. - Swap or replace the internal state of the agent from A into B (while keeping B’s actual observation). Then feed it through the policy head and see what action is chosen. - If by injecting A’s hidden state into B, the agent now chooses A’s move in B’s context, that indicates those hidden features from A carried the “plan” or intent, overriding B’s natural behavior. This is a strong sign of a causal representation[\[21\]](https://arxiv.org/html/2407.15421v3#:~:text=,DRC%20causally%20represents%20its%20plan). - We can do this at different layers or time steps to pinpoint where the plan is stored. Perhaps the last LSTM layer’s hidden state is a good candidate. - Another example: if a neuron is suspected to encode “go up bias”, we can manually set it to a high value and see if the agent then moves up more often in an environment where it originally wouldn’t. - We will use our ability to set internal states in the model (since we control the code, we can feed in altered hidden states). We may need to run the network in a somewhat manual way to do this (e.g., run one step in env A to get hidden state, then run one step in env B but override its hidden with A’s). - These experiments help confirm findings from probes. In literature, such interventions were key to demonstrate causal plan representations[\[21\]](https://arxiv.org/html/2407.15421v3#:~:text=,DRC%20causally%20represents%20its%20plan) and to show one can “edit” a network’s strategy by manipulating neurons[\[3\]](https://ar5iv.labs.arxiv.org/html/2411.00867#:~:text=specific%20pathways%20and%20decision,the%20behavior%20of%20RL%20models). - We will carefully log the outcomes: e.g., “By transplanting the hidden state from scenario A at time t into scenario B at time t, the agent in B now followed A’s plan to go to the top-right corner, suggesting the hidden state carries the plan for that move.”

**5.6 Interactive and Visual Tools (optional extension):** If time permits, consider building an interactive visualization (perhaps a Jupyter Notebook or small web app) to step through a puzzle with the agent and see live activation values or saliency: - For instance, a notebook where one can configure a board, run the agent step by step, and at each step print the saliency map or highlight which neurons are firing. This could use matplotlib to show the board and maybe color-code certain neuron intensities. - This is not strictly required, but it can be a great way to demonstrate the findings and allow further ad-hoc probing. (Tristan et al. mentioned developing novel tools for interactively exploring layers[\[28\]](https://ar5iv.labs.arxiv.org/html/2411.00867#:~:text=even%20in%20the%20absence%20of,for%20interactively%20exploring%20layer%20activations) – we could mimic that on a smaller scale for our project.) - Even without a full interactive UI, we will prepare **visualizations** as final outputs: e.g., figures showing an example board with the agent’s trajectory and maybe an overlay of important features (like a heatmap at decision time), or charts showing probe accuracies, etc.

**5.7 Document Findings:** As we conduct the above analyses, we will document results in a coherent narrative (likely in a report or as comments/markdown in notebooks). Key things to record: - What did we learn about how the agent makes decisions? For example, “The agent’s ConvLSTM has neurons that clearly track the target robot’s relative position to the goal, and it appears to ‘plan’ the sequence of moves since a linear probe can predict 4 out of 5 subsequent moves with ~90% accuracy from the hidden state.” - Any evidence of undesirable behaviors or biases: e.g., “We found the agent sometimes focuses on the wrong goal if a specific pattern of walls appears (saliency map shows attention to an irrelevant corner) – suggesting a learned bias or misgeneralization.” - Comparison to human or classical planning: e.g., does the agent internally emulate something like the A\* heuristic? If we find a correlation like a neuron that correlates with Manhattan distance, that could imply it learned a heuristic similar to the solver’s. - These insights will guide future improvements (for example, if we identify a misgeneralization, one could think of training adjustments to correct it).

By the end of Step 5, we will have a comprehensive interpretability analysis of the model. The code deliverables here include the scripts for probing, visualization, etc., and the data/plots produced.

## Step 6: Consolidate Code, Testing, and Documentation

Now we focus on wrapping up the project with organized code and instructions, ensuring everything is reproducible and maintainable for potential future extensions.

**6.1 Code Organization:** - Structure the repository with clear modules: - env/ricochet_env.py: contains the RicochetRobotsEnv class and possibly sub-functions (like a Board class, the solver function, etc.). This module should be self-contained (perhaps we also include some predefined board configurations or utility functions here). - models/policies.py: defines our policy networks. For example, RicochetPolicyConvLSTM, RicochetPolicyCNN, etc., each as PyTorch nn.Module classes. Also define any custom layers like ConvLSTMCell or ConvLSTMNetwork here if needed. We will keep the implementations readable and well-commented, since this is a complex part. - train_agent.py: the training script to launch training. It will parse arguments (like which model architecture, which algorithm, number of timesteps, etc.), set up the environment and model, and run training. We’ll include usage examples in comments or documentation (e.g., “Run python train_agent.py --model=conv_lstm --timesteps=5000000 to train the ConvLSTM agent”). - evaluate_agent.py: a script to load a trained model and evaluate it on some random puzzles, printing stats and maybe saving a log. Possibly this can also take an argument to compare against solver. - solve_puzzle.py: (optional) a utility script that given a board (from a file or random seed) runs the solver and prints the optimal moves (for debugging or curiosity). - interpretability/ (folder for analysis scripts or notebooks): - probe_analysis.py or a Jupyter notebook ProbeAnalysis.ipynb: code for training probes on saved activations and printing the results. - saliency_analysis.py: generates saliency maps for a few test states and saves images. - visualize_features.py: script or notebook that plots feature maps or does PCA on hidden states, etc. - intervention_experiments.py: runs the activation patching tests, outputting what happens when we manipulate states. - We might also collate the results of these analyses into a single report notebook for clarity. - utils/ for any generic utilities (e.g., a util to hook layers, or to load models, or plotting utilities). - tests/ directory for test files: - test_env.py: contains unit tests for environment dynamics. We will test things like: moving a robot in an empty board goes to boundary; moving a robot into another robot stops next to it; reaching goal triggers done, etc. - test_solver.py: tests the solver on a simple scenario. - test_model.py: maybe test that the forward pass of each policy yields correctly sized outputs (given a fake input). - test_training.py: could do a short training run (like 100 steps) just to ensure no runtime errors in the loop. - A README.md: describing how to install dependencies (e.g., stable-baselines3, PyTorch, etc.), how to run training, and how to reproduce interpretability analyses. We’ll mention the Python version and any library versions used.

- We will ensure that the code is **well-documented**. Important classes and functions will have docstrings explaining their purpose and usage. Complex logic (like ConvLSTM internals or the solver algorithm) will have inline comments.

**6.2 Flexibility and Extensibility:** - The code is designed to be modular, so future researchers (or automated agents) can adjust components: - New environment variations (e.g., different board sizes, different game rules) can be added by subclassing or modifying the environment. - New models: Since we separated model definitions, one can add a new architecture (say a transformer-based planner model) in the models module and use it with minimal changes to the training script. - Different RL algorithms: If we wanted to try DQN or a different PPO implementation, we can swap out the algorithm in training. For example, SB3’s API makes it easy to use A2C or DQN instead of PPO with similar code. Our environment is standard and should work with any off-the-shelf algorithm. - Curriculum strategy changes: because we likely implement curriculum via a callback or an external schedule, this can be tweaked independently (for instance, one could try a different threshold for increasing difficulty without touching the rest of the code). - Interpretability methods: We might not exhaust all possibilities. The framework we set (extracting activations, probing, etc.) can be extended. For example, someone might plug in a different interpretability tool (like SHAP values or TCAV) and reuse our collected data. By organizing interpretability code in a separate folder, we make it easier to add new analysis scripts.

**6.3 Deliverables and Reproducibility:** - We will produce **several artifacts**: - The _codebase_ as described. - **Trained models**: the final weights of the agent (and possibly intermediate checkpoints). We might provide these (perhaps via a link or just in the repository if not too large) so others can directly load and run analysis without retraining. Since the Sokoban project released their model publicly[\[29\]](https://arxiv.org/html/2407.15421v3#:~:text=Specifically%2C%20we%20train%20and%20open,Di%C2%A0Langosco%20et%C2%A0al.%2C%202022), we aim to do similarly. - **Solver data**: If our solver can generate solutions for many puzzles, we could include a small dataset of puzzles and their optimal solutions (the Sokoban paper did this on HuggingFace[\[30\]](https://arxiv.org/html/2407.15421v3#:~:text=)). This is optional but would be a nice reference to have for evaluation. - **Documentation**: The README, plus possibly a short report summarizing findings (which could be in a Markdown or PDF document). This report would compile the main interpretability findings and could include some of the plots we generated. - We will ensure that running the code from scratch (with the appropriate hardware and time) can reproduce the results. That means keeping track of random seeds for training (so we can get the same model if needed) and for any analysis that involves randomness (like splitting data or running probes).

**6.4 Testing and Verification:** - Before finalizing, run all tests in the tests/ suite to make sure nothing breaks with last-minute changes. - Perform a short end-to-end dry run: e.g., train for a very reduced number of steps and then run one interpretability script to see it flows (though results won’t make sense with an untrained model, it checks the pipeline). - Code style and clarity: use a linter/formatter on the code to keep it consistent.

**6.5 Timeline Estimate (for each stage):** - Environment + Solver implementation: ~3-5 days (including testing and bug fixes). - Model implementation and training setup: ~5 days to implement and debug (ConvLSTM might take time to get right). Initial training experiments (maybe 1-2 weeks to run and tweak, depending on how many iterations to get it right). - Full training run for final model: possibly several days of compute. We might run multiple in parallel if possible (e.g., CNN baseline vs ConvLSTM). - Evaluation and interpretability coding: ~1 week to implement all the analysis tools and run them on the model. - Documentation and final organization: ~2 days to clean up, write README and ensure everything is clear.

(This timeline is flexible; interpretability especially can expand if we dive deep into analysis. We’ll prioritize the most insightful analyses first.)

By following the above plan, we will create a solid codebase and a detailed understanding of how an RL agent tackles the _Ricochet Robots_ puzzle. We will have not only built an agent that _works_, but also gained insight into its _inner reasoning_, aligning with the spirit of the referenced research in mechanistic interpretability of RL agents[\[2\]](https://ar5iv.labs.arxiv.org/html/2411.00867#:~:text=fundamental%20features%20like%20maze%20walls,for%20interactively%20exploring%20layer%20activations)[\[21\]](https://arxiv.org/html/2407.15421v3#:~:text=,DRC%20causally%20represents%20its%20plan). This comprehensive approach ensures that we can trust and explain the agent’s decisions, and also sets the stage for future research (for instance, comparing planning strategies across different games or architectures).

[\[1\]](https://arxiv.org/html/2407.15421v3#:~:text=Guez%20et%C2%A0al.%20,encode%20the%20observation%20with%20filters) [\[5\]](https://arxiv.org/html/2407.15421v3#:~:text=Guez%20et%C2%A0al.%20,encode%20the%20observation%20with%20filters) [\[6\]](https://arxiv.org/html/2407.15421v3#:~:text=,steps) [\[7\]](https://arxiv.org/html/2407.15421v3#:~:text=With%20each%20internal%20iteration%2C%20the,See%20Section%204) [\[8\]](https://arxiv.org/html/2407.15421v3#:~:text=) [\[9\]](https://arxiv.org/html/2407.15421v3#:~:text=We%20train%20each%20network%20for,see%20Section%20A.1) [\[12\]](https://arxiv.org/html/2407.15421v3#:~:text=) [\[13\]](https://arxiv.org/html/2407.15421v3#:~:text=To%20avoid%20convolution%20edge%20effects,padded%2C%20maintaining%20the%20output%20size) [\[14\]](https://arxiv.org/html/2407.15421v3#:~:text=) [\[15\]](https://arxiv.org/html/2407.15421v3#:~:text=the%20RNN%20has%20a%20causal,distribution%20Sokoban%20puzzles) [\[16\]](https://arxiv.org/html/2407.15421v3#:~:text=With%20each%20internal%20iteration%2C%20the,See%20Section%204) [\[17\]](https://arxiv.org/html/2407.15421v3#:~:text=This%20is%20a%20convolutional%20residual,The%20ResNet%20has%20M%20parameters) [\[18\]](https://arxiv.org/html/2407.15421v3#:~:text=This%20is%20a%20convolutional%20residual,The%20ResNet%20has%20M%20parameters) [\[19\]](https://arxiv.org/html/2407.15421v3#:~:text=transitions%20on%20256%20actors%20using,to%20at%20the%20end%20of) [\[20\]](https://arxiv.org/html/2407.15421v3#:~:text=In%20this%20work%2C%20we%20focus,for%20another%20DRC%20network%20and) [\[21\]](https://arxiv.org/html/2407.15421v3#:~:text=,DRC%20causally%20represents%20its%20plan) [\[22\]](https://arxiv.org/html/2407.15421v3#:~:text=Guez%20et%C2%A0al.%20,between%20plans%20and%20search%20algorithms) [\[23\]](https://arxiv.org/html/2407.15421v3#:~:text=prior%20work%2C%20we%20analyze%20a,distribution%20Sokoban%20puzzles) [\[24\]](https://arxiv.org/html/2407.15421v3#:~:text=,net%2Fforum%3Fid%3DPPjSKy40XUB) [\[26\]](https://arxiv.org/html/2407.15421v3#:~:text=50%20steps%20in%20advance,Leveraging%20these%20insights%2C%20we%20extend) [\[27\]](https://arxiv.org/html/2407.15421v3#:~:text=%5E%7B8%7D%5E%7B8%7D8https%3A%2F%2Fhuggingface.co%2FAlignmentResearch%2Flearned) [\[29\]](https://arxiv.org/html/2407.15421v3#:~:text=Specifically%2C%20we%20train%20and%20open,Di%C2%A0Langosco%20et%C2%A0al.%2C%202022) [\[30\]](https://arxiv.org/html/2407.15421v3#:~:text=) Planning in a recurrent neural network that plays Sokoban

<https://arxiv.org/html/2407.15421v3>

[\[2\]](https://ar5iv.labs.arxiv.org/html/2411.00867#:~:text=fundamental%20features%20like%20maze%20walls,for%20interactively%20exploring%20layer%20activations) [\[3\]](https://ar5iv.labs.arxiv.org/html/2411.00867#:~:text=specific%20pathways%20and%20decision,the%20behavior%20of%20RL%20models) [\[25\]](https://ar5iv.labs.arxiv.org/html/2411.00867#:~:text=To%20confirm%20our%20hypothesis%2C%20we,These%20are%20shown%20in%20Fig%C2%A03) [\[28\]](https://ar5iv.labs.arxiv.org/html/2411.00867#:~:text=even%20in%20the%20absence%20of,for%20interactively%20exploring%20layer%20activations) \[2411.00867\] Mechanistic Interpretability of Reinforcement Learning Agents

<https://ar5iv.labs.arxiv.org/html/2411.00867>

[\[4\]](http://proceedings.mlr.press/v97/guez19a/guez19a.pdf#:~:text=a%20distributed%20framework%20and%20the,2) [\[10\]](http://proceedings.mlr.press/v97/guez19a/guez19a.pdf#:~:text=action%20distribution%20%CF%80t%20and%20a,that%20these%20are%20not%20crucial) [\[11\]](http://proceedings.mlr.press/v97/guez19a/guez19a.pdf#:~:text=full%20network,ADDITIONAL%20DETAILS) proceedings.mlr.press

<http://proceedings.mlr.press/v97/guez19a/guez19a.pdf>
```

I updated the plan directly to incorporate the agreed Step 2 refinements.

- I standardized recurrent training on sb3-contrib RecurrentPPO and removed the suggestion to hand-roll recurrent rollout/optimization in Step 2.
- I mandated bank-based curriculum for training-time sampling (no solver calls during sampling), with any solver-gated “online” approach marked experimental and out-of-scope for Step 2.
- I added a requirement for fixed-size RGB observations across curriculum levels (resize/pad as needed) to ensure vec-env compatibility.
- I clarified hyperparams language to explicitly reference RecurrentPPO for recurrent cases and reiterated single-GPU considerations aligned with the above.

Key edits in `ResearchPlan.md`:
- In 2.2, replaced manual recurrent loop guidance with: use RecurrentPPO and make the DRC policy compatible with its API.
- In 2.3, clarified recurrent settings are for RecurrentPPO and kept other defaults.
- In “Curriculum Learning Integration”, specified offline bank-based curriculum, no solver during training, fixed-size `rgb_image` observations, and optional staged no-op experiments with logging.
- In 2.4, strengthened single-GPU guidance: avoid solver during sampling, prefer bank curriculum, enforce fixed observation size.

- The research plan for Step 2 now commits to RecurrentPPO for recurrent policies, bank-only curriculum during training, and fixed-size RGB observations, reducing risk and aligning the plan with practical single-GPU constraints.

---

**User**

My computer memory is getting filled up when I pass 128x128 rgb images into this; I think it's this block. Is it possible to make this smaller? I'd like you to make attributes (hardcoded for now) for the different numbers of channels and have them used in the various blocks so that I can play around with different numbers, and write notes about the conditions they must satisfy (all in this block of code). I know that we have to be careful/there are rules which must be satisfied. I don't know if all the '32's here have to be the same, or if they can be changed independently, which is why I want the attribute level variables so that I can see which ones must be the same

---

**Cursor**

I'll open `models/resnet.py` to see the full context of `ResNetBackbone` and update it to use configurable channel attributes with inline notes on constraints.

```python
from __future__ import annotations

import torch
import torch.nn as nn
from gymnasium.spaces import Box
from stable_baselines3.common.torch_layers import BaseFeaturesExtractor


class BasicBlock2Branch(nn.Module):
    def __init__(self, channels: int, kernel_size: int = 3) -> None:
        super().__init__()
        p = kernel_size // 2
        self.trunk = nn.Conv2d(channels, channels, kernel_size, padding=p, bias=True)
        self.branch_a = nn.Conv2d(channels, channels, kernel_size, padding=p, bias=True)
        self.branch_b = nn.Conv2d(channels, channels, kernel_size, padding=p, bias=True)
        # Avoid in-place to prevent versioning issues when reusing trunk output
        self.act = nn.ReLU(inplace=False)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        t = self.trunk(x)
        a = self.branch_a(self.act(t))
        b = self.branch_b(self.act(t))
        return t + a + b


class ResNetBackbone(nn.Module):
    def __init__(self, in_channels: int, kernel_size: int = 3) -> None:
        super().__init__()
        p = kernel_size // 2

        # Channel configuration (hardcoded attributes for easy experimentation)
        # Notes/constraints:
        # - BasicBlock2Branch(C) requires in_channels == out_channels == C (residual add).
        # - The 1x1 conv is the only width-changing op between stages; its out_channels
        #   must match the channel count expected by the subsequent blocks.
        # - The final backbone out_channels determines the input size of the projection
        #   head in ResNetFeaturesExtractor: proj_in = out_channels * H * W.
        # - To reduce memory on 128x128 RGB inputs, lower these widths (e.g., 16→32
        #   instead of 32→64), or introduce downsampling (not done here to preserve H×W).
        # - Keep kernel_size odd so padding=k//2 preserves spatial size.
        self.stem_channels: int = 32   # width after stem conv (was 32)
        self.stage1_channels: int = 32 # width inside stage 1 residual blocks (was 32)
        self.stage2_channels: int = 64 # width inside stage 2 residual blocks (was 64)
        self.num_blocks_stage1: int = 2 # number of BasicBlock2Branch in stage 1 (was 2)
        self.num_blocks_stage2: int = 7 # number of BasicBlock2Branch in stage 2 (was 7)

        # Stem
        self.stem = nn.Conv2d(in_channels, self.stem_channels, kernel_size, padding=p, bias=True)

        # Stages
        blocks: list[nn.Module] = []
        # Stage 1: residual blocks at stage1_channels
        # Constraint: stem_channels must equal stage1_channels for residual add without a projection.
        # If they differ, insert a 1×1 projection from stem_channels→stage1_channels before blocks.
        if self.stem_channels != self.stage1_channels:
            blocks.append(nn.Conv2d(self.stem_channels, self.stage1_channels, kernel_size=1, bias=True))
        blocks += [BasicBlock2Branch(self.stage1_channels, kernel_size) for _ in range(self.num_blocks_stage1)]

        # Transition: channel jump via 1×1 conv (preserve H×W)
        if self.stage1_channels != self.stage2_channels:
            blocks += [nn.Conv2d(self.stage1_channels, self.stage2_channels, kernel_size=1, bias=True)]

        # Stage 2: residual blocks at stage2_channels
        blocks += [BasicBlock2Branch(self.stage2_channels, kernel_size) for _ in range(self.num_blocks_stage2)]

        self.blocks = nn.Sequential(*blocks)

        # Expose backbone output channels for downstream projection sizing
        self.out_channels: int = self.stage2_channels

        # Init weights: Kaiming for convs
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight, mode="fan_in", nonlinearity="relu")
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0.0)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        x = self.stem(x)
        return self.blocks(x)


class ResNetFeaturesExtractor(BaseFeaturesExtractor):
    """ResNet baseline feature extractor: 9 blocks, no pooling; outputs projection dim.

    Features dim is the projection dim (default 256) for SB3 policy heads.
    """

    def __init__(self, observation_space: Box, features_dim: int = 256, kernel_size: int = 3) -> None:
        super().__init__(observation_space, features_dim)
        assert len(observation_space.shape) == 3, "Expected image observation (C,H,W)"
        c_in, H, W = int(observation_space.shape[0]), int(observation_space.shape[1]), int(observation_space.shape[2])

        self.backbone = ResNetBackbone(c_in, kernel_size)
        # Projection sizing constraint: proj_in = backbone.out_channels * H * W
        # If you change backbone widths or add spatial downsampling, update this accordingly.
        self.proj = nn.Linear(self.backbone.out_channels * H * W, features_dim)
        # Avoid in-place here as well for safe autograd on MPS/CUDA/CPU
        self.act = nn.ReLU(inplace=False)
        self._features_dim = int(features_dim)

        # Init proj
        nn.init.kaiming_normal_(self.proj.weight, mode="fan_in", nonlinearity="relu")
        if self.proj.bias is not None:
            nn.init.constant_(self.proj.bias, 0.0)

    def forward(self, observations: torch.Tensor) -> torch.Tensor:
        x = self.backbone(observations)
        flat = x.flatten(1)
        return self.act(self.proj(flat))



```

- I added channel attributes in `ResNetBackbone` and threaded them through the blocks, plus notes on constraints and where widths must match or be projected.

What changed:
- `ResNetBackbone` now exposes:
  - `stem_channels`, `stage1_channels`, `stage2_channels`
  - `num_blocks_stage1`, `num_blocks_stage2`
  - `out_channels` (used downstream)
- Inserted 1x1 projections automatically if `stem_channels != stage1_channels` and for the stage1→stage2 transition.
- `ResNetFeaturesExtractor`’s projection now uses `self.backbone.out_channels * H * W`, so changing widths is safe without touching the extractor.

Constraints documented inline in that block:
- BasicBlock2Branch requires equal in/out channels.
- Only the 1x1 “transition” ops change width; subsequent blocks must match that width.
- The extractor’s projection input depends on final `out_channels` and spatial size.
- Use odd kernel sizes to preserve H×W; memory can be reduced by lowering channel widths (e.g., 16/32) or by adding downsampling (not done here).

To reduce memory immediately:
- Try setting `stem_channels=16`, `stage1_channels=16`, `stage2_channels=32`, and/or reduce `num_blocks_stage2`. This will shrink activations and the final projection matrix size.

