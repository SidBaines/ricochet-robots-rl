# Review feedback for research proposal step 1.1
_Exported on 11/09/2025 at 19:13:24 BST from Cursor (1.4.5)_

---

**User**

You are a language model, working as part of a team to implement a research proposal. Please take the time to read the ResearchPlan.md.

You have been tasked with reviewing the work of a colleague who has been tasked with implementing 'step 1.1' of the plan detailed in ResearchPlan.md. You are not to change any code, but instead to review the work and give a list of feedback, ordered roughly from highest importance to lowest. Please go ahead and get started. Be as critical as you like.

---

**Cursor**

I'll scan the research plan and the relevant implementation files to assess how well step 1.1 was met and prepare prioritized feedback.

I reviewed the plan’s Step 1.1 requirements against the current environment and core implementation. Here’s prioritized feedback.

- Bold items are the highest-impact gaps or risks first.

- References use file/function names like `env/ricochet_env.py::RicochetRobotsEnv.step`.

- I’m not changing any code; this is review-only.

- If you want me to propose edits later, I can.

- **Target robot not exposed in observation**
  - The observation doesn’t reveal which robot is the target. The agent sees per-robot one-hot channels but nothing distinguishes the target robot from others. This prevents optimal behavior learning for the actual objective.
  - Where: `ricochet_env._board_to_obs` builds channels: walls (0), goal (1), then one channel per robot; no target indicator.
  - Fix suggestion: add a “target robot mask” channel or encode target ID in the observation.

- **Wall representation diverges from Ricochet Robots rules**
  - Official walls are on cell edges; current implementation uses walls as blocked cells. This changes reachability and legal stopping mechanics (e.g., inner-corner behaviors and classic “bumpers” differ).
  - Where: `ricochet_core.Board.walls` is a cell-occupancy grid; movement blocks if next cell is occupied by a wall.
  - Risk: Learned policies may not transfer to canonical puzzles or literature comparisons.

- **No formal guarantee of solvable reset layout when `ensure_solvable=True` fails or times out**
  - The code falls back to unsolved boards after 20 attempts or if solver import fails, silently breaking the Step 1.1 “ensure solvable” guidance.
  - Where: `ricochet_env.reset` loop sets `_board` and breaks even if no solution found after attempts or on ImportError.
  - Suggest at least flag in `info`, or raise/soft-fail to avoid training on unsolvable boards when the flag is set.

- **No unit tests for multi-robot interactions (blocking and stopping)**
  - Current tests check sliding and a specific goal path, but don’t cover stopping next to another robot, robots as “bumpers,” or collisions across multiple robots.
  - Where: `tests/test_env.py` and `tests/test_solver.py`.
  - Recommend tests: “moving robot A stops at robot B,” “robot B movement unaffected by A after move,” “chain pushes are impossible (correct).”

- **Action decoding behavior for no-op is leaky and surprising**
  - `_decode_action` maps out-of-range to `(robot_id=0, direction=UP)` which could mask bugs. No-op is handled elsewhere; decoding should reject invalid actions.
  - Where: `ricochet_env._decode_action`.

- **Observation layout may be incompatible with common CNN policy defaults**
  - Channel-last `(H, W, C)` is fine, but many RL libs expect channel-first `(C, H, W)`. This can cause friction or silent transposes later.
  - Where: `ricochet_env.__init__` sets `spaces.Box` with `(H, W, C)`.

- **Reward shaping is okay but lacks explicit no-op cost differentiation**
  - Step penalty applies equally to no-op and movement, which is defensible, but Step 1.1 suggests possibly a distinct smaller penalty for no-op “thinking” to study pacing behavior.
  - Where: `ricochet_env.step` adds `step_penalty` regardless.

- **Lack of symbolic observation mode mentioned in plan**
  - Step 1.1 proposes supporting an “image” vs “symbolic” observation mode for debugging; only image mode is implemented.
  - Where: `ricochet_env` constructors and `_board_to_obs`.

- **Random generator seeding semantics**
  - Passing `seed` to `reset` replaces the RNG rather than re-seeding deterministically from a base seed with episode counter. It’s fine, but can lead to surprises (e.g., same seed reused across resets yields identical board each time, which is sometimes desirable, sometimes not).
  - Where: `ricochet_env.reset` sets `self._rng = np.random.default_rng(seed)`.

- **`observation_space` and `action_space` when `gymnasium` missing**
  - Dummy `spaces.Box`/`spaces.Discrete` placeholders are present; but `Discrete.n` is set to 0 in the dummy and not updated from env, which could mislead external tooling relying on `n`.
  - Where: `ricochet_env` fallback `_SpacesDummy`.

- **Solver state serialization excludes non-robot aspects by assumption**
  - `serialize` tracks only robot positions, assuming walls/goal/target are constant. This is valid within a single search but brittle if ever used across boards. Add a clarifying comment or guard.
  - Where: `env/solver.py::serialize`.

- **No explicit tests for `ensure_solvable=True` path**
  - There’s no test that the env actually returns solvable instances when requested.
  - Where: `tests`.

- **`apply_action` updates position even if no movement**
  - The docstring implies “unchanged,” but it still writes the same coordinates. Harmless but inconsistent with the docstring’s spirit.
  - Where: `ricochet_core.apply_action`.

- **Basic ASCII rendering is sufficient; but no image render**
  - For debugging CNNs, a quick matplotlib renderer often helps visually validate inputs vs channels.
  - Where: `ricochet_env.render`.

- **FixedLayout tests are purpose-built; consider broader fixtures**
  - With multiple robots and more walls, fixtures can validate corner cases (edges, corners, tight corridors).
  - Where: `tests/test_env.py`.

- **`max_steps` truncation test missing**
  - No test to assert truncated=True when step limit exceeded.
  - Where: `tests/test_env.py`.

- **Info dict lacks diagnostics**
  - Consider `info={"success":..., "steps":..., "target_robot":...}` to support logging and curriculum later.
  - Where: `ricochet_env.step`.

- **Performance considerations**
  - Recomputing `occupied` per slide is okay for small boards; if scaling up, caching per step might be needed. Not urgent for Step 1.1.

- **Demo CLI good, but single-robot only**
  - The CLI only drives robot 0; a quick mapping to switch robots would help manual QA of multi-robot behavior.

In brief: the biggest blockers for Step 1.1 are the missing target-robot signal in observations and the wall model deviating from canonical rules. The rest are quality-of-life, robustness, and testing gaps that are straightforward to address.

I found and reviewed the environment, core movement logic, solver, tests, and the demo CLI. Now I’ll stop here unless you want proposed edits.

- Implemented rules and state are solid for a basic version: sliding, stopping, discrete action encoding, rewards and terminations align with Step 1.1.

- Two high-priority issues: observation lacks target-robot identity; wall model differs from the canonical Ricochet Robots which may affect downstream validity.

- Test coverage is good on basics but thin on multi-robot and solvability guarantees.

---

**User**

Okay, the codebase has now been improved according to your feedback. Please repeat the review

---

**Cursor**

I'll re-scan the updated environment, core, solver, tests, and demo to reassess Step 1.1 compliance and produce a new prioritized review.

I re-read the environment, core, solver, tests, and demo to reassess Step 1.1. Here’s an updated, prioritized review.

- Bold items are the highest-impact gaps or risks first.
- References use `file::function` notation.

- **Wall model still diverges from canonical Ricochet Robots edge-walls**
  - The implementation continues to model walls as blocked cells, not edge walls between cells. This changes legal stopping behavior and puzzle structure relative to the canonical game and much of the literature.
  - Where: `env/ricochet_core.py::Board.walls`, `slide_until_blocked`.
  - If canonical fidelity matters (for comparability and transfer), consider refactoring to edge-walls (four boolean grids or per-cell bitmask) and adjust sliding logic to stop on edge collisions.

- **No tests for ensure_solvable=True behavior**
  - With the new strict behavior (raising on failure or missing solver), there should be tests validating:
    - A solvable random board is produced (or that it raises after the attempt limit).
    - The error path when solver import is unavailable.
  - Where: `env/ricochet_env.py::reset` now raises, but `tests/` lacks coverage.

- **Image-mode observation encodes target only via a positional mask**
  - This is workable, but it forces the policy to infer target identity by matching the mask to one of the per-robot channels. Consider adding a dedicated target-ID one-hot channel replicated spatially or a small constant feature plane for each robot ID (e.g., “this is the target” plane), which can simplify learning.
  - Where: `env/ricochet_env.py::_board_to_image_obs`.

- **`info["level_solvable"]` is always True even for `FixedLayout`**
  - For fixed boards, `level_solvable` is set True without verifying solvability, which can mislead downstream logging/evaluation.
  - Where: `env/ricochet_env.py::reset`. Consider omitting or setting based on an optional on-demand solver check.

- **Missing tests for new observation variants**
  - Add tests to verify:
    - `channels_first=True` produces `(C,H,W)` with correct channel ordering and values.
    - `obs_mode="symbolic"` layout conforms to documented vector structure and matches the board state.
    - `include_noop=True` applies `noop_penalty` and leaves state unchanged.
  - Where: `tests/test_env.py`.

- **Dummy gym space fallback improved but still fragile**
  - The placeholders now carry shape, dtype, and `n`, which is better. If any external tooling introspects additional attributes (e.g., `low`, `high`), ensure parity with Gym’s semantics.
  - Where: `env/ricochet_env.py` fallback `_SpacesDummy`.

- **Random generation still uses cell walls**
  - If you later adopt edge-walls, ensure the generator produces consistent, non-degenerate edge constraints and still integrates with the solver.
  - Where: `env/ricochet_env.py::_generate_random_board`.

- **Solver state serialization assumption is narrow**
  - `serialize` only tracks robot positions and assumes walls/goal remain constant in the search space. That’s fine within a single BFS call but should be clearly scoped to avoid reuse across different boards.
  - Where: `env/solver.py::serialize`. Add a clarifying comment or guard if you later expand solver usage.

- **CLI now supports multi-robot control; consider a quick view of target identity**
  - Displaying target robot ID in the CLI header helps manual QA.
  - Where: `demo_cli.py`.

Positives acknowledged:

- Target identity is now available (image mode via target mask; symbolic via one-hot). Good alignment with Step 1.1.
- Added `obs_mode` (“image”/“symbolic”) and `channels_first` switch; matches plan flexibility.
- Introduced `noop_penalty` and stricter `_decode_action` validation, removing previous leaky behavior.
- `ensure_solvable=True` is now strict and fails noisily instead of silently proceeding; good for training correctness.
- Tests added for multi-robot blocking and truncation; demo CLI supports switching robots.

If canonical rule fidelity isn’t required, the environment is now in a solid state for Step 1.1. If it is required, the edge-wall refactor becomes the main remaining blocker.

- I re-scanned all relevant files and validated the new features against Step 1.1. The environment now exposes target identity, supports both image and symbolic observations, and enforces solvable level generation when requested. The biggest remaining gap is the non-canonical wall model; add tests for ensure_solvable, observation variants, and no-op behavior to round out coverage.

---

**User**

Okay, now more updates have been added - please compare to spec for step 1.1 again :)

---

**Cursor**

I'll fetch the updated env, core, solver, tests, and demo in parallel to reassess compliance with Step 1.1 and then provide prioritized feedback.

I reloaded the updated env, core, solver, tests, and demo and compared against Step 1.1. Here’s the revised, prioritized review.

- **Canonical edge-wall dynamics implemented, but image observation under-specifies walls**
  - You now model edge walls correctly in `env/ricochet_core.py` and use them for sliding. However, the image observation collapses edges into a single cell-level “any wall around this cell” mask (channel 0). This loses directional information that is often crucial for learning. 
  - Suggestion: add explicit wall channels (e.g., four binary planes for up/down/left/right edge presence per cell, or two planes for h/v walls with alignment), or provide separate `h_wall` and `v_wall` encodings so CNNs can learn the canonical geometry directly.
  - Where: `env/ricochet_env.py::_board_to_image_obs`.

- **Missing tests for ensure_solvable=True and error paths**
  - With strict behavior added, tests should cover: solvable generation succeeds; exceeding attempt limit raises; missing solver raises when `ensure_solvable=True`.
  - Where: `env/ricochet_env.py::reset`; no corresponding tests in `tests/`.

- **No tests for observation variants and no-op semantics**
  - Add tests to verify:
    - `channels_first=True` produces correct shape and channel ordering.
    - `obs_mode="symbolic"` vector layout (goal, target one-hot, robots) matches board state.
    - No-op: state unchanged and `noop_penalty` applied (distinct from `step_penalty`).
  - Where: `tests/test_env.py`.

- **Terminated vs truncated can both become True on the same step**
  - Currently, if the goal is reached on the step that also hits `max_steps`, both flags may be True. Gymnasium typically treats them as distinct causes; many consumers expect mutual exclusivity.
  - Suggestion: set `truncated` only if not `terminated`.
  - Where: `env/ricochet_env.py::step`.

- **`level_solvable` semantics differ between fixed and random**
  - You now set `level_solvable=None` for fixed layouts and `True` for random with `ensure_solvable`. This is reasonable, but be explicit in docstrings so downstream consumers don’t assume True means “proven by solver” in all cases.
  - Where: `env/ricochet_env.py::reset`.

- **Solver serialization scope clarified; good—consider a small guard**
  - Comment now warns against reuse across boards. If you add solver utilities later, consider asserting board identity (e.g., id or hash) when reusing visited sets to avoid accidental cross-board contamination.
  - Where: `env/solver.py::serialize`.

- **Random edge-wall generator can produce tight boxes**
  - With random interior edges, some cells can become tiny alcoves. This is fine with `ensure_solvable=True` but can create degenerate episodes otherwise. You might want a simple density cap or connectivity heuristic if training without solver enforcement.
  - Where: `env/ricochet_env.py::_generate_random_board`.

- **CLI improved; consider printing target and step count each tick**
  - You already show target on reset. Keeping the header in sync (e.g., read from `info["target_robot"]` after each step) helps manual QA.
  - Where: `demo_cli.py`.

What’s solid now:
- Canonical edge-wall mechanics with correct sliding and stopping.
- Target identity available (image target mask; symbolic one-hot).
- Symbolic vs image modes; `channels_first` toggle.
- Strict `ensure_solvable` with clear errors; random generator builds bordered edge walls.
- Tests include multi-robot blocking and truncation; BFS solver updated to edge-walls; ASCII render reflects edges; CLI supports robot switching and shows target.

In short: Step 1.1 is essentially complete and faithful. The main remaining improvement is upgrading the image observation to carry directional wall information, plus adding tests for solvability/error paths, observation variants, and no-op behavior.

