# Review step 2 implementation feedback
_Exported on 16/09/2025 at 12:20:00 BST from Cursor (1.4.5)_

---

**User**

You are a research engineer, working as part of a team to implement a research proposal. Please take the time to read the ResearchPlan.md. Your colleagues have already implemented and reviewed steps 1.1 - 1.4 of the plan, which have been checked and are deemed completed, so you do not need to worry about this. You can read the ProgressNotes left by your colleagues, and read the code they have written, in order to understand what you are working with.

Now, we have implemented step 2, and we are comparing the code against the spec detailed in the research plan (and, more generally just making sure that it works). We want to make sure that it is as good as possible before moving on to step 3.
Your task is to help conduct this review, and provide detailed feedback for your colleagues to implement any changes necessary before we can move on to step 3.


You are not to change any code, but instead to review the work and give a list of feedback, ordered roughly from highest importance to lowest. Be as critical as you can - it's really important that we get this step up to standard to set us up in a way which makes the rest of the research plan implementation as easy and smooth as possible. Therefore, you should be as harsh/detail-oriented as possible - it is very important that we get the repository in as good a state as possible before moving on to the next stage.

---

**Cursor**

I'll start by reading the research plan and step 2 progress notes to understand the spec and what was implemented, then scan the main training, env, and model files to compare against the spec.

I'll scan the core env, model, training, evaluation, and test files to compare the Step 2 implementation against the ResearchPlan spec, then produce a prioritized feedback list.

I'm going to scan the environment, solver, curriculum, and test files to verify Step 2 integration points and test coverage before drafting prioritized feedback.

- High: Custom ConvLSTM is not actually used for training. The CLI `--convlstm` path switches to sb3-contrib’s `CnnLstmPolicy` instead of your `ConvLSTMFeaturesExtractor`/DRC stack. This deviates from Step 2’s goal to implement and train the DRC-like ConvLSTM. Action: wire a proper recurrent SB3 policy that calls your `ConvLSTMFeaturesExtractor` and maintains hidden states across steps; add an end-to-end training test that exercises that path.

- High: No tests validate recurrent training or hidden-state correctness. There’s no unit/e2e coverage for:
  - ConvLSTM hidden state threading across rollouts/batches
  - Sequence truncation/masks and deterministic behavior of recurrent inference
  - Learning signal flowing through the ConvLSTM stack
  Action: add tests that (a) step a recurrent policy for multiple timesteps and assert hidden shapes and no state leakage across episodes; (b) run a short training with the ConvLSTM policy on v0/v1 and assert it learns.

- High: Optimality metrics not integrated end-to-end. The training callbacks expect `optimality_gap`/`optimal_length` in episode infos, but the env only sets `optimal_length` at reset (ensure_solvable) and never computes/attaches an episode-level `optimality_gap`. Action: on episode termination, compute agent path length and attach `optimal_length` (carry from reset) and `optimality_gap=(agent_steps - optimal_length)` to info so monitoring logs are meaningful.

- High: Curriculum defaults possibly inconsistent in difficulty progression. In `create_default_curriculum`, level 2 `max_optimal_length=6` then level 3 `max_optimal_length=5` (harder level allows shorter optimal paths), which undermines monotonic difficulty. Your validator allows this due to the 0.8 tolerance; still, it’s counterintuitive. Action: ensure `max_optimal_length` is non-decreasing or justify/rename the metric.

- High: Missing tests for curriculum gating. There’s no automated check that:
  - Online curriculum actually advances levels at the specified thresholds
  - Bank curriculum emits puzzles matching the specified criteria per level
  - Observation spaces remain stable across levels (especially `rgb_image`)
  Action: add deterministic curriculum unit tests using small windows and fixed seeds, asserting level transitions and shape stability.

- High: Evaluate script has correctness issues. In the non-contrib fallback loop, `success` and `lengths` are incremented twice (once inside the per-episode loop and again after the loop). This inflates metrics. Action: fix duplication and add a test for eval reporting.

- Medium: DRC stack fidelity gaps vs spec.
  - You downsample the encoder with `AvgPool2d(2)`, which discards spatial resolution before ConvLSTM. DRC typically maintains full-resolution feature maps for planning on grids. Consider making downsampling optional and test both.
  - Output gate uses `tanh` instead of standard sigmoid. If this is intentional, document the rationale; otherwise align with standard ConvLSTM gating for stability.
  - Pool-and-inject and boundary channel are present (good), but there are no ablations/tests demonstrating their benefit. Add toggles in CLI and tests.

- Medium: No performance profiling for heavy code paths by default. The env’s `rgb_image` mode uses PIL resizing per step, which is costly. Action: add a benchmark or profiling-based test; gate costly resizing behind a flag or amortize via cached render buffers.

- Medium: Observation normalization handling is inconsistent. For image mode you set `normalize_images=False` but for `rgb_image` you flip normalization on depending on the policy path. Ensure consistent preprocessing: either scale to float [0,1] in env and keep `normalize_images=False`, or always feed uint8 and let SB3 normalize.

- Medium: No tests for seeding/reproducibility. Step 2/3 rely on stable seeds. Action: add tests that with a fixed seed, env resets produce identical boards across runs for each mode (`fixed_layout`, random solvable, and bank curricula).

- Medium: No explicit tests validating core environment dynamics against the solver. Step 1.4 suggests verifying that applying solver actions in the env reaches the goal. Action: add a test that runs BFS on a random solvable board, applies the returned actions in `RicochetRobotsEnv`, and asserts success in exactly `optimal_length` steps.

- Medium: Action space/no-op usage not validated. The plan contemplates “thinking” via no-op. There’s no test showing no-op is correctly penalized and usable. Action: add tests that no-op produces `noop_penalty`, does not change state, and that agents can train with no-op allowed without degenerate endless no-ops (e.g., terminate if >K no-ops).

- Medium: Vectorization and `VecNormalize` coverage gaps. You support VecNormalize save/load but don’t test it. Action: add a test that trains briefly with VecNormalize, saves stats, reloads both model and vecnorm, and validates rollout runs without shape/type errors.

- Medium: Device management and autocast edge cases. MPS autocast can be flaky; there’s no guardrail test. Action: add a smoke test for forward passes on CPU and MPS (if available) with autocast toggled off/on for the ConvLSTM extractor.

- Low: CLI surface is broad but under-documented in README. Many research-critical flags (curriculum, bank curriculum, `rgb_image`, profiling, evaluators) are not explained. Action: expand README with examples for: online curriculum run, bank curriculum run, recurrent PPO run, fixed-set evaluation using solver cache, and interpretability-prep checkpoints.

- Low: Monitoring hub is optional but errors are swallowed silently in multiple places. Action: log a single clear line at startup about which backends are active; add a unit test that verifies no exceptions are thrown when neither TB nor wandb are present.

- Low: `ricochet_env` asserts square boards; the research plan leaves room for varying sizes. If square-only is a design decision, document it in README and error messages.

- Low: Missing tests for channel naming consistency. You expose `channel_names` only in image mode and not `rgb_image`. If analyses will rely on names, add a consistent mapping or a test showing they are present when needed.

- Low: Solver API surface is good, but there’s no time/expansion-budget test. Add tests that `max_nodes`/`max_depth` cut off as expected and that `heuristic` modes behave (admissible vs non-admissible).

- Low: Minor UX issues
  - `train_agent.py` warns “Use --convlstm” while actually switching to `CnnLstmPolicy`—reword the message to reflect reality until custom ConvLSTM is wired.
  - `evaluate_agent.py` ignores `policy_type` determination; either use it or remove to avoid confusion.

- Low: Lint/type hygiene. Some broad excepts hide real errors (e.g., imports in monitoring blocks). Consider narrowing catches and logging exceptions to ease debugging.

- Nice-to-have: Add small golden tests for `rgb_image` frame shape and simple visual properties (e.g., borders drawn, non-empty frames) to prevent regressions in rendering.

- Nice-to-have: Provide a tiny fixed “evaluation set” JSON with 5–10 puzzles and include example usage for the fixed-set evaluator + solver cache in README.

- Nice-to-have: Add checksum/versioning to saved checkpoints and config export to ensure full reproducibility of Step 3 analyses.

Summary of most critical changes
- Make ConvLSTM/DRC actually trainable via a proper recurrent policy integration; add tests that it learns on v0/v1.
- Emit and log optimality metrics at episode end; add tests for logging and correctness.
- Fix `evaluate_agent.py` metric double-counting.
- Add curriculum advancement tests and ensure monotonic difficulty configuration.
- Add solver→env execution equivalence tests and seeding determinism tests.

