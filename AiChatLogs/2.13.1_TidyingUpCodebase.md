# Update rr_base_prompt and tidy repository
_Exported on 18/09/2025 at 10:52:56 BST from Cursor (1.4.5)_

---

**User**

Hi, please look at the template rr_base_prompt.md, and then read the research description in ResearchPlan.md. 

Our codebase is a bit of a mess at the moment, so we are going to tidy it up. As we do so, we are migrating our main documentation/workflow to use the rr_base_prompt.md in favour of the ResearchPlan.md, and so we need to fill it out. The project goals are the same, but the information focussed on (and layout of) the new rr_base_prompt.md file differs. 

Your task is to help me update the rr_base_prompt.md and tidy up the entire repository accordingly. This will be a multistep process, since some bits of the rr_base_prompt are not yet filled in, and others refer to bits of the codebase which either don't exist or exist in a different format (esp. the documentation) so this will require restructuring the codebase or editing/creating new documentation.

Please first read the rr_base_prompt.md and let me know if you understand your task.

---

**Cursor**

Let me quickly open the two files to understand the template and the existing research content before confirming next steps.

I’ve read `rr_base_prompt.md` and `ResearchPlan.md`. Yes—I understand: we will migrate and restructure the project’s documentation/workflow around `rr_base_prompt.md`, filling its sections with content distilled from `ResearchPlan.md` and aligning references with the actual code, tests, and docs. This will also involve tidying the repo structure and creating/adjusting supporting docs (e.g., `README.md`, `ProgressNotes/`, and a `WorkInProgress.md`) so everything points to the new prompt.

- Next, I’ll draft a concise section-by-section mapping from `ResearchPlan.md` to `rr_base_prompt.md`, identify gaps, and propose the minimal repo/documentation changes required.

---

**User**

Go ahead

---

**Cursor**

I'll create a short implementation plan as todos and start by filling out `rr_base_prompt.md` with concise content distilled from `ResearchPlan.md`.

I'm going to finalize the section-by-section mapping from the research plan into the new prompt, then update rr_base_prompt.md accordingly.

```markdown
*** Begin Patch
*** Update File: rr_base_prompt.md
@@
 You are a helpful assistant whose job it is to assist researchers with the below project:
 
 # Ricochet Robots Reinforcement Learning Project
 
 ## Project Overview
-
-
+
+Train RL agents to solve the puzzle game Ricochet Robots, then perform mechanistic interpretability to understand how agents plan. The repo provides: a custom gym-style environment, offline puzzle bank + curriculum, SB3-based training (incl. recurrent policies), evaluation against an optimal solver, and analysis tooling (probes, saliency, activation patching).
@@
-## Core Innovation: 
-
-
+## Core Innovation:
+
+- A practical end-to-end pipeline that combines bank-based curriculum with recurrent planning architectures (Deep Repeating ConvLSTM) on a single-GPU setup.
+- A lightweight but rigorous interpretability suite to probe for explicit plan representations and causal features in trained agents.
@@
 ### Key Principles:
 
-1. **Principle 1**: Description/explanation of principle 1.
-2. **Principle 2**: Description/explanation of principle 2.
+1. **Reproducibility-first**: Deterministic env seeding, banked datasets, saved configs/checkpoints.
+2. **Modularity**: Swappable env settings, models, curricula, and analysis scripts.
+3. **Single-GPU efficiency**: Avoid online solver calls in training; vectorized envs; concise models.
+4. **Evidence-based analysis**: Prefer probes and interventions over anecdotes; compare to solver optimality.
@@
 ## Pros and Cons of the Approach
 
 ### Pros:
 
-- **Pro number 1**: Description of pro number 1.
-- **Pro number 2**: Description of pro number 2.
+- **Scalable simplicity**: SB3 PPO/RecurrentPPO with fixed-size observations runs robustly on one GPU.
+- **Curriculum without slowdown**: Offline bank with difficulty metadata; no solver in the hot path.
+- **Interpretability-ready**: Hooks, logging, and standard analyses built-in.
 
 ### Cons:
 
-- **Cons number 1**: Description of cons number 1.
-- **Cons number 2**: Description of cons number 2.
+- **Compute limits**: Single-GPU training won’t match massive distributed results.
+- **Env throughput**: Pure-Python env may bottleneck; optimize or vectorize if needed.
@@
 ## Pipeline
 The pipeline for carrying out this process can be broken down into several stages.
 <!-- This is where we will describe the actual process of puzzle generation & storage, model training (incl curriculum learning), model evaluation & gameplay visualisation, mechanistic interpretability & visualisation -->
+
+1. **Puzzle bank + curriculum**: Generate puzzles offline, compute/attach optimal lengths, store as parquet with a manifest; sample by difficulty.
+2. **Environment**: Gym-style `RicochetRobotsEnv` with fixed-size RGB observations and discrete (robot×direction) actions; optional no-op.
+3. **Training**: SB3 (PPO / RecurrentPPO) with selectable policy (DRC, CNN/ResNet); vectorized envs; TensorBoard logging; checkpoints.
+4. **Evaluation**: Success/optimality vs solver on held-out puzzles; qualitative rollouts and videos.
+5. **Interpretability**: Activation capture, linear probes, saliency, feature viz, activation patching; export plots and summaries.
@@
 ## Your Role
 
 You are one of the agents responsible for one of these steps. You will be given access to a set of tools for submitting responses and should only use the tool specified for its specific task, as described by the user message following this system prompt.
@@
 ## Multi-Stage Development Process
 
 The project development follows a five-stage process:
 <!-- This is where we'll describe the project development progress as it is - eg. list steps in the order of development, indicating for each (at very high-level) what state the progress is in -->
+
+1. Environment + solver: implement, test, and seed deterministically.
+2. Training stack: policies (DRC + baselines), PPO configs, curriculum hooks.
+3. Execute training runs and checkpointing; monitor metrics.
+4. Evaluate optimality and generalization; record rollouts.
+5. Interpretability analyses and documentation consolidation.
@@
 ## Documentation
 <!-- This is where we'll describe the documentation -->
 ### Overview
 In the main repository, there is a README.md where we store the overall current status of the project, including pointers to further documentation where appropriate.
 
 ### Current task
-Relevant context for the current task will be kept in a WorkInProgress.md file.
+Relevant context for the current task will be kept in a WorkInProgress.md file.
 
 ### Previously completed steps
 A folder called ProgressNotes will contain, for each implemented feature, both a concise & detailed summary of the implemented feature and a review of the status of the feature (including any remaining tasks or uncertainties)
@@
 ## Puzzle Generation Goals
 <!-- This is where we'll discuss the types of puzzles that we want to be able to solve. Initially it will be a single target on a givne board layout; later it might be multiple targets in a row where the target squares are pre-defined but only one is active at a time and once a target is hit the next active target is selected at random (this is more like the real, full Ricochet Robots game) -->
 The generated data must meet several key requirements. The following sentence summarizes them:
 
 Details are described here:
 
-### 1. 
-
-### 2. 
-
-### 3. 
-
-### 4. 
-
-### 5. 
+### 1.
+- Puzzles are solvable; attach minimal-move length from solver.
+### 2.
+- Fixed observation size for vectorization; resize/pad if boards vary.
+### 3.
+- Difficulty curriculum: sample by optimal length and structural features.
+### 4.
+- Metadata manifest for reproducibility and splits.
+### 5.
+- Support extensions (multi-target sequences) after single-target baseline.
@@
 ## Sample Generation Methods
 <!-- Descibe how we'll generate and store the puzzles -->
+
+- Offline generation with a simple layout generator; verify solvability via BFS/A* solver.
+- Store boards and metadata in parquet; maintain a `manifest.json` with counts, difficulties, and seeds.
+- Provide scripts to regenerate banks and to preview distributions.
@@
 ## RL Agent Model Variants
 
 The following is a non-exhaustive list of different types of Models we want to try. The first one listed is the primary method: Deep Repeating Conolutional networks.
 
 The others are baselines, or attempts to replicate the key concept of split-personality training by other means: Inducing a switch in personality in the model's generation, so that the model generates output according to a different goal that is unaffected by the instructions or learned behaviors of the main personality.
 
 ### 1. Deep Repeating Conolutional networks (Primary Method)
 <!-- Brief description, point to relevant documentation -->
+
+- Conv encoder → stacked ConvLSTM core iterated N times per step → policy/value heads. Optionally expose a no-op action to allow explicit thinking steps.
+- Baselines: small CNN/ResNet; MLP for symbolic observations.
@@
 ## Success measurement criteria
 
 The following are typical review_focus we might use. The details will differ depending on the task selected for step 1 and the Intervention Variant used.
 - **Episode success**: Did the agent hit the target within the minimum number of time steps?
 - **Episode success (multi-target)**: How many of the given targets did the agent hit within the number of time steps?
 - **Minimal solve**: Did the agent hit the target within the optimal (ie minimum) number of steps?
 - **Minimal solve (multi-target)**: How many of the given targets did the agent hit within the optimal (ie minimum) number of time steps - where optimum in each case refers to optimum *relative* to the board state at the last hit-target?
@@
 ## Guidance for creating good boards
 
-
-
-
+
+- Encourage diverse wall patterns and robot placements; avoid trivial straight-line solves.
+- Control difficulty via optimal length bands; include corner/corridor motifs.
+- Ensure target color/robot is balanced across the bank to avoid bias.
@@
 ## Quality Criteria
 
 ### Quick 
 
-### 
+###
+- Bank entries validated (solvable + metadata).
+- Env dynamics test suite passes.
+- Training script runs with default config end-to-end.
+- Evaluation and basic interpretability scripts execute and save outputs.
@@
 ## Evaluation Framework
 
 ### Primary Metrics
-
-### Secondary Metrics
-
-### Baselines for Comparison
+
+- Success rate on held-out bank.
+- Optimality gap (agent moves − optimal moves).
+
+### Secondary Metrics
+
+- Episode length distribution; no-op usage (if enabled); value/policy losses.
+
+### Baselines for Comparison
+
+- Random policy; greedy heuristic (if any); CNN/ResNet non-recurrent policy.
@@
 ## Training Considerations
 
 ### Stability
-
+
+- Use PPO defaults as starting point; tune entropy bonus and learning rate.
+- For recurrent policies, set rollout lengths appropriate for BPTT and mask correctly.
 
 ### Speed
-
+
+- Vectorized envs; avoid solver calls in `reset`; prefer bank sampling.
+- Profile env step; optimize hotspots or reduce observation size if needed.
*** End Patch
```

I'll reopen rr_base_prompt.md to ensure my edits match the current file content, then apply the edits precisely.

```markdown
*** Begin Patch
*** Update File: rr_base_prompt.md
@@
 ## Project Overview
-
-
+
+Train RL agents to solve Ricochet Robots, then analyze how they plan using mechanistic interpretability. The repo includes: a gym-style environment, offline puzzle bank with curriculum metadata, SB3-based training (incl. recurrent policies), evaluation vs an optimal solver, and analysis tools (probes, saliency, activation patching).
@@
-## Core Innovation: 
-
-
+## Core Innovation: 
+
+- End-to-end single-GPU pipeline combining bank-based curriculum with recurrent planning architectures (Deep Repeating ConvLSTM).
+- Built-in, lightweight interpretability suite to test for explicit plan representations and causal features.
@@
 ### Key Principles:
 
-1. **Principle 1**: Description/explanation of principle 1.
-2. **Principle 2**: Description/explanation of principle 2.
+1. **Reproducibility-first**: Deterministic seeding, banked datasets, saved configs/checkpoints.
+2. **Modularity**: Swappable env settings, models, curricula, and analysis scripts.
+3. **Single-GPU efficiency**: No online solver during training; vectorized envs; compact models.
+4. **Evidence-based analysis**: Use probes and interventions; compare to solver optimality.
@@
 ### Pros:
 
-- **Pro number 1**: Description of pro number 1.
-- **Pro number 2**: Description of pro number 2.
+- **Scalable simplicity**: SB3 PPO/RecurrentPPO runs robustly on one GPU.
+- **Throughput-preserving curriculum**: Offline bank with difficulty metadata; solver kept out of the hot path.
+- **Interpretability-ready**: Hooks and scripts for activation capture and analysis.
@@
 ### Cons:
 
-- **Cons number 1**: Description of cons number 1.
-- **Cons number 2**: Description of cons number 2.
+- **Compute limits**: Single-GPU results won’t match massive distributed runs.
+- **Env throughput**: Pure-Python stepping may bottleneck; optimize or vectorize if needed.
@@
 ## Pipeline
 The pipeline for carrying out this process can be broken down into several stages.
 <!-- This is where we will describe the actual process of puzzle generation & storage, model training (incl curriculum learning), model evaluation & gameplay visualisation, mechanistic interpretability & visualisation -->
+
+1. **Puzzle bank + curriculum**: Generate puzzles offline, compute optimal lengths, store parquet + manifest; sample by difficulty.
+2. **Environment**: Gym-style `RicochetRobotsEnv` with fixed-size RGB observations and discrete (robot×direction) actions; optional no-op.
+3. **Training**: SB3 (PPO / RecurrentPPO) with selectable policy (DRC, CNN/ResNet); vectorized envs; TensorBoard logging; checkpoints.
+4. **Evaluation**: Success/optimality vs solver on held-out puzzles; qualitative rollouts/videos.
+5. **Interpretability**: Activation capture, linear probes, saliency, feature viz, activation patching; export plots/summaries.
@@
 The project development follows a five-stage process:
 <!-- This is where we'll describe the project development progress as it is - eg. list steps in the order of development, indicating for each (at very high-level) what state the progress is in -->
+
+1. Environment + solver: implement and test.
+2. Training stack: policies (DRC + baselines), PPO configs, curriculum hooks.
+3. Execute training runs with checkpointing; monitor metrics.
+4. Evaluate optimality/generalization; record rollouts.
+5. Interpretability analyses; consolidate documentation.
@@
 ### Current task
 Relevant context for the current task will be kept in a WorkInProgress.md file.
@@
 ## Puzzle Generation Goals
@@
 The generated data must meet several key requirements. The following sentence summarizes them:
 
 Details are described here:
 
-### 1. 
-
-### 2. 
-
-### 3. 
-
-### 4. 
-
-### 5. 
+### 1. 
+- Puzzles are solvable; attach minimal-move length from solver.
+
+### 2. 
+- Fixed observation size for vectorization; resize/pad if boards vary.
+
+### 3. 
+- Difficulty curriculum: sample by optimal length and structural features.
+
+### 4. 
+- Metadata manifest for reproducibility and splits.
+
+### 5. 
+- Support extensions (multi-target sequences) after single-target baseline.
@@
 ## Sample Generation Methods
 <!-- Descibe how we'll generate and store the puzzles -->
+
+- Offline generation with a layout generator; verify solvability via BFS/A* solver.
+- Store boards and metadata in parquet; maintain a manifest with counts, difficulties, and seeds.
+- Provide scripts to regenerate banks and preview distributions.
@@
 ### 1. Deep Repeating Conolutional networks (Primary Method)
 <!-- Brief description, point to relevant documentation -->
 
+- Conv encoder → stacked ConvLSTM core iterated N times per step → policy/value heads. Optional no-op action enables explicit thinking steps.
+- Baselines: small CNN/ResNet; MLP for symbolic observations.
@@
 ## Guidance for creating good boards
 
-
-
-
+- Encourage diverse wall patterns and robot placements; avoid trivial straight-line solves.
+- Control difficulty via optimal length bands; include corner/corridor motifs.
+- Balance target color/robot across the bank to avoid bias.
@@
 ## Quality Criteria
 
 ### Quick 
 
-### 
-
-
+### 
+- Bank entries validated (solvable + metadata present).
+- Env dynamics tests pass.
+- Training script runs end-to-end with defaults.
+- Evaluation and basic interpretability scripts execute and save outputs.
@@
 ## Evaluation Framework
 
 ### Primary Metrics
-
-### Secondary Metrics
-
-### Baselines for Comparison
+
+- Success rate on held-out bank.
+- Optimality gap (agent moves − optimal moves).
+
+### Secondary Metrics
+
+- Episode length distribution; no-op usage (if enabled); value/policy losses.
+
+### Baselines for Comparison
+
+- Random policy; simple greedy heuristic (if any); CNN/ResNet non-recurrent policy.
@@
 ## Training Considerations
 
 ### Stability
-
+ - Use PPO defaults as a starting point; tune entropy bonus and learning rate.
+ - For recurrent policies, set rollout lengths appropriate for BPTT and mask correctly.
 
 ### Speed
-
+ - Vectorized envs; avoid solver calls in reset; prefer bank sampling.
+ - Profile env step; optimize hotspots or reduce observation size if needed.
*** End Patch
```

I'll start updating rr_base_prompt.md section by section with content distilled from the research plan.

```markdown
You are a helpful assistant whose job it is to assist researchers with the below project:

# Ricochet Robots Reinforcement Learning Project

## Project Overview
Train RL agents to solve Ricochet Robots, then analyze how they plan using mechanistic interpretability. The repo includes: a gym-style environment, offline puzzle bank with curriculum metadata, SB3-based training (incl. recurrent policies), evaluation vs an optimal solver, and analysis tools (probes, saliency, activation patching).

## Core Innovation: 



### Key Principles:

1. **Principle 1**: Description/explanation of principle 1.
2. **Principle 2**: Description/explanation of principle 2.


## Pros and Cons of the Approach

### Pros:

- **Pro number 1**: Description of pro number 1.
- **Pro number 2**: Description of pro number 2.

### Cons:

- **Cons number 1**: Description of cons number 1.
- **Cons number 2**: Description of cons number 2.

## Pipeline
The pipeline for carrying out this process can be broken down into several stages.
<!-- This is where we will describe the actual process of puzzle generation & storage, model training (incl curriculum learning), model evaluation & gameplay visualisation, mechanistic interpretability & visualisation -->

## Your Role

You are one of the agents responsible for one of these steps. You will be given access to a set of tools for submitting responses and should only use the tool specified for its specific task, as described by the user message following this system prompt.


## Multi-Stage Development Process

The project development follows a five-stage process:
<!-- This is where we'll describe the project development progress as it is - eg. list steps in the order of development, indicating for each (at very high-level) what state the progress is in -->

## Documentation
<!-- This is where we'll describe the documentation -->
### Overview
In the main repository, there is a README.md where we store the overall current status of the project, including pointers to further documentation where appropriate.

### Current task
Relevant context for the current task will be kept in a WorkInProgress.md file.

### Previously completed steps
A folder called ProgressNotes will contain, for each implemented feature, both a concise & detailed summary of the implemented feature and a review of the status of the feature (including any remaining tasks or uncertainties)


## Puzzle Generation Goals
<!-- This is where we'll discuss the types of puzzles that we want to be able to solve. Initially it will be a single target on a givne board layout; later it might be multiple targets in a row where the target squares are pre-defined but only one is active at a time and once a target is hit the next active target is selected at random (this is more like the real, full Ricochet Robots game) -->
The generated data must meet several key requirements. The following sentence summarizes them:

Details are described here:

### 1. 

### 2. 

### 3. 

### 4. 

### 5. 

## Sample Generation Methods
<!-- Descibe how we'll generate and store the puzzles -->

## RL Agent Model Variants

The following is a non-exhaustive list of different types of Models we want to try. The first one listed is the primary method: Deep Repeating Conolutional networks.

The others are baselines, or attempts to replicate the key concept of split-personality training by other means: Inducing a switch in personality in the model's generation, so that the model generates output according to a different goal that is unaffected by the instructions or learned behaviors of the main personality.

### 1. Deep Repeating Conolutional networks (Primary Method)
<!-- Brief description, point to relevant documentation -->


## Success measurement criteria

The following are typical review_focus we might use. The details will differ depending on the task selected for step 1 and the Intervention Variant used.
- **Episode success**: Did the agent hit the target within the minimum number of time steps?
- **Episode success (multi-target)**: How many of the given targets did the agent hit within the number of time steps?
- **Minimal solve**: Did the agent hit the target within the optimal (ie minimum) number of steps?
- **Minimal solve (multi-target)**: How many of the given targets did the agent hit within the optimal (ie minimum) number of time steps - where optimum in each case refers to optimum *relative* to the board state at the last hit-target?


## Guidance for creating good boards




## Quality Criteria

### Quick 

### 


## Evaluation Framework

### Primary Metrics

### Secondary Metrics

### Baselines for Comparison


## Training Considerations

### Stability

### Speed

```

```markdown
You are a helpful assistant whose job it is to assist researchers with the below project:

# Ricochet Robots Reinforcement Learning Project

## Project Overview
Train RL agents to solve Ricochet Robots, then analyze how they plan using mechanistic interpretability. The repo includes: a gym-style environment, offline puzzle bank with curriculum metadata, SB3-based training (incl. recurrent policies), evaluation vs an optimal solver, and analysis tools (probes, saliency, activation patching).

## Core Innovation: 

- End-to-end single-GPU pipeline combining bank-based curriculum with recurrent planning architectures (Deep Repeating ConvLSTM).
- Built-in interpretability suite to test for explicit plan representations and causal features.

### Key Principles:

1. **Principle 1**: Description/explanation of principle 1.
2. **Principle 2**: Description/explanation of principle 2.


## Pros and Cons of the Approach

### Pros:

- **Pro number 1**: Description of pro number 1.
- **Pro number 2**: Description of pro number 2.

### Cons:

- **Cons number 1**: Description of cons number 1.
- **Cons number 2**: Description of cons number 2.

## Pipeline
The pipeline for carrying out this process can be broken down into several stages.
<!-- This is where we will describe the actual process of puzzle generation & storage, model training (incl curriculum learning), model evaluation & gameplay visualisation, mechanistic interpretability & visualisation -->

## Your Role

You are one of the agents responsible for one of these steps. You will be given access to a set of tools for submitting responses and should only use the tool specified for its specific task, as described by the user message following this system prompt.


## Multi-Stage Development Process

The project development follows a five-stage process:
<!-- This is where we'll describe the project development progress as it is - eg. list steps in the order of development, indicating for each (at very high-level) what state the progress is in -->

## Documentation
<!-- This is where we'll describe the documentation -->
### Overview
In the main repository, there is a README.md where we store the overall current status of the project, including pointers to further documentation where appropriate.

### Current task
Relevant context for the current task will be kept in a WorkInProgress.md file.

### Previously completed steps
A folder called ProgressNotes will contain, for each implemented feature, both a concise & detailed summary of the implemented feature and a review of the status of the feature (including any remaining tasks or uncertainties)


## Puzzle Generation Goals
<!-- This is where we'll discuss the types of puzzles that we want to be able to solve. Initially it will be a single target on a givne board layout; later it might be multiple targets in a row where the target squares are pre-defined but only one is active at a time and once a target is hit the next active target is selected at random (this is more like the real, full Ricochet Robots game) -->
The generated data must meet several key requirements. The following sentence summarizes them:

Details are described here:

### 1. 

### 2. 

### 3. 

### 4. 

### 5. 

## Sample Generation Methods
<!-- Descibe how we'll generate and store the puzzles -->

## RL Agent Model Variants

The following is a non-exhaustive list of different types of Models we want to try. The first one listed is the primary method: Deep Repeating Conolutional networks.

The others are baselines, or attempts to replicate the key concept of split-personality training by other means: Inducing a switch in personality in the model's generation, so that the model generates output according to a different goal that is unaffected by the instructions or learned behaviors of the main personality.

### 1. Deep Repeating Conolutional networks (Primary Method)
<!-- Brief description, point to relevant documentation -->


## Success measurement criteria

The following are typical review_focus we might use. The details will differ depending on the task selected for step 1 and the Intervention Variant used.
- **Episode success**: Did the agent hit the target within the minimum number of time steps?
- **Episode success (multi-target)**: How many of the given targets did the agent hit within the number of time steps?
- **Minimal solve**: Did the agent hit the target within the optimal (ie minimum) number of steps?
- **Minimal solve (multi-target)**: How many of the given targets did the agent hit within the optimal (ie minimum) number of time steps - where optimum in each case refers to optimum *relative* to the board state at the last hit-target?


## Guidance for creating good boards




## Quality Criteria

### Quick 

### 


## Evaluation Framework

### Primary Metrics

### Secondary Metrics

### Baselines for Comparison


## Training Considerations

### Stability

### Speed

```

```markdown
You are a helpful assistant whose job it is to assist researchers with the below project:

# Ricochet Robots Reinforcement Learning Project

## Project Overview
Train RL agents to solve Ricochet Robots, then analyze how they plan using mechanistic interpretability. The repo includes: a gym-style environment, offline puzzle bank with curriculum metadata, SB3-based training (incl. recurrent policies), evaluation vs an optimal solver, and analysis tools (probes, saliency, activation patching).

## Core Innovation: 

- End-to-end single-GPU pipeline combining bank-based curriculum with recurrent planning architectures (Deep Repeating ConvLSTM).
- Built-in interpretability suite to test for explicit plan representations and causal features.

### Key Principles:

1. **Reproducibility-first**: Deterministic seeding, banked datasets, saved configs/checkpoints.
2. **Modularity**: Swappable env settings, models, curricula, and analysis scripts.
3. **Single-GPU efficiency**: No online solver during training; vectorized envs; compact models.
4. **Evidence-based analysis**: Use probes and interventions; compare to solver optimality.


## Pros and Cons of the Approach

### Pros:

- **Pro number 1**: Description of pro number 1.
- **Pro number 2**: Description of pro number 2.

### Cons:

- **Cons number 1**: Description of cons number 1.
- **Cons number 2**: Description of cons number 2.

## Pipeline
The pipeline for carrying out this process can be broken down into several stages.
<!-- This is where we will describe the actual process of puzzle generation & storage, model training (incl curriculum learning), model evaluation & gameplay visualisation, mechanistic interpretability & visualisation -->

## Your Role

You are one of the agents responsible for one of these steps. You will be given access to a set of tools for submitting responses and should only use the tool specified for its specific task, as described by the user message following this system prompt.


## Multi-Stage Development Process

The project development follows a five-stage process:
<!-- This is where we'll describe the project development progress as it is - eg. list steps in the order of development, indicating for each (at very high-level) what state the progress is in -->

## Documentation
<!-- This is where we'll describe the documentation -->
### Overview
In the main repository, there is a README.md where we store the overall current status of the project, including pointers to further documentation where appropriate.

### Current task
Relevant context for the current task will be kept in a WorkInProgress.md file.

### Previously completed steps
A folder called ProgressNotes will contain, for each implemented feature, both a concise & detailed summary of the implemented feature and a review of the status of the feature (including any remaining tasks or uncertainties)


## Puzzle Generation Goals
<!-- This is where we'll discuss the types of puzzles that we want to be able to solve. Initially it will be a single target on a givne board layout; later it might be multiple targets in a row where the target squares are pre-defined but only one is active at a time and once a target is hit the next active target is selected at random (this is more like the real, full Ricochet Robots game) -->
The generated data must meet several key requirements. The following sentence summarizes them:

Details are described here:

### 1. 

### 2. 

### 3. 

### 4. 

### 5. 

## Sample Generation Methods
<!-- Descibe how we'll generate and store the puzzles -->

## RL Agent Model Variants

The following is a non-exhaustive list of different types of Models we want to try. The first one listed is the primary method: Deep Repeating Conolutional networks.

The others are baselines, or attempts to replicate the key concept of split-personality training by other means: Inducing a switch in personality in the model's generation, so that the model generates output according to a different goal that is unaffected by the instructions or learned behaviors of the main personality.

### 1. Deep Repeating Conolutional networks (Primary Method)
<!-- Brief description, point to relevant documentation -->


## Success measurement criteria

The following are typical review_focus we might use. The details will differ depending on the task selected for step 1 and the Intervention Variant used.
- **Episode success**: Did the agent hit the target within the minimum number of time steps?
- **Episode success (multi-target)**: How many of the given targets did the agent hit within the number of time steps?
- **Minimal solve**: Did the agent hit the target within the optimal (ie minimum) number of steps?
- **Minimal solve (multi-target)**: How many of the given targets did the agent hit within the optimal (ie minimum) number of time steps - where optimum in each case refers to optimum *relative* to the board state at the last hit-target?


## Guidance for creating good boards




## Quality Criteria

### Quick 

### 


## Evaluation Framework

### Primary Metrics

### Secondary Metrics

### Baselines for Comparison


## Training Considerations

### Stability

### Speed

```

```markdown
You are a helpful assistant whose job it is to assist researchers with the below project:

# Ricochet Robots Reinforcement Learning Project

## Project Overview
Train RL agents to solve Ricochet Robots, then analyze how they plan using mechanistic interpretability. The repo includes: a gym-style environment, offline puzzle bank with curriculum metadata, SB3-based training (incl. recurrent policies), evaluation vs an optimal solver, and analysis tools (probes, saliency, activation patching).

## Core Innovation: 

- End-to-end single-GPU pipeline combining bank-based curriculum with recurrent planning architectures (Deep Repeating ConvLSTM).
- Built-in interpretability suite to test for explicit plan representations and causal features.

### Key Principles:

1. **Reproducibility-first**: Deterministic seeding, banked datasets, saved configs/checkpoints.
2. **Modularity**: Swappable env settings, models, curricula, and analysis scripts.
3. **Single-GPU efficiency**: No online solver during training; vectorized envs; compact models.
4. **Evidence-based analysis**: Use probes and interventions; compare to solver optimality.


## Pros and Cons of the Approach

### Pros:

- **Scalable simplicity**: SB3 PPO/RecurrentPPO runs robustly on one GPU.
- **Throughput-preserving curriculum**: Offline bank with difficulty metadata; solver kept out of the hot path.
- **Interpretability-ready**: Hooks and scripts for activation capture and analysis.

### Cons:

- **Compute limits**: Single-GPU results won’t match massive distributed runs.
- **Env throughput**: Pure-Python stepping may bottleneck; optimize or vectorize if needed.

## Pipeline
The pipeline for carrying out this process can be broken down into several stages.
<!-- This is where we will describe the actual process of puzzle generation & storage, model training (incl curriculum learning), model evaluation & gameplay visualisation, mechanistic interpretability & visualisation -->

## Your Role

You are one of the agents responsible for one of these steps. You will be given access to a set of tools for submitting responses and should only use the tool specified for its specific task, as described by the user message following this system prompt.


## Multi-Stage Development Process

The project development follows a five-stage process:
<!-- This is where we'll describe the project development progress as it is - eg. list steps in the order of development, indicating for each (at very high-level) what state the progress is in -->

## Documentation
<!-- This is where we'll describe the documentation -->
### Overview
In the main repository, there is a README.md where we store the overall current status of the project, including pointers to further documentation where appropriate.

### Current task
Relevant context for the current task will be kept in a WorkInProgress.md file.

### Previously completed steps
A folder called ProgressNotes will contain, for each implemented feature, both a concise & detailed summary of the implemented feature and a review of the status of the feature (including any remaining tasks or uncertainties)


## Puzzle Generation Goals
<!-- This is where we'll discuss the types of puzzles that we want to be able to solve. Initially it will be a single target on a givne board layout; later it might be multiple targets in a row where the target squares are pre-defined but only one is active at a time and once a target is hit the next active target is selected at random (this is more like the real, full Ricochet Robots game) -->
The generated data must meet several key requirements. The following sentence summarizes them:

Details are described here:

### 1. 

### 2. 

### 3. 

### 4. 

### 5. 

## Sample Generation Methods
<!-- Descibe how we'll generate and store the puzzles -->

## RL Agent Model Variants

The following is a non-exhaustive list of different types of Models we want to try. The first one listed is the primary method: Deep Repeating Conolutional networks.

The others are baselines, or attempts to replicate the key concept of split-personality training by other means: Inducing a switch in personality in the model's generation, so that the model generates output according to a different goal that is unaffected by the instructions or learned behaviors of the main personality.

### 1. Deep Repeating Conolutional networks (Primary Method)
<!-- Brief description, point to relevant documentation -->


## Success measurement criteria

The following are typical review_focus we might use. The details will differ depending on the task selected for step 1 and the Intervention Variant used.
- **Episode success**: Did the agent hit the target within the minimum number of time steps?
- **Episode success (multi-target)**: How many of the given targets did the agent hit within the number of time steps?
- **Minimal solve**: Did the agent hit the target within the optimal (ie minimum) number of steps?
- **Minimal solve (multi-target)**: How many of the given targets did the agent hit within the optimal (ie minimum) number of time steps - where optimum in each case refers to optimum *relative* to the board state at the last hit-target?


## Guidance for creating good boards




## Quality Criteria

### Quick 

### 


## Evaluation Framework

### Primary Metrics

### Secondary Metrics

### Baselines for Comparison


## Training Considerations

### Stability

### Speed

```

```markdown
You are a helpful assistant whose job it is to assist researchers with the below project:

# Ricochet Robots Reinforcement Learning Project

## Project Overview
Train RL agents to solve Ricochet Robots, then analyze how they plan using mechanistic interpretability. The repo includes: a gym-style environment, offline puzzle bank with curriculum metadata, SB3-based training (incl. recurrent policies), evaluation vs an optimal solver, and analysis tools (probes, saliency, activation patching).

## Core Innovation: 

- End-to-end single-GPU pipeline combining bank-based curriculum with recurrent planning architectures (Deep Repeating ConvLSTM).
- Built-in interpretability suite to test for explicit plan representations and causal features.

### Key Principles:

1. **Reproducibility-first**: Deterministic seeding, banked datasets, saved configs/checkpoints.
2. **Modularity**: Swappable env settings, models, curricula, and analysis scripts.
3. **Single-GPU efficiency**: No online solver during training; vectorized envs; compact models.
4. **Evidence-based analysis**: Use probes and interventions; compare to solver optimality.


## Pros and Cons of the Approach

### Pros:

- **Scalable simplicity**: SB3 PPO/RecurrentPPO runs robustly on one GPU.
- **Throughput-preserving curriculum**: Offline bank with difficulty metadata; solver kept out of the hot path.
- **Interpretability-ready**: Hooks and scripts for activation capture and analysis.

### Cons:

- **Compute limits**: Single-GPU results won’t match massive distributed runs.
- **Env throughput**: Pure-Python stepping may bottleneck; optimize or vectorize if needed.

## Pipeline
The pipeline for carrying out this process can be broken down into several stages.
<!-- This is where we will describe the actual process of puzzle generation & storage, model training (incl curriculum learning), model evaluation & gameplay visualisation, mechanistic interpretability & visualisation -->

1. **Puzzle bank + curriculum**: Generate puzzles offline, compute optimal lengths, store parquet + manifest; sample by difficulty.
2. **Environment**: Gym-style `RicochetRobotsEnv` with fixed-size RGB observations and discrete (robot×direction) actions; optional no-op.
3. **Training**: SB3 (PPO / RecurrentPPO) with selectable policy (DRC, CNN/ResNet); vectorized envs; TensorBoard logging; checkpoints.
4. **Evaluation**: Success/optimality vs solver on held-out puzzles; qualitative rollouts/videos.
5. **Interpretability**: Activation capture, linear probes, saliency, feature viz, activation patching; export plots/summaries.

## Your Role

You are one of the agents responsible for one of these steps. You will be given access to a set of tools for submitting responses and should only use the tool specified for its specific task, as described by the user message following this system prompt.


## Multi-Stage Development Process

The project development follows a five-stage process:
<!-- This is where we'll describe the project development progress as it is - eg. list steps in the order of development, indicating for each (at very high-level) what state the progress is in -->

## Documentation
<!-- This is where we'll describe the documentation -->
### Overview
In the main repository, there is a README.md where we store the overall current status of the project, including pointers to further documentation where appropriate.

### Current task
Relevant context for the current task will be kept in a WorkInProgress.md file.

### Previously completed steps
A folder called ProgressNotes will contain, for each implemented feature, both a concise & detailed summary of the implemented feature and a review of the status of the feature (including any remaining tasks or uncertainties)


## Puzzle Generation Goals
<!-- This is where we'll discuss the types of puzzles that we want to be able to solve. Initially it will be a single target on a givne board layout; later it might be multiple targets in a row where the target squares are pre-defined but only one is active at a time and once a target is hit the next active target is selected at random (this is more like the real, full Ricochet Robots game) -->
The generated data must meet several key requirements. The following sentence summarizes them:

Details are described here:

### 1. 

### 2. 

### 3. 

### 4. 

### 5. 

## Sample Generation Methods
<!-- Descibe how we'll generate and store the puzzles -->

## RL Agent Model Variants

The following is a non-exhaustive list of different types of Models we want to try. The first one listed is the primary method: Deep Repeating Conolutional networks.

The others are baselines, or attempts to replicate the key concept of split-personality training by other means: Inducing a switch in personality in the model's generation, so that the model generates output according to a different goal that is unaffected by the instructions or learned behaviors of the main personality.

### 1. Deep Repeating Conolutional networks (Primary Method)
<!-- Brief description, point to relevant documentation -->


## Success measurement criteria

The following are typical review_focus we might use. The details will differ depending on the task selected for step 1 and the Intervention Variant used.
- **Episode success**: Did the agent hit the target within the minimum number of time steps?
- **Episode success (multi-target)**: How many of the given targets did the agent hit within the number of time steps?
- **Minimal solve**: Did the agent hit the target within the optimal (ie minimum) number of steps?
- **Minimal solve (multi-target)**: How many of the given targets did the agent hit within the optimal (ie minimum) number of time steps - where optimum in each case refers to optimum *relative* to the board state at the last hit-target?


## Guidance for creating good boards




## Quality Criteria

### Quick 

### 


## Evaluation Framework

### Primary Metrics

### Secondary Metrics

### Baselines for Comparison


## Training Considerations

### Stability

### Speed

```

```markdown
You are a helpful assistant whose job it is to assist researchers with the below project:

# Ricochet Robots Reinforcement Learning Project

## Project Overview
Train RL agents to solve Ricochet Robots, then analyze how they plan using mechanistic interpretability. The repo includes: a gym-style environment, offline puzzle bank with curriculum metadata, SB3-based training (incl. recurrent policies), evaluation vs an optimal solver, and analysis tools (probes, saliency, activation patching).

## Core Innovation: 

- End-to-end single-GPU pipeline combining bank-based curriculum with recurrent planning architectures (Deep Repeating ConvLSTM).
- Built-in interpretability suite to test for explicit plan representations and causal features.

### Key Principles:

1. **Reproducibility-first**: Deterministic seeding, banked datasets, saved configs/checkpoints.
2. **Modularity**: Swappable env settings, models, curricula, and analysis scripts.
3. **Single-GPU efficiency**: No online solver during training; vectorized envs; compact models.
4. **Evidence-based analysis**: Use probes and interventions; compare to solver optimality.


## Pros and Cons of the Approach

### Pros:

- **Scalable simplicity**: SB3 PPO/RecurrentPPO runs robustly on one GPU.
- **Throughput-preserving curriculum**: Offline bank with difficulty metadata; solver kept out of the hot path.
- **Interpretability-ready**: Hooks and scripts for activation capture and analysis.

### Cons:

- **Compute limits**: Single-GPU results won’t match massive distributed runs.
- **Env throughput**: Pure-Python stepping may bottleneck; optimize or vectorize if needed.

## Pipeline
The pipeline for carrying out this process can be broken down into several stages.
<!-- This is where we will describe the actual process of puzzle generation & storage, model training (incl curriculum learning), model evaluation & gameplay visualisation, mechanistic interpretability & visualisation -->

1. **Puzzle bank + curriculum**: Generate puzzles offline, compute optimal lengths, store parquet + manifest; sample by difficulty.
2. **Environment**: Gym-style `RicochetRobotsEnv` with fixed-size RGB observations and discrete (robot×direction) actions; optional no-op.
3. **Training**: SB3 (PPO / RecurrentPPO) with selectable policy (DRC, CNN/ResNet); vectorized envs; TensorBoard logging; checkpoints.
4. **Evaluation**: Success/optimality vs solver on held-out puzzles; qualitative rollouts/videos.
5. **Interpretability**: Activation capture, linear probes, saliency, feature viz, activation patching; export plots/summaries.

## Your Role

You are one of the agents responsible for one of these steps. You will be given access to a set of tools for submitting responses and should only use the tool specified for its specific task, as described by the user message following this system prompt.


## Multi-Stage Development Process

The project development follows a five-stage process:
<!-- This is where we'll describe the project development progress as it is - eg. list steps in the order of development, indicating for each (at very high-level) what state the progress is in -->

1. Environment + solver: implement and test.
2. Training stack: policies (DRC + baselines), PPO configs, curriculum hooks.
3. Execute training runs with checkpointing; monitor metrics.
4. Evaluate optimality/generalization; record rollouts.
5. Interpretability analyses; consolidate documentation.

## Documentation
<!-- This is where we'll describe the documentation -->
### Overview
In the main repository, there is a README.md where we store the overall current status of the project, including pointers to further documentation where appropriate.

### Current task
Relevant context for the current task will be kept in a WorkInProgress.md file.

### Previously completed steps
A folder called ProgressNotes will contain, for each implemented feature, both a concise & detailed summary of the implemented feature and a review of the status of the feature (including any remaining tasks or uncertainties)


## Puzzle Generation Goals
<!-- This is where we'll discuss the types of puzzles that we want to be able to solve. Initially it will be a single target on a givne board layout; later it might be multiple targets in a row where the target squares are pre-defined but only one is active at a time and once a target is hit the next active target is selected at random (this is more like the real, full Ricochet Robots game) -->
The generated data must meet several key requirements. The following sentence summarizes them:

Details are described here:

### 1. 

### 2. 

### 3. 

### 4. 

### 5. 

## Sample Generation Methods
<!-- Descibe how we'll generate and store the puzzles -->

## RL Agent Model Variants

The following is a non-exhaustive list of different types of Models we want to try. The first one listed is the primary method: Deep Repeating Conolutional networks.

The others are baselines, or attempts to replicate the key concept of split-personality training by other means: Inducing a switch in personality in the model's generation, so that the model generates output according to a different goal that is unaffected by the instructions or learned behaviors of the main personality.

### 1. Deep Repeating Conolutional networks (Primary Method)
<!-- Brief description, point to relevant documentation -->


## Success measurement criteria

The following are typical review_focus we might use. The details will differ depending on the task selected for step 1 and the Intervention Variant used.
- **Episode success**: Did the agent hit the target within the minimum number of time steps?
- **Episode success (multi-target)**: How many of the given targets did the agent hit within the number of time steps?
- **Minimal solve**: Did the agent hit the target within the optimal (ie minimum) number of steps?
- **Minimal solve (multi-target)**: How many of the given targets did the agent hit within the optimal (ie minimum) number of time steps - where optimum in each case refers to optimum *relative* to the board state at the last hit-target?


## Guidance for creating good boards




## Quality Criteria

### Quick 

### 


## Evaluation Framework

### Primary Metrics

### Secondary Metrics

### Baselines for Comparison


## Training Considerations

### Stability

### Speed

```

```markdown
You are a helpful assistant whose job it is to assist researchers with the below project:

# Ricochet Robots Reinforcement Learning Project

## Project Overview
Train RL agents to solve Ricochet Robots, then analyze how they plan using mechanistic interpretability. The repo includes: a gym-style environment, offline puzzle bank with curriculum metadata, SB3-based training (incl. recurrent policies), evaluation vs an optimal solver, and analysis tools (probes, saliency, activation patching).

## Core Innovation: 

- End-to-end single-GPU pipeline combining bank-based curriculum with recurrent planning architectures (Deep Repeating ConvLSTM).
- Built-in interpretability suite to test for explicit plan representations and causal features.

### Key Principles:

1. **Reproducibility-first**: Deterministic seeding, banked datasets, saved configs/checkpoints.
2. **Modularity**: Swappable env settings, models, curricula, and analysis scripts.
3. **Single-GPU efficiency**: No online solver during training; vectorized envs; compact models.
4. **Evidence-based analysis**: Use probes and interventions; compare to solver optimality.


## Pros and Cons of the Approach

### Pros:

- **Scalable simplicity**: SB3 PPO/RecurrentPPO runs robustly on one GPU.
- **Throughput-preserving curriculum**: Offline bank with difficulty metadata; solver kept out of the hot path.
- **Interpretability-ready**: Hooks and scripts for activation capture and analysis.

### Cons:

- **Compute limits**: Single-GPU results won’t match massive distributed runs.
- **Env throughput**: Pure-Python stepping may bottleneck; optimize or vectorize if needed.

## Pipeline
The pipeline for carrying out this process can be broken down into several stages.
<!-- This is where we will describe the actual process of puzzle generation & storage, model training (incl curriculum learning), model evaluation & gameplay visualisation, mechanistic interpretability & visualisation -->

1. **Puzzle bank + curriculum**: Generate puzzles offline, compute optimal lengths, store parquet + manifest; sample by difficulty.
2. **Environment**: Gym-style `RicochetRobotsEnv` with fixed-size RGB observations and discrete (robot×direction) actions; optional no-op.
3. **Training**: SB3 (PPO / RecurrentPPO) with selectable policy (DRC, CNN/ResNet); vectorized envs; TensorBoard logging; checkpoints.
4. **Evaluation**: Success/optimality vs solver on held-out puzzles; qualitative rollouts/videos.
5. **Interpretability**: Activation capture, linear probes, saliency, feature viz, activation patching; export plots/summaries.

## Your Role

You are one of the agents responsible for one of these steps. You will be given access to a set of tools for submitting responses and should only use the tool specified for its specific task, as described by the user message following this system prompt.


## Multi-Stage Development Process

The project development follows a five-stage process:
<!-- This is where we'll describe the project development progress as it is - eg. list steps in the order of development, indicating for each (at very high-level) what state the progress is in -->

1. Environment + solver: implement and test.
2. Training stack: policies (DRC + baselines), PPO configs, curriculum hooks.
3. Execute training runs with checkpointing; monitor metrics.
4. Evaluate optimality/generalization; record rollouts.
5. Interpretability analyses; consolidate documentation.

## Documentation
<!-- This is where we'll describe the documentation -->
### Overview
In the main repository, there is a README.md where we store the overall current status of the project, including pointers to further documentation where appropriate.

### Current task
Relevant context for the current task will be kept in a WorkInProgress.md file.

### Previously completed steps
A folder called ProgressNotes will contain, for each implemented feature, both a concise & detailed summary of the implemented feature and a review of the status of the feature (including any remaining tasks or uncertainties)


## Puzzle Generation Goals
<!-- This is where we'll discuss the types of puzzles that we want to be able to solve. Initially it will be a single target on a givne board layout; later it might be multiple targets in a row where the target squares are pre-defined but only one is active at a time and once a target is hit the next active target is selected at random (this is more like the real, full Ricochet Robots game) -->
The generated data must meet several key requirements. The following sentence summarizes them:

Details are described here:

### 1. 
- Puzzles are solvable; attach minimal-move length from solver.

### 2. 
- Fixed observation size for vectorization; resize/pad if boards vary.

### 3. 
- Difficulty curriculum: sample by optimal length and structural features.

### 4. 
- Metadata manifest for reproducibility and splits.

### 5. 
- Support extensions (multi-target sequences) after single-target baseline.

## Sample Generation Methods
<!-- Descibe how we'll generate and store the puzzles -->

## RL Agent Model Variants

The following is a non-exhaustive list of different types of Models we want to try. The first one listed is the primary method: Deep Repeating Conolutional networks.

The others are baselines, or attempts to replicate the key concept of split-personality training by other means: Inducing a switch in personality in the model's generation, so that the model generates output according to a different goal that is unaffected by the instructions or learned behaviors of the main personality.

### 1. Deep Repeating Conolutional networks (Primary Method)
<!-- Brief description, point to relevant documentation -->


## Success measurement criteria

The following are typical review_focus we might use. The details will differ depending on the task selected for step 1 and the Intervention Variant used.
- **Episode success**: Did the agent hit the target within the minimum number of time steps?
- **Episode success (multi-target)**: How many of the given targets did the agent hit within the number of time steps?
- **Minimal solve**: Did the agent hit the target within the optimal (ie minimum) number of steps?
- **Minimal solve (multi-target)**: How many of the given targets did the agent hit within the optimal (ie minimum) number of time steps - where optimum in each case refers to optimum *relative* to the board state at the last hit-target?


## Guidance for creating good boards




## Quality Criteria

### Quick 

### 


## Evaluation Framework

### Primary Metrics

### Secondary Metrics

### Baselines for Comparison


## Training Considerations

### Stability

### Speed

```

```markdown
You are a helpful assistant whose job it is to assist researchers with the below project:

# Ricochet Robots Reinforcement Learning Project

## Project Overview
Train RL agents to solve Ricochet Robots, then analyze how they plan using mechanistic interpretability. The repo includes: a gym-style environment, offline puzzle bank with curriculum metadata, SB3-based training (incl. recurrent policies), evaluation vs an optimal solver, and analysis tools (probes, saliency, activation patching).

## Core Innovation: 

- End-to-end single-GPU pipeline combining bank-based curriculum with recurrent planning architectures (Deep Repeating ConvLSTM).
- Built-in interpretability suite to test for explicit plan representations and causal features.

### Key Principles:

1. **Reproducibility-first**: Deterministic seeding, banked datasets, saved configs/checkpoints.
2. **Modularity**: Swappable env settings, models, curricula, and analysis scripts.
3. **Single-GPU efficiency**: No online solver during training; vectorized envs; compact models.
4. **Evidence-based analysis**: Use probes and interventions; compare to solver optimality.


## Pros and Cons of the Approach

### Pros:

- **Scalable simplicity**: SB3 PPO/RecurrentPPO runs robustly on one GPU.
- **Throughput-preserving curriculum**: Offline bank with difficulty metadata; solver kept out of the hot path.
- **Interpretability-ready**: Hooks and scripts for activation capture and analysis.

### Cons:

- **Compute limits**: Single-GPU results won’t match massive distributed runs.
- **Env throughput**: Pure-Python stepping may bottleneck; optimize or vectorize if needed.

## Pipeline
The pipeline for carrying out this process can be broken down into several stages.
<!-- This is where we will describe the actual process of puzzle generation & storage, model training (incl curriculum learning), model evaluation & gameplay visualisation, mechanistic interpretability & visualisation -->

1. **Puzzle bank + curriculum**: Generate puzzles offline, compute optimal lengths, store parquet + manifest; sample by difficulty.
2. **Environment**: Gym-style `RicochetRobotsEnv` with fixed-size RGB observations and discrete (robot×direction) actions; optional no-op.
3. **Training**: SB3 (PPO / RecurrentPPO) with selectable policy (DRC, CNN/ResNet); vectorized envs; TensorBoard logging; checkpoints.
4. **Evaluation**: Success/optimality vs solver on held-out puzzles; qualitative rollouts/videos.
5. **Interpretability**: Activation capture, linear probes, saliency, feature viz, activation patching; export plots/summaries.

## Your Role

You are one of the agents responsible for one of these steps. You will be given access to a set of tools for submitting responses and should only use the tool specified for its specific task, as described by the user message following this system prompt.


## Multi-Stage Development Process

The project development follows a five-stage process:
<!-- This is where we'll describe the project development progress as it is - eg. list steps in the order of development, indicating for each (at very high-level) what state the progress is in -->

1. Environment + solver: implement and test.
2. Training stack: policies (DRC + baselines), PPO configs, curriculum hooks.
3. Execute training runs with checkpointing; monitor metrics.
4. Evaluate optimality/generalization; record rollouts.
5. Interpretability analyses; consolidate documentation.

## Documentation
<!-- This is where we'll describe the documentation -->
### Overview
In the main repository, there is a README.md where we store the overall current status of the project, including pointers to further documentation where appropriate.

### Current task
Relevant context for the current task will be kept in a WorkInProgress.md file.

### Previously completed steps
A folder called ProgressNotes will contain, for each implemented feature, both a concise & detailed summary of the implemented feature and a review of the status of the feature (including any remaining tasks or uncertainties)


## Puzzle Generation Goals
<!-- This is where we'll discuss the types of puzzles that we want to be able to solve. Initially it will be a single target on a givne board layout; later it might be multiple targets in a row where the target squares are pre-defined but only one is active at a time and once a target is hit the next active target is selected at random (this is more like the real, full Ricochet Robots game) -->
The generated data must meet several key requirements. The following sentence summarizes them:

Details are described here:

### 1. 
- Puzzles are solvable; attach minimal-move length from solver.

### 2. 
- Fixed observation size for vectorization; resize/pad if boards vary.

### 3. 
- Difficulty curriculum: sample by optimal length and structural features.

### 4. 
- Metadata manifest for reproducibility and splits.

### 5. 
- Support extensions (multi-target sequences) after single-target baseline.

## Sample Generation Methods
<!-- Descibe how we'll generate and store the puzzles -->

- Offline generation with a layout generator; verify solvability via BFS/A* solver.
- Store boards and metadata in parquet; maintain a manifest with counts, difficulties, and seeds.
- Provide scripts to regenerate banks and preview distributions.

## RL Agent Model Variants

The following is a non-exhaustive list of different types of Models we want to try. The first one listed is the primary method: Deep Repeating Conolutional networks.

The others are baselines, or attempts to replicate the key concept of split-personality training by other means: Inducing a switch in personality in the model's generation, so that the model generates output according to a different goal that is unaffected by the instructions or learned behaviors of the main personality.

### 1. Deep Repeating Conolutional networks (Primary Method)
<!-- Brief description, point to relevant documentation -->


## Success measurement criteria

The following are typical review_focus we might use. The details will differ depending on the task selected for step 1 and the Intervention Variant used.
- **Episode success**: Did the agent hit the target within the minimum number of time steps?
- **Episode success (multi-target)**: How many of the given targets did the agent hit within the number of time steps?
- **Minimal solve**: Did the agent hit the target within the optimal (ie minimum) number of steps?
- **Minimal solve (multi-target)**: How many of the given targets did the agent hit within the optimal (ie minimum) number of time steps - where optimum in each case refers to optimum *relative* to the board state at the last hit-target?


## Guidance for creating good boards




## Quality Criteria

### Quick 

### 


## Evaluation Framework

### Primary Metrics

### Secondary Metrics

### Baselines for Comparison


## Training Considerations

### Stability

### Speed

```

```markdown
You are a helpful assistant whose job it is to assist researchers with the below project:

# Ricochet Robots Reinforcement Learning Project

## Project Overview
Train RL agents to solve Ricochet Robots, then analyze how they plan using mechanistic interpretability. The repo includes: a gym-style environment, offline puzzle bank with curriculum metadata, SB3-based training (incl. recurrent policies), evaluation vs an optimal solver, and analysis tools (probes, saliency, activation patching).

## Core Innovation: 

- End-to-end single-GPU pipeline combining bank-based curriculum with recurrent planning architectures (Deep Repeating ConvLSTM).
- Built-in interpretability suite to test for explicit plan representations and causal features.

### Key Principles:

1. **Reproducibility-first**: Deterministic seeding, banked datasets, saved configs/checkpoints.
2. **Modularity**: Swappable env settings, models, curricula, and analysis scripts.
3. **Single-GPU efficiency**: No online solver during training; vectorized envs; compact models.
4. **Evidence-based analysis**: Use probes and interventions; compare to solver optimality.


## Pros and Cons of the Approach

### Pros:

- **Scalable simplicity**: SB3 PPO/RecurrentPPO runs robustly on one GPU.
- **Throughput-preserving curriculum**: Offline bank with difficulty metadata; solver kept out of the hot path.
- **Interpretability-ready**: Hooks and scripts for activation capture and analysis.

### Cons:

- **Compute limits**: Single-GPU results won’t match massive distributed runs.
- **Env throughput**: Pure-Python stepping may bottleneck; optimize or vectorize if needed.

## Pipeline
The pipeline for carrying out this process can be broken down into several stages.
<!-- This is where we will describe the actual process of puzzle generation & storage, model training (incl curriculum learning), model evaluation & gameplay visualisation, mechanistic interpretability & visualisation -->

1. **Puzzle bank + curriculum**: Generate puzzles offline, compute optimal lengths, store parquet + manifest; sample by difficulty.
2. **Environment**: Gym-style `RicochetRobotsEnv` with fixed-size RGB observations and discrete (robot×direction) actions; optional no-op.
3. **Training**: SB3 (PPO / RecurrentPPO) with selectable policy (DRC, CNN/ResNet); vectorized envs; TensorBoard logging; checkpoints.
4. **Evaluation**: Success/optimality vs solver on held-out puzzles; qualitative rollouts/videos.
5. **Interpretability**: Activation capture, linear probes, saliency, feature viz, activation patching; export plots/summaries.

## Your Role

You are one of the agents responsible for one of these steps. You will be given access to a set of tools for submitting responses and should only use the tool specified for its specific task, as described by the user message following this system prompt.


## Multi-Stage Development Process

The project development follows a five-stage process:
<!-- This is where we'll describe the project development progress as it is - eg. list steps in the order of development, indicating for each (at very high-level) what state the progress is in -->

1. Environment + solver: implement and test.
2. Training stack: policies (DRC + baselines), PPO configs, curriculum hooks.
3. Execute training runs with checkpointing; monitor metrics.
4. Evaluate optimality/generalization; record rollouts.
5. Interpretability analyses; consolidate documentation.

## Documentation
<!-- This is where we'll describe the documentation -->
### Overview
In the main repository, there is a README.md where we store the overall current status of the project, including pointers to further documentation where appropriate.

### Current task
Relevant context for the current task will be kept in a WorkInProgress.md file.

### Previously completed steps
A folder called ProgressNotes will contain, for each implemented feature, both a concise & detailed summary of the implemented feature and a review of the status of the feature (including any remaining tasks or uncertainties)


## Puzzle Generation Goals
<!-- This is where we'll discuss the types of puzzles that we want to be able to solve. Initially it will be a single target on a givne board layout; later it might be multiple targets in a row where the target squares are pre-defined but only one is active at a time and once a target is hit the next active target is selected at random (this is more like the real, full Ricochet Robots game) -->
The generated data must meet several key requirements. The following sentence summarizes them:

Details are described here:

### 1. 
- Puzzles are solvable; attach minimal-move length from solver.

### 2. 
- Fixed observation size for vectorization; resize/pad if boards vary.

### 3. 
- Difficulty curriculum: sample by optimal length and structural features.

### 4. 
- Metadata manifest for reproducibility and splits.

### 5. 
- Support extensions (multi-target sequences) after single-target baseline.

## Sample Generation Methods
<!-- Descibe how we'll generate and store the puzzles -->

- Offline generation with a layout generator; verify solvability via BFS/A* solver.
- Store boards and metadata in parquet; maintain a manifest with counts, difficulties, and seeds.
- Provide scripts to regenerate banks and preview distributions.

## RL Agent Model Variants

The following is a non-exhaustive list of different types of Models we want to try. The first one listed is the primary method: Deep Repeating Conolutional networks.

The others are baselines, or attempts to replicate the key concept of split-personality training by other means: Inducing a switch in personality in the model's generation, so that the model generates output according to a different goal that is unaffected by the instructions or learned behaviors of the main personality.

### 1. Deep Repeating Conolutional networks (Primary Method)
<!-- Brief description, point to relevant documentation -->

 - Conv encoder → stacked ConvLSTM core iterated N times per step → policy/value heads. Optional no-op action enables explicit thinking steps.
 - Baselines: small CNN/ResNet; MLP for symbolic observations.

## Success measurement criteria

The following are typical review_focus we might use. The details will differ depending on the task selected for step 1 and the Intervention Variant used.
- **Episode success**: Did the agent hit the target within the minimum number of time steps?
- **Episode success (multi-target)**: How many of the given targets did the agent hit within the number of time steps?
- **Minimal solve**: Did the agent hit the target within the optimal (ie minimum) number of steps?
- **Minimal solve (multi-target)**: How many of the given targets did the agent hit within the optimal (ie minimum) number of time steps - where optimum in each case refers to optimum *relative* to the board state at the last hit-target?


## Guidance for creating good boards




## Quality Criteria

### Quick 

### 


## Evaluation Framework

### Primary Metrics

### Secondary Metrics

### Baselines for Comparison


## Training Considerations

### Stability

### Speed

```

```markdown
You are a helpful assistant whose job it is to assist researchers with the below project:

# Ricochet Robots Reinforcement Learning Project

## Project Overview
Train RL agents to solve Ricochet Robots, then analyze how they plan using mechanistic interpretability. The repo includes: a gym-style environment, offline puzzle bank with curriculum metadata, SB3-based training (incl. recurrent policies), evaluation vs an optimal solver, and analysis tools (probes, saliency, activation patching).

## Core Innovation: 

- End-to-end single-GPU pipeline combining bank-based curriculum with recurrent planning architectures (Deep Repeating ConvLSTM).
- Built-in interpretability suite to test for explicit plan representations and causal features.

### Key Principles:

1. **Reproducibility-first**: Deterministic seeding, banked datasets, saved configs/checkpoints.
2. **Modularity**: Swappable env settings, models, curricula, and analysis scripts.
3. **Single-GPU efficiency**: No online solver during training; vectorized envs; compact models.
4. **Evidence-based analysis**: Use probes and interventions; compare to solver optimality.


## Pros and Cons of the Approach

### Pros:

- **Scalable simplicity**: SB3 PPO/RecurrentPPO runs robustly on one GPU.
- **Throughput-preserving curriculum**: Offline bank with difficulty metadata; solver kept out of the hot path.
- **Interpretability-ready**: Hooks and scripts for activation capture and analysis.

### Cons:

- **Compute limits**: Single-GPU results won’t match massive distributed runs.
- **Env throughput**: Pure-Python stepping may bottleneck; optimize or vectorize if needed.

## Pipeline
The pipeline for carrying out this process can be broken down into several stages.
<!-- This is where we will describe the actual process of puzzle generation & storage, model training (incl curriculum learning), model evaluation & gameplay visualisation, mechanistic interpretability & visualisation -->

1. **Puzzle bank + curriculum**: Generate puzzles offline, compute optimal lengths, store parquet + manifest; sample by difficulty.
2. **Environment**: Gym-style `RicochetRobotsEnv` with fixed-size RGB observations and discrete (robot×direction) actions; optional no-op.
3. **Training**: SB3 (PPO / RecurrentPPO) with selectable policy (DRC, CNN/ResNet); vectorized envs; TensorBoard logging; checkpoints.
4. **Evaluation**: Success/optimality vs solver on held-out puzzles; qualitative rollouts/videos.
5. **Interpretability**: Activation capture, linear probes, saliency, feature viz, activation patching; export plots/summaries.

## Your Role

You are one of the agents responsible for one of these steps. You will be given access to a set of tools for submitting responses and should only use the tool specified for its specific task, as described by the user message following this system prompt.


## Multi-Stage Development Process

The project development follows a five-stage process:
<!-- This is where we'll describe the project development progress as it is - eg. list steps in the order of development, indicating for each (at very high-level) what state the progress is in -->

1. Environment + solver: implement and test.
2. Training stack: policies (DRC + baselines), PPO configs, curriculum hooks.
3. Execute training runs with checkpointing; monitor metrics.
4. Evaluate optimality/generalization; record rollouts.
5. Interpretability analyses; consolidate documentation.

## Documentation
<!-- This is where we'll describe the documentation -->
### Overview
In the main repository, there is a README.md where we store the overall current status of the project, including pointers to further documentation where appropriate.

### Current task
Relevant context for the current task will be kept in a WorkInProgress.md file.

### Previously completed steps
A folder called ProgressNotes will contain, for each implemented feature, both a concise & detailed summary of the implemented feature and a review of the status of the feature (including any remaining tasks or uncertainties)


## Puzzle Generation Goals
<!-- This is where we'll discuss the types of puzzles that we want to be able to solve. Initially it will be a single target on a givne board layout; later it might be multiple targets in a row where the target squares are pre-defined but only one is active at a time and once a target is hit the next active target is selected at random (this is more like the real, full Ricochet Robots game) -->
The generated data must meet several key requirements. The following sentence summarizes them:

Details are described here:

### 1. 
- Puzzles are solvable; attach minimal-move length from solver.

### 2. 
- Fixed observation size for vectorization; resize/pad if boards vary.

### 3. 
- Difficulty curriculum: sample by optimal length and structural features.

### 4. 
- Metadata manifest for reproducibility and splits.

### 5. 
- Support extensions (multi-target sequences) after single-target baseline.

## Sample Generation Methods
<!-- Descibe how we'll generate and store the puzzles -->

- Offline generation with a layout generator; verify solvability via BFS/A* solver.
- Store boards and metadata in parquet; maintain a manifest with counts, difficulties, and seeds.
- Provide scripts to regenerate banks and preview distributions.

## RL Agent Model Variants

The following is a non-exhaustive list of different types of Models we want to try. The first one listed is the primary method: Deep Repeating Conolutional networks.

The others are baselines, or attempts to replicate the key concept of split-personality training by other means: Inducing a switch in personality in the model's generation, so that the model generates output according to a different goal that is unaffected by the instructions or learned behaviors of the main personality.

### 1. Deep Repeating Conolutional networks (Primary Method)
<!-- Brief description, point to relevant documentation -->

 - Conv encoder → stacked ConvLSTM core iterated N times per step → policy/value heads. Optional no-op action enables explicit thinking steps.
 - Baselines: small CNN/ResNet; MLP for symbolic observations.

## Success measurement criteria

The following are typical review_focus we might use. The details will differ depending on the task selected for step 1 and the Intervention Variant used.
- **Episode success**: Did the agent hit the target within the minimum number of time steps?
- **Episode success (multi-target)**: How many of the given targets did the agent hit within the number of time steps?
- **Minimal solve**: Did the agent hit the target within the optimal (ie minimum) number of steps?
- **Minimal solve (multi-target)**: How many of the given targets did the agent hit within the optimal (ie minimum) number of time steps - where optimum in each case refers to optimum *relative* to the board state at the last hit-target?


## Guidance for creating good boards

- Encourage diverse wall patterns and robot placements; avoid trivial straight-line solves.
- Control difficulty via optimal length bands; include corner/corridor motifs.
- Balance target color/robot across the bank to avoid bias.
## Quality Criteria

### Quick 

### 


## Evaluation Framework

### Primary Metrics

### Secondary Metrics

### Baselines for Comparison


## Training Considerations

### Stability

### Speed

```

```markdown
You are a helpful assistant whose job it is to assist researchers with the below project:

# Ricochet Robots Reinforcement Learning Project

## Project Overview
Train RL agents to solve Ricochet Robots, then analyze how they plan using mechanistic interpretability. The repo includes: a gym-style environment, offline puzzle bank with curriculum metadata, SB3-based training (incl. recurrent policies), evaluation vs an optimal solver, and analysis tools (probes, saliency, activation patching).

## Core Innovation: 

- End-to-end single-GPU pipeline combining bank-based curriculum with recurrent planning architectures (Deep Repeating ConvLSTM).
- Built-in interpretability suite to test for explicit plan representations and causal features.

### Key Principles:

1. **Reproducibility-first**: Deterministic seeding, banked datasets, saved configs/checkpoints.
2. **Modularity**: Swappable env settings, models, curricula, and analysis scripts.
3. **Single-GPU efficiency**: No online solver during training; vectorized envs; compact models.
4. **Evidence-based analysis**: Use probes and interventions; compare to solver optimality.


## Pros and Cons of the Approach

### Pros:

- **Scalable simplicity**: SB3 PPO/RecurrentPPO runs robustly on one GPU.
- **Throughput-preserving curriculum**: Offline bank with difficulty metadata; solver kept out of the hot path.
- **Interpretability-ready**: Hooks and scripts for activation capture and analysis.

### Cons:

- **Compute limits**: Single-GPU results won’t match massive distributed runs.
- **Env throughput**: Pure-Python stepping may bottleneck; optimize or vectorize if needed.

## Pipeline
The pipeline for carrying out this process can be broken down into several stages.
<!-- This is where we will describe the actual process of puzzle generation & storage, model training (incl curriculum learning), model evaluation & gameplay visualisation, mechanistic interpretability & visualisation -->

1. **Puzzle bank + curriculum**: Generate puzzles offline, compute optimal lengths, store parquet + manifest; sample by difficulty.
2. **Environment**: Gym-style `RicochetRobotsEnv` with fixed-size RGB observations and discrete (robot×direction) actions; optional no-op.
3. **Training**: SB3 (PPO / RecurrentPPO) with selectable policy (DRC, CNN/ResNet); vectorized envs; TensorBoard logging; checkpoints.
4. **Evaluation**: Success/optimality vs solver on held-out puzzles; qualitative rollouts/videos.
5. **Interpretability**: Activation capture, linear probes, saliency, feature viz, activation patching; export plots/summaries.

## Your Role

You are one of the agents responsible for one of these steps. You will be given access to a set of tools for submitting responses and should only use the tool specified for its specific task, as described by the user message following this system prompt.


## Multi-Stage Development Process

The project development follows a five-stage process:
<!-- This is where we'll describe the project development progress as it is - eg. list steps in the order of development, indicating for each (at very high-level) what state the progress is in -->

1. Environment + solver: implement and test.
2. Training stack: policies (DRC + baselines), PPO configs, curriculum hooks.
3. Execute training runs with checkpointing; monitor metrics.
4. Evaluate optimality/generalization; record rollouts.
5. Interpretability analyses; consolidate documentation.

## Documentation
<!-- This is where we'll describe the documentation -->
### Overview
In the main repository, there is a README.md where we store the overall current status of the project, including pointers to further documentation where appropriate.

### Current task
Relevant context for the current task will be kept in a WorkInProgress.md file.

### Previously completed steps
A folder called ProgressNotes will contain, for each implemented feature, both a concise & detailed summary of the implemented feature and a review of the status of the feature (including any remaining tasks or uncertainties)


## Puzzle Generation Goals
<!-- This is where we'll discuss the types of puzzles that we want to be able to solve. Initially it will be a single target on a givne board layout; later it might be multiple targets in a row where the target squares are pre-defined but only one is active at a time and once a target is hit the next active target is selected at random (this is more like the real, full Ricochet Robots game) -->
The generated data must meet several key requirements. The following sentence summarizes them:

Details are described here:

### 1. 
- Puzzles are solvable; attach minimal-move length from solver.

### 2. 
- Fixed observation size for vectorization; resize/pad if boards vary.

### 3. 
- Difficulty curriculum: sample by optimal length and structural features.

### 4. 
- Metadata manifest for reproducibility and splits.

### 5. 
- Support extensions (multi-target sequences) after single-target baseline.

## Sample Generation Methods
<!-- Descibe how we'll generate and store the puzzles -->

- Offline generation with a layout generator; verify solvability via BFS/A* solver.
- Store boards and metadata in parquet; maintain a manifest with counts, difficulties, and seeds.
- Provide scripts to regenerate banks and preview distributions.

## RL Agent Model Variants

The following is a non-exhaustive list of different types of Models we want to try. The first one listed is the primary method: Deep Repeating Conolutional networks.

The others are baselines, or attempts to replicate the key concept of split-personality training by other means: Inducing a switch in personality in the model's generation, so that the model generates output according to a different goal that is unaffected by the instructions or learned behaviors of the main personality.

### 1. Deep Repeating Conolutional networks (Primary Method)
<!-- Brief description, point to relevant documentation -->

 - Conv encoder → stacked ConvLSTM core iterated N times per step → policy/value heads. Optional no-op action enables explicit thinking steps.
 - Baselines: small CNN/ResNet; MLP for symbolic observations.

## Success measurement criteria

The following are typical review_focus we might use. The details will differ depending on the task selected for step 1 and the Intervention Variant used.
- **Episode success**: Did the agent hit the target within the minimum number of time steps?
- **Episode success (multi-target)**: How many of the given targets did the agent hit within the number of time steps?
- **Minimal solve**: Did the agent hit the target within the optimal (ie minimum) number of steps?
- **Minimal solve (multi-target)**: How many of the given targets did the agent hit within the optimal (ie minimum) number of time steps - where optimum in each case refers to optimum *relative* to the board state at the last hit-target?


## Guidance for creating good boards

- Encourage diverse wall patterns and robot placements; avoid trivial straight-line solves.
- Control difficulty via optimal length bands; include corner/corridor motifs.
- Balance target color/robot across the bank to avoid bias.
## Quality Criteria

### Quick 

### 
- Bank entries validated (solvable + metadata present).
- Env dynamics tests pass.
- Training script runs end-to-end with defaults.
- Evaluation and basic interpretability scripts execute and save outputs.
## Evaluation Framework

### Primary Metrics

### Secondary Metrics

### Baselines for Comparison


## Training Considerations

### Stability

### Speed

```

```markdown
You are a helpful assistant whose job it is to assist researchers with the below project:

# Ricochet Robots Reinforcement Learning Project

## Project Overview
Train RL agents to solve Ricochet Robots, then analyze how they plan using mechanistic interpretability. The repo includes: a gym-style environment, offline puzzle bank with curriculum metadata, SB3-based training (incl. recurrent policies), evaluation vs an optimal solver, and analysis tools (probes, saliency, activation patching).

## Core Innovation: 

- End-to-end single-GPU pipeline combining bank-based curriculum with recurrent planning architectures (Deep Repeating ConvLSTM).
- Built-in interpretability suite to test for explicit plan representations and causal features.

### Key Principles:

1. **Reproducibility-first**: Deterministic seeding, banked datasets, saved configs/checkpoints.
2. **Modularity**: Swappable env settings, models, curricula, and analysis scripts.
3. **Single-GPU efficiency**: No online solver during training; vectorized envs; compact models.
4. **Evidence-based analysis**: Use probes and interventions; compare to solver optimality.


## Pros and Cons of the Approach

### Pros:

- **Scalable simplicity**: SB3 PPO/RecurrentPPO runs robustly on one GPU.
- **Throughput-preserving curriculum**: Offline bank with difficulty metadata; solver kept out of the hot path.
- **Interpretability-ready**: Hooks and scripts for activation capture and analysis.

### Cons:

- **Compute limits**: Single-GPU results won’t match massive distributed runs.
- **Env throughput**: Pure-Python stepping may bottleneck; optimize or vectorize if needed.

## Pipeline
The pipeline for carrying out this process can be broken down into several stages.
<!-- This is where we will describe the actual process of puzzle generation & storage, model training (incl curriculum learning), model evaluation & gameplay visualisation, mechanistic interpretability & visualisation -->

1. **Puzzle bank + curriculum**: Generate puzzles offline, compute optimal lengths, store parquet + manifest; sample by difficulty.
2. **Environment**: Gym-style `RicochetRobotsEnv` with fixed-size RGB observations and discrete (robot×direction) actions; optional no-op.
3. **Training**: SB3 (PPO / RecurrentPPO) with selectable policy (DRC, CNN/ResNet); vectorized envs; TensorBoard logging; checkpoints.
4. **Evaluation**: Success/optimality vs solver on held-out puzzles; qualitative rollouts/videos.
5. **Interpretability**: Activation capture, linear probes, saliency, feature viz, activation patching; export plots/summaries.

## Your Role

You are one of the agents responsible for one of these steps. You will be given access to a set of tools for submitting responses and should only use the tool specified for its specific task, as described by the user message following this system prompt.


## Multi-Stage Development Process

The project development follows a five-stage process:
<!-- This is where we'll describe the project development progress as it is - eg. list steps in the order of development, indicating for each (at very high-level) what state the progress is in -->

1. Environment + solver: implement and test.
2. Training stack: policies (DRC + baselines), PPO configs, curriculum hooks.
3. Execute training runs with checkpointing; monitor metrics.
4. Evaluate optimality/generalization; record rollouts.
5. Interpretability analyses; consolidate documentation.

## Documentation
<!-- This is where we'll describe the documentation -->
### Overview
In the main repository, there is a README.md where we store the overall current status of the project, including pointers to further documentation where appropriate.

### Current task
Relevant context for the current task will be kept in a WorkInProgress.md file.

### Previously completed steps
A folder called ProgressNotes will contain, for each implemented feature, both a concise & detailed summary of the implemented feature and a review of the status of the feature (including any remaining tasks or uncertainties)


## Puzzle Generation Goals
<!-- This is where we'll discuss the types of puzzles that we want to be able to solve. Initially it will be a single target on a givne board layout; later it might be multiple targets in a row where the target squares are pre-defined but only one is active at a time and once a target is hit the next active target is selected at random (this is more like the real, full Ricochet Robots game) -->
The generated data must meet several key requirements. The following sentence summarizes them:

Details are described here:

### 1. 
- Puzzles are solvable; attach minimal-move length from solver.

### 2. 
- Fixed observation size for vectorization; resize/pad if boards vary.

### 3. 
- Difficulty curriculum: sample by optimal length and structural features.

### 4. 
- Metadata manifest for reproducibility and splits.

### 5. 
- Support extensions (multi-target sequences) after single-target baseline.

## Sample Generation Methods
<!-- Descibe how we'll generate and store the puzzles -->

- Offline generation with a layout generator; verify solvability via BFS/A* solver.
- Store boards and metadata in parquet; maintain a manifest with counts, difficulties, and seeds.
- Provide scripts to regenerate banks and preview distributions.

## RL Agent Model Variants

The following is a non-exhaustive list of different types of Models we want to try. The first one listed is the primary method: Deep Repeating Conolutional networks.

The others are baselines, or attempts to replicate the key concept of split-personality training by other means: Inducing a switch in personality in the model's generation, so that the model generates output according to a different goal that is unaffected by the instructions or learned behaviors of the main personality.

### 1. Deep Repeating Conolutional networks (Primary Method)
<!-- Brief description, point to relevant documentation -->

 - Conv encoder → stacked ConvLSTM core iterated N times per step → policy/value heads. Optional no-op action enables explicit thinking steps.
 - Baselines: small CNN/ResNet; MLP for symbolic observations.

## Success measurement criteria

The following are typical review_focus we might use. The details will differ depending on the task selected for step 1 and the Intervention Variant used.
- **Episode success**: Did the agent hit the target within the minimum number of time steps?
- **Episode success (multi-target)**: How many of the given targets did the agent hit within the number of time steps?
- **Minimal solve**: Did the agent hit the target within the optimal (ie minimum) number of steps?
- **Minimal solve (multi-target)**: How many of the given targets did the agent hit within the optimal (ie minimum) number of time steps - where optimum in each case refers to optimum *relative* to the board state at the last hit-target?


## Guidance for creating good boards

- Encourage diverse wall patterns and robot placements; avoid trivial straight-line solves.
- Control difficulty via optimal length bands; include corner/corridor motifs.
- Balance target color/robot across the bank to avoid bias.
## Quality Criteria

### Quick 

### 
- Bank entries validated (solvable + metadata present).
- Env dynamics tests pass.
- Training script runs end-to-end with defaults.
- Evaluation and basic interpretability scripts execute and save outputs.
## Evaluation Framework

### Primary Metrics

- Success rate on held-out bank.
- Optimality gap (agent moves − optimal moves).

### Secondary Metrics

- Episode length distribution; no-op usage (if enabled); value/policy losses.

### Baselines for Comparison

- Random policy; simple greedy heuristic (if any); CNN/ResNet non-recurrent policy.


## Training Considerations

### Stability

### Speed

```

```markdown
*** Begin Patch
*** Update File: rr_base_prompt.md
@@
 ## Training Considerations
 
 ### Stability
- 
+ - Use PPO defaults as a starting point; tune entropy bonus and learning rate.
+ - For recurrent policies, set rollout lengths appropriate for BPTT and mask correctly.
  
 ### Speed
- 
+ - Vectorized envs; avoid solver calls in reset; prefer bank sampling.
+ - Profile env step; optimize hotspots or reduce observation size if needed.
*** End Patch
```

```markdown
You are a helpful assistant whose job it is to assist researchers with the below project:

# Ricochet Robots Reinforcement Learning Project

## Project Overview
Train RL agents to solve Ricochet Robots, then analyze how they plan using mechanistic interpretability. The repo includes: a gym-style environment, offline puzzle bank with curriculum metadata, SB3-based training (incl. recurrent policies), evaluation vs an optimal solver, and analysis tools (probes, saliency, activation patching).

## Core Innovation: 

- End-to-end single-GPU pipeline combining bank-based curriculum with recurrent planning architectures (Deep Repeating ConvLSTM).
- Built-in interpretability suite to test for explicit plan representations and causal features.

### Key Principles:

1. **Reproducibility-first**: Deterministic seeding, banked datasets, saved configs/checkpoints.
2. **Modularity**: Swappable env settings, models, curricula, and analysis scripts.
3. **Single-GPU efficiency**: No online solver during training; vectorized envs; compact models.
4. **Evidence-based analysis**: Use probes and interventions; compare to solver optimality.


## Pros and Cons of the Approach

### Pros:

- **Scalable simplicity**: SB3 PPO/RecurrentPPO runs robustly on one GPU.
- **Throughput-preserving curriculum**: Offline bank with difficulty metadata; solver kept out of the hot path.
- **Interpretability-ready**: Hooks and scripts for activation capture and analysis.

### Cons:

- **Compute limits**: Single-GPU results won’t match massive distributed runs.
- **Env throughput**: Pure-Python stepping may bottleneck; optimize or vectorize if needed.

## Pipeline
The pipeline for carrying out this process can be broken down into several stages.
<!-- This is where we will describe the actual process of puzzle generation & storage, model training (incl curriculum learning), model evaluation & gameplay visualisation, mechanistic interpretability & visualisation -->

1. **Puzzle bank + curriculum**: Generate puzzles offline, compute optimal lengths, store parquet + manifest; sample by difficulty.
2. **Environment**: Gym-style `RicochetRobotsEnv` with fixed-size RGB observations and discrete (robot×direction) actions; optional no-op.
3. **Training**: SB3 (PPO / RecurrentPPO) with selectable policy (DRC, CNN/ResNet); vectorized envs; TensorBoard logging; checkpoints.
4. **Evaluation**: Success/optimality vs solver on held-out puzzles; qualitative rollouts/videos.
5. **Interpretability**: Activation capture, linear probes, saliency, feature viz, activation patching; export plots/summaries.

## Your Role

You are one of the agents responsible for one of these steps. You will be given access to a set of tools for submitting responses and should only use the tool specified for its specific task, as described by the user message following this system prompt.


## Multi-Stage Development Process

The project development follows a five-stage process:
<!-- This is where we'll describe the project development progress as it is - eg. list steps in the order of development, indicating for each (at very high-level) what state the progress is in -->

1. Environment + solver: implement and test.
2. Training stack: policies (DRC + baselines), PPO configs, curriculum hooks.
3. Execute training runs with checkpointing; monitor metrics.
4. Evaluate optimality/generalization; record rollouts.
5. Interpretability analyses; consolidate documentation.

## Documentation
<!-- This is where we'll describe the documentation -->
### Overview
In the main repository, there is a README.md where we store the overall current status of the project, including pointers to further documentation where appropriate.

### Current task
Relevant context for the current task will be kept in a WorkInProgress.md file.

### Previously completed steps
A folder called ProgressNotes will contain, for each implemented feature, both a concise & detailed summary of the implemented feature and a review of the status of the feature (including any remaining tasks or uncertainties)


## Puzzle Generation Goals
<!-- This is where we'll discuss the types of puzzles that we want to be able to solve. Initially it will be a single target on a givne board layout; later it might be multiple targets in a row where the target squares are pre-defined but only one is active at a time and once a target is hit the next active target is selected at random (this is more like the real, full Ricochet Robots game) -->
The generated data must meet several key requirements. The following sentence summarizes them:

Details are described here:

### 1. 
- Puzzles are solvable; attach minimal-move length from solver.

### 2. 
- Fixed observation size for vectorization; resize/pad if boards vary.

### 3. 
- Difficulty curriculum: sample by optimal length and structural features.

### 4. 
- Metadata manifest for reproducibility and splits.

### 5. 
- Support extensions (multi-target sequences) after single-target baseline.

## Sample Generation Methods
<!-- Descibe how we'll generate and store the puzzles -->

- Offline generation with a layout generator; verify solvability via BFS/A* solver.
- Store boards and metadata in parquet; maintain a manifest with counts, difficulties, and seeds.
- Provide scripts to regenerate banks and preview distributions.

## RL Agent Model Variants

The following is a non-exhaustive list of different types of Models we want to try. The first one listed is the primary method: Deep Repeating Conolutional networks.

The others are baselines, or attempts to replicate the key concept of split-personality training by other means: Inducing a switch in personality in the model's generation, so that the model generates output according to a different goal that is unaffected by the instructions or learned behaviors of the main personality.

### 1. Deep Repeating Conolutional networks (Primary Method)
<!-- Brief description, point to relevant documentation -->

 - Conv encoder → stacked ConvLSTM core iterated N times per step → policy/value heads. Optional no-op action enables explicit thinking steps.
 - Baselines: small CNN/ResNet; MLP for symbolic observations.

## Success measurement criteria

The following are typical review_focus we might use. The details will differ depending on the task selected for step 1 and the Intervention Variant used.
- **Episode success**: Did the agent hit the target within the minimum number of time steps?
- **Episode success (multi-target)**: How many of the given targets did the agent hit within the number of time steps?
- **Minimal solve**: Did the agent hit the target within the optimal (ie minimum) number of steps?
- **Minimal solve (multi-target)**: How many of the given targets did the agent hit within the optimal (ie minimum) number of time steps - where optimum in each case refers to optimum *relative* to the board state at the last hit-target?


## Guidance for creating good boards

- Encourage diverse wall patterns and robot placements; avoid trivial straight-line solves.
- Control difficulty via optimal length bands; include corner/corridor motifs.
- Balance target color/robot across the bank to avoid bias.
## Quality Criteria

### Quick 

### 
- Bank entries validated (solvable + metadata present).
- Env dynamics tests pass.
- Training script runs end-to-end with defaults.
- Evaluation and basic interpretability scripts execute and save outputs.
## Evaluation Framework

### Primary Metrics

- Success rate on held-out bank.
- Optimality gap (agent moves − optimal moves).

### Secondary Metrics

- Episode length distribution; no-op usage (if enabled); value/policy losses.

### Baselines for Comparison

- Random policy; simple greedy heuristic (if any); CNN/ResNet non-recurrent policy.


## Training Considerations

### Stability

- Use PPO defaults as a starting point; tune entropy bonus and learning rate.
- For recurrent policies, set rollout lengths appropriate for BPTT and mask correctly.

### Speed

- Vectorized envs; avoid solver calls in reset; prefer bank sampling.
- Profile env step; optimize hotspots or reduce observation size if needed.

```

- I’ve mapped content from the research plan and filled `rr_base_prompt.md` with concise, accurate project context. I’ll now align surrounding docs.

Key updates made to `rr_base_prompt.md`:
- Project overview now summarizes the env, bank/curriculum, SB3 training, evaluation, and interpretability tooling.
- Core innovation and key principles emphasize single-GPU practicality, modularity, and evidence-based interpretability.
- Pros/cons reflect compute constraints and env throughput considerations.
- Pipeline lists: bank → env → training → evaluation → interpretability.
- Five-stage development process listed and sequenced.
- Puzzle generation goals, sample generation methods, and guidance for good boards clarified.
- RL variants summarize DRC and baselines.
- Quality criteria and evaluation framework set primary/secondary metrics and baselines.
- Training considerations include stability and speed tips.

Next, I’ll:
- Create/align `WorkInProgress.md`, ensure `README.md` points to the new prompt and docs, and add a brief index in `ProgressNotes/`.

---

**User**

Seems good (I made a couple of edits). Before we move on to the README.md, I think we need to tidy up the progress notes. Your next task is therefore to read the files in there, and combine them into a series of `<TASK>_Implementation.md` files, where the <TASK> is a specific thing that we have achieved. Later I'll populate with `<TASK>_Review.md` files. It is up to you to determine what counts as an individual `task`; you can take inspiration from the way the files are currently divided but if you think two files share a task, or a file contains more than one task, feel free to reallocate as you see fit. Once you have done this, I will check your work and then if I am happy I'll remove the original files (ie any that you don't create here). Let me know when you are done

---

**Cursor**

I'll scan the contents of ProgressNotes to determine natural task groupings, then draft consolidated <TASK>_Implementation.md files accordingly.

```markdown
*** Begin Patch
*** Add File: ProgressNotes/Environment_Implementation.md
+# Environment Implementation (Ricochet Robots)

## Overview
A canonical Ricochet Robots environment with edge-wall mechanics and a Gymnasium-style API. Supports both image and symbolic observations and optional no-op actions. Deterministic under seeds and compatible with solver-based solvability filtering.

## Core Features
- Canonical edge walls: `h_walls (H+1,W)`, `v_walls (H,W+1)` with borders
- Sliding dynamics: robots move until blocked by an edge-wall or another robot
- Actions: Discrete `num_robots*4 (+1 if noop)` encoded as `(robot_id, direction)`
- Termination: target robot reaches goal → `terminated=True`; `truncated=True` on `max_steps`
- Observation modes:
  - Image (channels-first or channels-last)
    - Channels: 4 directional wall planes, 1 goal plane, 1 target-robot mask, then one plane per robot
  - Symbolic: `[goal_r, goal_c, target_id_one_hot, robots_flat]`
- Optional no-op with separate `noop_penalty` from move `step_penalty`

## Random Generation & Spec Enforcement
- Constructive generator enforces `env/WallSpecifications.md`:
  - Forbidden central block (size depends on board parity)
  - Per-quadrant structures: Edge-T juts and Central-Ls with adjacency/placement constraints
  - Goal placement inside a Central-L cell (deterministic under seed with fallback)
  - Robots/goals never in central forbidden block
- Configurable counts per quadrant: `edge_t_per_quadrant`, `central_l_per_quadrant`

## Determinism & Seeding
- Stores base seed at init; re-seeds RNG on each `reset()` when no new seed passed
- `reset(seed=...)` updates base seed; `info["episode_seed"]` reported

## Rendering
- `render_mode="ascii"`: returns string frame
- `render_mode="rgb"`: returns RGB array (with profile hooks); configurable style via `render_rgb_config`

## API & Gymnasium Compliance
- `reset(seed=...) -> (obs, info)`; `step(action) -> (obs, reward, terminated, truncated, info)`
- `info` schema includes: `is_success`, `TimeLimit.truncated`, `level_solvable`, `ensure_solvable_enabled`, `optimal_length` (when enforced), `solver_limits`, `channel_names` (image)
- Public `get_board()` accessor returns a safe clone

## Solvability Integration
- `ensure_solvable=True` filters boards via BFS; reports `optimal_length` in `info`

## Tests
- Sliding/stopping on walls and robots
- Goal termination and truncation behavior
- Multi-robot blocking
- Observation shapes/semantics; channels-first/last and symbolic
- No-op semantics
- Env–solver roundtrip with reported `optimal_length`

## Known Limitations
- Pure-Python step may bottleneck throughput; consider profiling/optimization
- Wall-channel caching assumes immutable walls mid-episode
- Duplicate truncation signals possible if also TimeLimit-wrapped externally
*** End Patch
```

I'll create consolidated implementation files in ProgressNotes for Environment, Solver, Puzzle Bank, Training Framework, and Profiling based on the existing notes, without deleting originals. Then I'll mark the documentation alignment task complete.

```markdown
*** Begin Patch
*** Add File: ProgressNotes/Environment_Implementation.md
+# Environment_Implementation

## Overview
Canonical Ricochet Robots environment with edge-wall mechanics and Gymnasium-style API. Deterministic under seeds; supports image and symbolic observations; optional no-op action.

## Core Features
- Edge walls: `h_walls (H+1,W)`, `v_walls (H,W+1)` with borders
- Sliding dynamics: robots move until blocked by a wall or robot
- Actions: Discrete `num_robots*4 (+1 if noop)` encoded as `(robot_id, direction)`
- Termination: goal reached; truncation via `max_steps`
- Observations:
  - Image: directional wall planes (4), goal, target-robot mask, one plane per robot; channels-first/last
  - Symbolic: `[goal_r, goal_c, target_one_hot, robots_flat]`
- No-op: separate `noop_penalty` from step penalty

## Random Generation & Spec Enforcement
- Constructive generator enforces `env/WallSpecifications.md`:
  - Forbidden central block (size by parity)
  - Per-quadrant Edge-T juts and Central-Ls with adjacency/placement rules
  - Balanced orientations and counts via `edge_t_per_quadrant`, `central_l_per_quadrant`
  - Target placed inside a Central-L (deterministic under seed; safe fallback)
  - Robots/goals never inside the forbidden central block

## Determinism & Seeding
- Base seed stored at init; RNG reinitialized on each `reset()` unless a new seed is provided
- `reset(seed=...)` updates base seed; `info["episode_seed"]` reported

## Rendering
- `render_mode="ascii"`: returns string
- `render_mode="rgb"`: returns RGB array; profile hooks for draw steps; configurable style

## API & Info Schema
- Gymnasium: `reset(seed=...) -> (obs, info)`, `step(action) -> (obs, reward, terminated, truncated, info)`
- `info` keys: `is_success`, `TimeLimit.truncated`, `level_solvable`, `ensure_solvable_enabled`, `optimal_length` (when enforced), `solver_limits`, `channel_names` (image)
- Public `get_board()` returns safe clone

## Solvability Integration
- `ensure_solvable=True` filters boards via BFS and reports `optimal_length`

## Tests
- Sliding/stopping, goal termination, truncation, multi-robot blocking
- Observation shapes/semantics for image and symbolic; channels-first/last
- No-op semantics; env–solver roundtrip

## Notes & Limitations
- Pure-Python env may bottleneck; consider profiling/optimization
- Wall-channel caching assumes walls immutable mid-episode
- Avoid double truncation when externally TimeLimit-wrapped
*** End Patch
```

---

**User**

Please continue 

---

**Cursor**

I'll create consolidated implementation files in ProgressNotes for Environment, Solver, Puzzle Bank, Training Framework, and Profiling by distilling the existing notes, then mark the documentation alignment task complete.

```markdown
*** Begin Patch
*** Add File: ProgressNotes/Environment_Implementation.md
# Environment_Implementation

## Overview
Canonical Ricochet Robots environment with edge-wall mechanics and a Gymnasium-style API. Deterministic under seeds; supports image and symbolic observations; optional no-op action.

## Core Features
- Edge walls: `h_walls (H+1,W)`, `v_walls (H,W+1)` with borders
- Sliding dynamics: robots move until blocked by a wall or robot
- Actions: Discrete `num_robots*4 (+1 if noop)` encoded as `(robot_id, direction)`
- Termination: goal reached; truncation via `max_steps`
- Observations:
  - Image: directional wall planes (4), goal, target-robot mask, one plane per robot; channels-first/last
  - Symbolic: `[goal_r, goal_c, target_one_hot, robots_flat]`
- No-op: separate `noop_penalty` from step penalty

## Random Generation & Spec Enforcement
- Constructive generator enforces `env/WallSpecifications.md`:
  - Forbidden central block (size by parity)
  - Per-quadrant Edge-T juts and Central-Ls with adjacency/placement rules
  - Balanced orientations and counts via `edge_t_per_quadrant`, `central_l_per_quadrant`
  - Target placed inside a Central-L (deterministic under seed; safe fallback)
  - Robots/goals never inside the forbidden central block

## Determinism & Seeding
- Base seed stored at init; RNG reinitialized on each `reset()` unless a new seed is provided
- `reset(seed=...)` updates base seed; `info["episode_seed"]` reported

## Rendering
- `render_mode="ascii"`: returns string
- `render_mode="rgb"`: returns RGB array; profile hooks for draw steps; configurable style

## API & Info Schema
- Gymnasium: `reset(seed=...) -> (obs, info)`, `step(action) -> (obs, reward, terminated, truncated, info)`
- `info` keys: `is_success`, `TimeLimit.truncated`, `level_solvable`, `ensure_solvable_enabled`, `optimal_length` (when enforced), `solver_limits`, `channel_names` (image)
- Public `get_board()` returns safe clone

## Solvability Integration
- `ensure_solvable=True` filters boards via BFS and reports `optimal_length`

## Tests
- Sliding/stopping, goal termination, truncation, multi-robot blocking
- Observation shapes/semantics for image and symbolic; channels-first/last
- No-op semantics; env–solver roundtrip

## Notes & Limitations
- Pure-Python env may bottleneck; profile/optimize if needed
- Wall-channel caching assumes walls immutable mid-episode
- Avoid double truncation if externally TimeLimit-wrapped
*** End Patch
```

Creating consolidated ProgressNotes implementation files now: Environment, Solver, PuzzleBank, TrainingFramework, Profiling. Then I’ll mark the docs alignment task complete.

```markdown
*** Begin Patch
*** Add File: ProgressNotes/Environment_Implementation.md
# Environment_Implementation

## Overview
Canonical Ricochet Robots environment with edge-wall mechanics and a Gymnasium-style API. Deterministic under seeds; supports image and symbolic observations; optional no-op action.

## Core Features
- Edge walls: h_walls (H+1,W), v_walls (H,W+1) with borders
- Sliding dynamics: robots move until blocked by a wall or robot
- Actions: Discrete num_robots*4 (+1 if noop) encoded as (robot_id, direction)
- Termination: goal reached; truncation via max_steps
- Observations:
  - Image: 4 directional wall planes, goal, target-robot mask, one plane per robot; channels-first/last
  - Symbolic: [goal_r, goal_c, target_one_hot, robots_flat]
- No-op: separate noop_penalty from step penalty

## Random Generation & Spec Enforcement
- Constructive generator enforces env/WallSpecifications.md:
  - Forbidden central block (size by parity)
  - Per-quadrant Edge-T juts and Central-Ls with adjacency/placement rules
  - Balanced orientations and counts via edge_t_per_quadrant, central_l_per_quadrant
  - Target placed inside a Central-L (deterministic under seed; safe fallback)
  - Robots/goals never inside the forbidden central block

## Determinism & Seeding
- Base seed stored at init; RNG reinitialized on each reset() unless a new seed is provided
- reset(seed=...) updates base seed; info["episode_seed"] reported

## Rendering
- render_mode="ascii": returns string
- render_mode="rgb": returns RGB array; profile hooks for draw steps; configurable style

## API & Info Schema
- Gymnasium: reset(seed=...) -> (obs, info), step(action) -> (obs, reward, terminated, truncated, info)
- info keys: is_success, TimeLimit.truncated, level_solvable, ensure_solvable_enabled, optimal_length (when enforced), solver_limits, channel_names (image)
- Public get_board() returns safe clone

## Solvability Integration
- ensure_solvable=True filters boards via BFS and reports optimal_length

## Tests
- Sliding/stopping, goal termination, truncation, multi-robot blocking
- Observation shapes/semantics for image and symbolic; channels-first/last
- No-op semantics; env–solver roundtrip

## Notes & Limitations
- Pure-Python env may bottleneck; profile/optimize if needed
- Wall-channel caching assumes walls immutable mid-episode
- Avoid double truncation if externally TimeLimit-wrapped
*** End Patch
```

```markdown
# Environment_Implementation

## Overview
Canonical Ricochet Robots environment with edge-wall mechanics and a Gymnasium-style API. Deterministic under seeds; supports image and symbolic observations; optional no-op action.

## Core Features
- Edge walls: h_walls (H+1,W), v_walls (H,W+1) with borders
- Sliding dynamics: robots move until blocked by a wall or robot
- Actions: Discrete num_robots*4 (+1 if noop) encoded as (robot_id, direction)
- Termination: goal reached; truncation via max_steps
- Observations:
  - Image: 4 directional wall planes, goal, target-robot mask, one plane per robot; channels-first/last
  - Symbolic: [goal_r, goal_c, target_one_hot, robots_flat]
- No-op: separate noop_penalty from step penalty

## Random Generation & Spec Enforcement
- Constructive generator enforces env/WallSpecifications.md:
  - Forbidden central block (size by parity)
  - Per-quadrant Edge-T juts and Central-Ls with adjacency/placement rules
  - Balanced orientations and counts via edge_t_per_quadrant, central_l_per_quadrant
  - Target placed inside a Central-L (deterministic under seed; safe fallback)
  - Robots/goals never inside the forbidden central block

## Determinism & Seeding
- Base seed stored at init; RNG reinitialized on each reset() unless a new seed is provided
- reset(seed=...) updates base seed; info["episode_seed"] reported

## Rendering
- render_mode="ascii": returns string
- render_mode="rgb": returns RGB array; profile hooks for draw steps; configurable style

## API & Info Schema
- Gymnasium: reset(seed=...) -> (obs, info), step(action) -> (obs, reward, terminated, truncated, info)
- info keys: is_success, TimeLimit.truncated, level_solvable, ensure_solvable_enabled, optimal_length (when enforced), solver_limits, channel_names (image)
- Public get_board() returns safe clone

## Solvability Integration
- ensure_solvable=True filters boards via BFS and reports optimal_length

## Tests
- Sliding/stopping, goal termination, truncation, multi-robot blocking
- Observation shapes/semantics for image and symbolic; channels-first/last
- No-op semantics; env–solver roundtrip

## Notes & Limitations
- Pure-Python env may bottleneck; profile/optimize if needed
- Wall-channel caching assumes walls immutable mid-episode
- Avoid double truncation if externally TimeLimit-wrapped
```

```markdown
*** Begin Patch
*** Add File: ProgressNotes/Solver_Implementation.md
# Solver_Implementation

## Overview
BFS (optimal within limits) and A* solvers used for validation, curriculum metadata, and optional solvability filtering during environment reset.

## BFS
- Cost: one slide = one step (minimizes slides)
- Deterministic neighbor expansion (sorted robot ids)
- Pruning: skip non-moves; optional repeated (robot, direction) suppression
- Metadata result (actions, nodes_expanded, depth, found)
- Limits: max_depth, max_nodes; hitting limits may return None/unsolved

## A*
- Heuristics:
  - admissible_zero (h=0)
  - admissible_one (h ∈ {0,1})
  - manhattan_cells (non-admissible; faster, not guaranteed optimal)
- Metadata mirrors BFS

## Helpers
- apply_actions(board, actions): replay a plan
- serialize(board): hashable visited-state key over robot positions

## Determinism & Integration
- Deterministic given fixed board and parameters
- Env integration: ensure_solvable=True uses BFS; env reports level_solvable, ensure_solvable_enabled, optimal_length

## Tests
- Optimality on tiny boards
- Cutoff behavior
- Multi-robot prerequisite moves
- A* (admissible) matches BFS optimal plans
- Zero-length start-on-goal

## Notes
- Prefer BFS/A* (admissible) when curriculum thresholds require optimal lengths
- Potential speedups: greedy neighbor ordering without changing correctness
*** End Patch
```

```markdown
# Solver_Implementation

## Overview
BFS (optimal within limits) and A* solvers used for validation, curriculum metadata, and optional solvability filtering during environment reset.

## BFS
- Cost: one slide = one step (minimizes slides)
- Deterministic neighbor expansion (sorted robot ids)
- Pruning: skip non-moves; optional repeated (robot, direction) suppression
- Metadata result (actions, nodes_expanded, depth, found)
- Limits: max_depth, max_nodes; hitting limits may return None/unsolved

## A*
- Heuristics:
  - admissible_zero (h=0)
  - admissible_one (h ∈ {0,1})
  - manhattan_cells (non-admissible; faster, not guaranteed optimal)
- Metadata mirrors BFS

## Helpers
- apply_actions(board, actions): replay a plan
- serialize(board): hashable visited-state key over robot positions

## Determinism & Integration
- Deterministic given fixed board and parameters
- Env integration: ensure_solvable=True uses BFS; env reports level_solvable, ensure_solvable_enabled, optimal_length

## Tests
- Optimality on tiny boards
- Cutoff behavior
- Multi-robot prerequisite moves
- A* (admissible) matches BFS optimal plans
- Zero-length start-on-goal

## Notes
- Prefer BFS/A* (admissible) when curriculum thresholds require optimal lengths
- Potential speedups: greedy neighbor ordering without changing correctness
```

```markdown
# PuzzleBank_Implementation

## Overview
Offline, precomputed puzzle bank enabling curriculum learning without online solver calls during training. Stores puzzles, solution metadata, and supports efficient sampling by criteria.

## Components
- PuzzleBank (env/puzzle_bank.py): storage, querying, statistics
- SpecKey / SolverKey: generation and solver configurations
- PuzzleMetadata: puzzle data + solution traces
- BankSampler: sampling strategies for training
- Precomputation pipeline (env/precompute_pipeline.py): generators and batch creation
- Criteria filtering (env/criteria_env.py): PuzzleCriteria, CriteriaFilteredEnv, BankCurriculumManager
- Curriculum integration (env/curriculum.py): create_bank_curriculum_levels/manager, BankCurriculumWrapper

## Storage Format
- Parquet files per spec partition; manifest.json summarizing partitions and counts
- Entry schema: identifiers (seed, spec, solver, board_hash), solution metrics (optimal_length, robots_moved, nodes_expanded, solve_time_ms), flags (solved_within_limits, hit_depth_limit, hit_node_limit), plan trace (actions_encoded), optional layout_blob

## Curriculum Levels (example)
1. Level 0: 1 robot, no walls (1–3 moves)
2. Level 1: 1 robot, some walls (2–6 moves)
3. Level 2: Multiple robots, simple (3–8 moves)
4. Level 3: Multiple robots, more walls (4–10 moves)
5. Level 4: Full complexity (5–15 moves)

## Usage
- Precompute: precompute_bank.py with --bank_dir, --num_puzzles, --solver_depth
- Training: load PuzzleBank, create curriculum manager, wrap env with BankCurriculumWrapper (fixed-size RGB, channels_first)
- Direct criteria: CriteriaFilteredEnv with PuzzleCriteria

## Performance & Determinism
- Deterministic seeds and canonical solver configuration
- Fast sampling; no solver calls in the training hot path
- Partitioned storage for efficient filtering; consider compaction for large banks

## Testing
- Run test_bank_system.py and example_bank_usage.py

## Future Enhancements
- Stratified sampling across difficulty bins
- Optional layout blobs and incremental updates
- Distributed generation and advanced criteria
```

```markdown
# TrainingFramework_Implementation

## Overview
Complete SB3 PPO-based training stack with milestone environments, multiple policy architectures (MLP, SmallCNN, ConvLSTM), evaluation, logging, and checkpointing.

## Core
- Stable-Baselines3 PPO (primary); vectorized envs; Gymnasium-compatible
- Comprehensive CLI (20+ options), TensorBoard logging, periodic checkpoints, evaluation tools
- Auto policy selection by grid size/obs mode

## Architectures
- MLP: symbolic or tiny grids
- SmallCNN (models/policies.py): 3×3 convs, GAP, FC; safe for small images
- ConvLSTM (models/convlstm.py): multi-layer ConvLSTM; repeats per timestep (DRC-lite)

## Milestone Environments & Tests
- v0: 4×4 single-move; v1: four 5×5 one-move each direction
- tests/test_training_milestones.py: v0 ≥90% success, v1 ≥80%/direction
- tests/test_policy_outputs.py: shape checks, smoke runs

## Observation Handling
- Channels-first images, normalize_images=False
- Fixed layouts set geometry before building spaces; observation shapes consistent

## CLI Examples
- Basic, architecture selection, evaluation/checkpointing, custom hyperparams

## Notes & Limitations
- ConvLSTM not fully integrated with SB3 recurrent policy; memory-sensitive on long sequences
- Tiny grids auto-fallback to MLP; no-op action available but lightly tested

## Ready for Interpretability
- Checkpoints loadable; activation hooks via custom extractors; evaluation and logs available
```

```markdown
# Profiling_Implementation

## Overview
Comprehensive profiling for the RGB training pipeline to identify bottlenecks across environment rendering, model forward passes, and training.

## Components
- profiling/profiler.py: Profiler + ProfilerStats (timing, memory, call counts), threadsafe, low overhead
- env/ricochet_env.py: RGB observation profiling (rendering, resizing, draw steps)
- models/convlstm.py, models/recurrent_policy.py: ConvLSTM and policy forward profiling
- train_agent.py: flags --enable-profiling, --profiling-report, --profiling-summary-freq
- profiling/analyzer.py: bottleneck detection, category analysis, charts, recommendations
- Tests/examples and profiling/README.md

## Expected Bottlenecks
- env_rgb_rendering, env_rgb_resizing, convlstm_forward_pass, env_rgb_observation_generation

## Usage
- Enable in training with --enable-profiling and optional report/summary settings
- Analyze with profiling.analyzer to produce reports and charts

## Overheads
- ~0.1ms per operation; ~1MB memory overhead; negligible GPU overhead

## Next Steps
- Run profiling on short runs, analyze, optimize hotspots, iterate
```

- I created consolidated implementation notes in `ProgressNotes`:
  - `Environment_Implementation.md`
  - `Solver_Implementation.md`
  - `PuzzleBank_Implementation.md`
  - `TrainingFramework_Implementation.md`
  - `Profiling_Implementation.md`

- I left the original files untouched for your review. If you’re happy, you can remove the superseded originals you no longer need.

- I also incorporated your tweaks to `rr_base_prompt.md` references to match these files.

- Next, I can add a brief index section to `ProgressNotes/README.md` linking to each new `<TASK>_Implementation.md`, or proceed to `README.md` migration.

---

**User**

Great work! I've removed the unnecessary files myself. So yes, please can you go ahead and update the ProgressNotes/README.md and then continue on to the overall README.md?

---

**Cursor**

I’m going to update `ProgressNotes/README.md` to link to the new `<TASK>_Implementation.md` files, then read and update the root `README.md` to point to the new workflow centered on `rr_base_prompt.md`.

```markdown
## Progress Notes Overview

This folder now organizes progress by concrete tasks, each with an implementation summary. Reviews will be added separately as `<TASK>_Review.md` files.

See `env/WallSpecifications.md` for authoritative environment constraints.

# Progress Notes

## Task Index
- `Environment_Implementation.md`
- `Solver_Implementation.md`
- `PuzzleBank_Implementation.md`
- `TrainingFramework_Implementation.md`
- `Profiling_Implementation.md`

## Legacy Notes (kept for context)
- `decisions_and_rationale.md`
- `next_steps.md`

```

```markdown
# Ricochet Robots RL Project

This repository trains RL agents for Ricochet Robots and analyzes how they plan using mechanistic interpretability. The project workflow and documentation are centered on `rr_base_prompt.md`.

## Installation

```bash
pip install -r requirements.txt
```

## Quick Start

### Training

Train a PPO agent on milestone environments:

```bash
# v0: Single-move task (4x4 grid)
python train_agent.py --env-mode v0 --timesteps 2000 --n-envs 4

# v1: Four-direction task (5x5 grids)
python train_agent.py --env-mode v1 --timesteps 6000 --n-envs 4

# Random environment (8x8 grid)
python train_agent.py --env-mode random --timesteps 100000 --n-envs 8
```

### Model Architectures

Choose different policy architectures:

```bash
# Default MLP for small grids
python train_agent.py --env-mode v0 --obs-mode image

# Custom small CNN for larger grids
python train_agent.py --env-mode random --small-cnn --obs-mode image

# ConvLSTM (DRC-lite) for recurrent planning
python train_agent.py --env-mode random --convlstm --obs-mode image --lstm-layers 2 --lstm-repeats 1

# Symbolic observations with MLP
python train_agent.py --env-mode random --obs-mode symbolic
```

### Evaluation

Evaluate a trained model:

```bash
python evaluate_agent.py --model-path checkpoints/ppo_model.zip --env-mode v0 --episodes 50
```

## Environment Options

- `--env-mode`: `random`, `v0`, `v1` (milestone layouts)
- `--height`, `--width`: Grid dimensions (default 8x8)
- `--num-robots`: Number of robots (default 2)
- `--include-noop`: Enable no-op actions for "thinking"
- `--obs-mode`: `image` or `symbolic` observations
- `--ensure-solvable`: Only generate solvable puzzles (slower)

## Training Options

- `--timesteps`: Total training steps
- `--n-envs`: Number of parallel environments
- `--lr`: Learning rate (default 3e-4)
- `--n-steps`: Rollout length (default 128)
- `--batch-size`: Minibatch size (default 256)

## Logging and Checkpoints

- `--log-dir`: TensorBoard log directory
- `--save-path`: Model save path
- `--save-freq`: Checkpoint frequency (timesteps)
- `--eval-freq`: Evaluation frequency (timesteps)

## Testing

Run the test suite:

```bash
pytest tests/ -v
```

## Visualisation

Two offline renderers are available:

- Matplotlib-style raster renderer: `env/visuals/mpl_renderer.py`
- Plotly renderer (PNG export via kaleido): `env/visuals/plotly_renderer.py`

Use `initial_visualisation_cells.py` (with `#%%` cells) to:
- Load the bank curriculum from `puzzle_bank` and `curriculum_config_example2.json`
- Load a checkpointed PPO model
- Record and preview episodes at a few curriculum levels with both renderers

## Architecture Details

### SmallCNN
- 3x3 conv layers with padding=1 (safe for small grids)
- Global average pooling
- Suitable for 4x4+ grids

### ConvLSTM (DRC-lite)
- Convolutional LSTM layers for spatial planning
- Configurable layers and repeats per timestep
- Designed for recurrent planning behavior

### Milestone Environments
- **v0**: 4x4 grid, single RIGHT move to goal
- **v1**: 5x5 grids, one move in each of four directions

## Documentation Map

- Core prompt/workflow: `rr_base_prompt.md`
- Progress notes (task-based): `ProgressNotes/README.md`
  - `Environment_Implementation.md`
  - `Solver_Implementation.md`
  - `PuzzleBank_Implementation.md`
  - `TrainingFramework_Implementation.md`
  - `Profiling_Implementation.md`
- Bank curriculum examples: `curriculum_config_example*.json`
- Visualisation notebooks: `initial_visualisation_cells.py`
- Profiling tools: `profiling/`

## Next Steps

This completes Step 2 of the research plan. The next phase involves:
- Training agents on more complex environments
- Mechanistic interpretability analysis
- Probing internal representations
- Causal intervention experiments
```

- Updated `ProgressNotes/README.md` to index the new task-based files and keep legacy notes for context.
- Updated root `README.md` to point to `rr_base_prompt.md`, add a documentation map, and retain existing quick-start/use sections.

Anything else you want adjusted before we move on?

