# Review feedback for research proposal step 1.2
_Exported on 12/09/2025 at 10:23:44 BST from Cursor (1.4.5)_

---

**User**

You are a language model, working as part of a team to implement a research proposal. Please take the time to read the ResearchPlan.md.

You have been tasked with reviewing the work of a colleague who has been tasked with implementing 'step 1.2' of the plan detailed in ResearchPlan.md (note that another colleague has already checked and confirmed that the code is good for step 1.1). You are not to change any code, but instead to review the work and give a list of feedback, ordered roughly from highest importance to lowest. Please go ahead and get started. Be as critical as you like.

---

**Cursor**

I'll scan the research plan and the main environment implementation to understand what "step 1.2" entails, then pull in related supporting files and tests to assess alignment and completeness. Next, I'll review the remaining relevant modules and tests to verify behavior and identify gaps or issues.

I'll review the solver tests and environment tests to validate how well the step 1.2 solver integrates and meets the plan's requirements.

I read the research plan and the current solver/environment code to assess “step 1.2”.

- Status: I reviewed `env/solver.py`, its integration points in `env/ricochet_env.py`, and the tests under `tests/`. Below is prioritized feedback focused on step 1.2.

### Feedback (highest priority first)

- Optimality guarantee clarity
  - The BFS explores edges with uniform cost (each slide counts as one move), so it returns a minimal number-of-moves solution if not constrained. However, because `max_depth` and `max_nodes` can cut off the search, returning None may reflect limits rather than unsolvability. Consider documenting this explicitly in the solver docstring and in any call sites that interpret None.

- State hashing scope and future safety
  - `serialize` uses only robot positions; this is correct for a single-instance BFS where walls/goal/target are constant. It’s worth a warning in the docstring that the key is only valid for the current board specification and must not be reused across boards differing in walls/goal/target. If future code adds variable elements (e.g., toggles/doors), the hash must be extended.

- Performance and pruning
  - You already prune non-moves by skipping neighbors where the sliding results in the same position—good. Additional inexpensive pruning could help:
    - Skip “immediate reversals” patterns where a robot bounces back and forth with no intervening constraint changes. A simple last-action guard can reduce branching.
    - Optionally avoid repeating the same robot in the same direction twice in a row when walls/blocks haven’t changed (this often can’t help in Ricochet).
  - Consider ordering expansions (e.g., try moves with the target robot first) to reach goals earlier and shrink the search frontier; it doesn’t change correctness but improves average runtime.

- Memory/time efficiency
  - `apply_action` clones the board (deep copies arrays) for every neighbor; this is simple but costly. If performance becomes an issue, you could:
    - Store walls by reference (they are immutable during search), cloning only robot positions.
    - Represent `robot_positions` as a small fixed-size tuple instead of a dict for faster hashing and less per-state overhead.
  - Path construction via `path + [act]` copies lists per edge. A common BFS trick is to store predecessors and reconstruct the path when goal is found; this reduces allocations.

- A* option and heuristic
  - The plan mentioned A* as an alternative. Even a weak admissible heuristic (e.g., Manhattan distance of the target robot to goal, optionally wall-aware lower bounds) dramatically reduces nodes expanded on harder puzzles. Keeping the current BFS as default but exposing an A* variant would align better with 1.2’s “BFS or A*” guidance.

- API coherence
  - `SolveResult` is defined but unused; either return it from `solve_bfs` or remove it to avoid confusion. Returning a small dataclass with `actions` and simple metadata (depth, nodes_expanded) would be useful for curriculum and evaluation.

- Integration with curriculum (step 1.2 intent)
  - The environment uses the solver only as a solvability filter. Step 1.2 suggests also measuring optimal length for difficulty control. You could expose a thin wrapper that returns both solution and length (or None) to facilitate “generate puzzles solvable in ≤ N moves.”

- Tests: coverage and guarantees
  - `tests/test_solver.py` checks success but not optimality. Add a minimal puzzle where the unique optimal length is known (e.g., 1 or 2 moves) and assert `len(actions)` matches optimal. Also add a test that hits `max_depth` or `max_nodes` and expects None.
  - Consider a multi-robot test where solving requires moving a non-target robot first (validates branching over all robots).

- Documentation
  - Add brief docstrings to `neighbors`, `serialize`, and `solve_bfs` documenting assumptions (constant board spec), cost definition (one slide = one step), and the meaning of limits (`max_depth`, `max_nodes`).
  - In `RicochetRobotsEnv.reset(ensure_solvable=True)`, note that the solver’s None may be due to search limits rather than true unsolvability.

- Minor correctness/robustness notes
  - `neighbors` iterates `board.robot_positions.keys()` (insertion order); that’s fine because `serialize` uses sorted keys for consistency. For determinism, consider iterating `sorted(keys)` too.
  - Consider returning metadata from `solve_bfs` (e.g., nodes expanded) to aid profiling.

- Misc cleanup
  - Remove or use `SolveResult` to avoid dead code.
  - A tiny type hint polish: annotate `neighbors` return type concretely as `List[Tuple[Board, Action]]` (already done) and `solve_bfs` docstring return semantics.

- Future-proofing for evaluation (step 1.2)
  - Provide a small helper that applies an action list to a `Board`, returning the terminal board and success—handy for validation pipelines and keeps that logic out of tests/duplicated code.

- Nice-to-have ergonomics
  - Optional seedable move ordering (e.g., prioritize directions likely to reduce row/col deltas to the goal first) can speed early solutions without affecting optimality.

- Env integration nits (adjacent to 1.2)
  - `reset` with `ensure_solvable=True` sets `info["level_solvable"]=True` even though it doesn’t check optimal length; accurate, but consider including `info["optimal_length"]=len(solution)` when available for downstream curriculum.

- Test reproducibility
  - The tests construct walls with borders and a few internal walls; good. You might add a fixed multi-robot case used both in env tests and solver tests to ensure consistency across modules.

What I’ll do next if helpful: add targeted tests for optimality and cutoff behavior, and propose a minimal A* variant and small API changes to expose solution length for curriculum.

- Summary: The BFS solver is correct, minimal, and integrates cleanly with the env for solvability checks. Key improvements are clearer docs around constraints, modest pruning/efficiency enhancements, tests that assert optimality and cutoff behavior, and exposing solution length/metadata to support curriculum and evaluation as envisioned in step 1.2.

