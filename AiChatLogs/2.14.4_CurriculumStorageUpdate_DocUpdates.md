# Document implementation and update progress notes
_Exported on 18/09/2025 at 15:47:38 BST from Cursor (1.4.5)_

---

**User**

Please think about the current task. It has been implemented, and you can see the implementation and review notes in the WorkInProgress file. Please go through the implementation & notes, reading any code you think you need to, and then make an entry and/or updates to the ProgressNotes folder (as you see fit) as well as any additions to the project-wide and ProgressNotes READMEs that you think are necessary, to document the changes made during this stage of the work. Include whatever information you think is helpful for future colleagues looking at this codebase. Note that the WorkInProgress file will be wiped after you make your notes, so you should document anything that you think is important from there.

---

**Cursor**

I'll scan the current notes to extract what changed, then draft concise Implementation and Review docs in `docs/ProgressNotes/`, and update the top-level `README.md` and `docs/README.md` as needed. Proceeding to read `WorkInProgress.md`, the READMEs, and `docs/ProgressNotes` contents.

I'll open the relevant modules and READMEs in parallel to extract concrete details to document, then create/update the ProgressNotes entries and READMEs accordingly.

I'll create a concise todo list for the documentation updates, then make the new ProgressNotes entries and update the READMEs.

I'll start by adding the implementation note file capturing scope, changes, CLI usage, and migration notes. Then I'll proceed to the review doc and README updates.

```markdown
# Curriculum Bank Redesign — Implementation

## Summary
- Reworked curriculum to use a bank-first, offline precomputation workflow by default.
- Added manifest v1.1 with per-partition histograms keyed by (optimal_length, robots_moved), fingerprints scaffolding, and dedup section.
- Implemented robust replay via compact binary `layout_blob` with round-trip reconstruction; retained regeneration by seed + hash.
- Introduced a band-first precompute controller that solves broadly and stops when each curriculum level has sufficient coverage as measured by histograms.
- Added stratified sampling with quota allocation and leftover reallocation; integrated into curriculum env to avoid online fallback during training by default.

## Key Components and Edits
- `env/puzzle_bank.py`
  - Manifest upgrade to v1.1; track `puzzle_count` and histograms under `histogram.by_optimal_length_and_robots_moved` with keys like `ol=4|rm=2`.
  - `PuzzleMetadata` now carries `layout_blob`, `layout_hash`, `board_seed`, solver metadata (`nodes_expanded`, `solve_time_ms`, limits flags).
  - Added binary hashing for board/layout; added `serialize_layout_to_blob`/`deserialize_layout_blob` and `create_fixed_layout_from_metadata`/`_from_seed`.
  - `BankSampler` now maintains an LRU cache for filtered iterators and supports `sample_puzzles_stratified` with monitoring via `get_last_stratified_plan`.

- `env/precompute_pipeline.py`
  - `PuzzleGenerator` solves with metadata (`solve_bfs_with_metadata`/`solve_astar_with_metadata`), stores layout blob and hashes, and records robots moved.
  - Clarified solver cutoff naming via `solver_max_depth_override` in controller path.
  - `compute_level_coverage_from_histograms` computes per-level availability using manifest histograms.
  - `run_band_first_precompute_controller` iterates in chunks per spec until each level reaches `target_per_level` or global budget is hit.

- `generate_curriculum_bank.py`
  - New flags: `--use_controller`, `--target_per_level`, `--max_puzzles_global`, `--chunk_per_spec`, plus solver options.
  - When using the controller, generation is band-first and curriculum filters are not applied during generation.

- Curriculum integration
  - `CriteriaFilteredEnv` and `BankCurriculumManager` use the bank samplers by default; `allow_fallback` is False by default to prevent silent online solving during training.

## CLI Quickstart
- Band-first controller (recommended):
```
python generate_curriculum_bank.py \
  --use_controller \
  --target_per_level 1000 \
  --max_puzzles_global 200000 \
  --chunk_per_spec 200 \
  --bank_dir ./puzzle_bank
```
- Training with bank curriculum:
```
python train_agent.py --curriculum --use_bank_curriculum --obs-mode image --n-envs 8
```

## Data and Schema Notes
- Manifest format v1.1:
  - `version`, `partitions.{partition_key}.spec_key`, `puzzle_count`, `histogram.by_optimal_length_and_robots_moved`, `fingerprints`, `dedup`.
- Parquet partition path is keyed by `SpecKey.to_partition_key()`.
- Histograms are incrementally updated during `add_puzzles`; a verification tool is recommended for periodic reconciliation.

## Behavioral Changes
- Training defaults to bank-based sampling; online filtered generation is for smoke tests and explicit opt-in only.
- `CriteriaFilteredEnv` raises with guidance if sampling underfills and `allow_fallback=False`.
- RGB renderer fixes ensure consistent visuals across renderers.

## Migration & Backward Compatibility
- Existing banks are upgraded in-memory to manifest v1.1 on load; new fields are backfilled with defaults.
- Banks written prior to layout blobs continue to work via seed regeneration; new runs will add blobs for robust replay.
- Recommend running a one-off verification to detect duplicates and reconcile histograms for legacy data.

## Open Items / Future Work
- Add `verify_manifest` utility to recompute histograms and detect duplicates by `(board_hash)`.
- Consider per-level residual accounting in controller to avoid overestimation with overlapping bands.
- Add approximate byte-size tracking for sampler cache and expose metrics via `monitoring`.
- Optional: bloom filter dedup by `layout_hash` per partition; add blob version header for future-proofing.
```

```markdown
# Curriculum Bank Redesign — Review

## Context
This review consolidates two rounds of notes after implementing the bank-first curriculum generation, manifest v1.1, stratified sampling, and the controller.

## Strengths
- Offline, bank-first workflow eliminates online solving during training; throughput-friendly.
- Manifest histograms enable controller stopping criteria and quick coverage introspection.
- Robust replay via `layout_blob` reduces risk from generator drift; seed+hash remains as fallback.
- Stratified sampling with leftover reallocation smooths distributions across optimal-length bands.
- Clear CLI to drive controller and reproducible generation runs.

## Concerns and Risks
- Manifest fidelity: histogram counts can drift if partitions are externally edited or duplicated; no built-in verification yet.
- Coverage estimation: controller sums raw histogram counts per level; overlapping bands/specs can be double-counted, leading to premature stop.
- Depth override: using a global max depth for all specs within a loop can over-solve easy specs; prefer per-spec depth limits.
- Naming clarity: some CLI paths still mix curriculum constraints with solver cutoffs (`max_depth`). Continue migrating to `solver_max_depth_override` in generation paths.
- Sampler cache memory: LRU eviction is bounded by entries but not memory footprint; large filtered DataFrames can accumulate.
- Layout blob versioning: no version header; if we later change schema or decide to make target-agnostic blobs, need explicit versioning.
- Tests: controller behavior not fully validated (stop conditions, convergence under budget, overlap handling, and residual accounting).

## Decisions / Policies
- Training path must set `allow_fallback=False`; on sampling failure, error out with coverage diagnostics.
- Default solver BFS; allow A* variants via CLI; store solver fingerprints in `SolverKey`.
- Dedup strategy: prefer layout-level dedup when we eventually enforce it to allow multi-round use of layouts.

## Recommendations (Actionable)
1. Add `verify_manifest` tool
   - Recompute histograms from Parquet; detect duplicates by `board_hash`; optionally repair manifest; CI gate for demo bank.
2. Improve controller accounting
   - Compute per-spec contributions to levels; avoid double-counting; adopt per-spec `solver_max_depth_override`.
3. Strengthen CLI separation
   - When `--use_controller` is set, do not apply curriculum filters during generation; keep solver overrides only.
4. Sampler metrics & memory
   - Track approximate cached bytes/rows; expose via `monitoring/hub.py`; add a hard cap.
5. Blob versioning
   - Prepend magic + version to `layout_blob`; add round-trip tests and clear mismatch errors.
6. Tests
   - Add e2e tests for controller convergence, budget respect, and overlap cases; add round-trip hash tests for blobs; assert observation shapes and constants.

## Evidence (from code/tests)
- `PuzzleBank.add_puzzles` updates histograms per `(ol, rm)` pair; controller reads from manifest.
- `BankSampler.sample_puzzles_stratified` implements allocation + leftover reallocation; plan exported via `get_last_stratified_plan`.
- `PuzzleGenerator` records `nodes_expanded` and timing; stores `layout_blob` and hashes; RGB rendering fixes landed.

## Follow-ups
- Implement verification utility and wire it into dev workflow.
- Update `generate_curriculum_bank.py` help text to clearly warn against mixing curriculum filters with controller runs.
- Provide a tiny seeded demo bank for CI and documentation examples.
```

```markdown
# Documentation
This folder contains detailed specifications for (in `ComponentSpecifications`) and a summary of the progress made (in `ProgressNotes`) for various components in the Ricochet Robots (RR) Reinforcement Learning (RL) project.


## Progress Notes Overview
The folder ProgressNotes now organizes progress by concrete tasks, each with an implementation summary. Reviews are added separately as `<TASK>_Review.md` files.

### Task Index
- `Environment_Implementation.md`
- `Solver_Implementation.md`
- `PuzzleBank_Implementation.md`
- `TrainingFramework_Implementation.md`
- `Profiling_Implementation.md`
- `CurriculumBank_Implementation.md`
- `CurriculumBank_Review.md`

### Legacy Notes (kept for context)
- `decisions_and_rationale.md`
- `next_steps.md`


## Component Specifications Overview
Contains the detailed specs and descriptions for individual components of the RR RL plan
### Component index
 - `drc_implementation_instructions.md`: Deep Repeating Convolutional networks, a type of model used which aims to help agents to plan
 - `resnet_baseline_instructions.md`: Residual networks, a type of model which can be used for a pretty capable agent which acts as a good baseline against which we can measure agent performance.
 - `WallSpecifications.md`: Specifications for the wall layout of ricochet robots puzzles.

```

```markdown
# Ricochet Robots RL Project

This repository trains RL agents for Ricochet Robots and analyzes how they plan using mechanistic interpretability.

## Repository Map
### Documentation
- `WorkInProgress.md`: The current task on which we are working.
- `README.md`: This file
- `ResearchPlan.md`: Original research plan
- `docs/`: Documentation folder
  - `docs/ProgressNotes/`: Progress notes (task-based)
  - `docs/ComponentSpecifications/`: Specifications (component-based)
### Code
- `train_agent.py`: Script to run training
- `generate_curriculum_bank.py`: Script to generate the puzzle bank for use during training 
- `env/`: Code to create and set up the environments, including curriculum learning environment wrappers and solvers
  - `env/visuals`: Visualisation helpers for rendering nice, human-readable demonstrations
- `models/`: Code to define custom models and policies
- `profiling/`: Profiling tools for checking memory/computational cost of different components
- `monitoring/`: Monitoring tools for logging performance during training runs
- `tests/`: Pytest-based unit and integration tests
- `curriculum_config_example*.json`: Bank curriculum configs for generating and reading from puzzle bank(s)
- `initial_visualisation_cells.py`, `initial_testing_cells.py`: Visualisation notebooks for quick testing


## Current status

This section gives a high-level overview of the current status of the various stages of the project. The steps are laid out in rough chronological order, but they need not be executed in exactly this order.
More details on any given step, if they are present, will be in `docs/ProgressNotes`.
Recall that the task we're currently working on, if any, can be found in `WorkInProgress.md`.

1) Environment & Generator (complete)
   - Canonical edge-wall mechanics, sliding dynamics, Gymnasium API; deterministic seeding; ASCII/RGB rendering; optional no-op action.

2) Solver & Bank Precomputation (complete)
   - BFS/A* solvers for optimal lengths; offline puzzle bank with metadata (optimal_length, robots_moved, etc.); fast curriculum sampling without online solving.

3) Training Framework (complete)
   - SB3 PPO with vectorized envs, CLI, logging, checkpoints; policies: MLP (symbolic/tiny), SmallCNN (image), ConvLSTM (DRC-lite prototype).

4) Curriculum Training & Evaluation (in progress)
   - Run staged curricula on fixed-size RGB observations; evaluate success rate and optimality gap vs solver on held-out sets; produce rollouts/videos.

5) Mechanistic Interpretability Suite (planned)
   - Activation capture, linear probes, saliency/attribution, feature visualization, activation patching; summarize findings on plan representations and causal features.

6) Profiling & Monitoring (complete)
   - Lightweight profiler across env rendering/model forward; monitoring hooks and TensorBoard metrics to track throughput and resource use.

7) Documentation & Repo Hygiene (in progress)
   - Consolidate progress notes under docs, keep `WorkInProgress.md` current, and ensure README examples and references stay aligned with code.


## Installation

If using the provided virtualenv:

```bash
source rlenv/bin/activate
pip install -r requirements.txt
```

## Quick Start

### Training

Train a PPO agent on milestone environments:

```bash
# v0: Single-move task (4x4 grid) to check that training works
python train_agent.py --env-mode v0 --timesteps 2000 --n-envs 4

# v1: Four-direction task (5x5 grids) to check that slightly more difficult training works
python train_agent.py --env-mode v1 --timesteps 6000 --n-envs 4

# Random environment (8x8 grid)
python train_agent.py --env-mode random --timesteps 100000 --n-envs 8

# Curriculum environment (bank-based; image observation; small-cnn)
python train_agent.py --curriculum --use_bank_curriculum --timesteps 10000 --n-envs 4 --obs-mode image --small-cnn
```

Notes:
- Bank curriculum disables online fallback by default; ensure your bank has coverage for the requested levels.

### Model Architectures

Choose different policy architectures:

```bash
# Default MLP for small grids
python train_agent.py --env-mode v0 --obs-mode image

# Custom small CNN for larger grids
python train_agent.py --env-mode random --small-cnn --obs-mode image

# ConvLSTM (DRC-lite) for recurrent planning
python train_agent.py --env-mode random --convlstm --obs-mode image --lstm-layers 2 --lstm-repeats 1

# Symbolic observations with MLP
python train_agent.py --env-mode random --obs-mode symbolic
```

### Evaluation

Evaluate a trained model:

```bash
python evaluate_agent.py --model-path checkpoints/ppo_model.zip --env-mode v0 --episodes 50
```

### Bank Precomputation (band-first controller)

Generate the curriculum bank using the band-first controller. It solves broadly and stops when each level has enough available puzzles according to bank histograms.

```bash
# Use controller to target 1000 available puzzles per level (approx.)
python generate_curriculum_bank.py \
  --use_controller \
  --target_per_level 1000 \
  --max_puzzles_global 200000 \
  --chunk_per_spec 200 \
  --bank_dir ./puzzle_bank
```

Notes:
- The controller relies on manifest histograms and can be re-run to incrementally top-up without duplicating layouts.
- Default solver is BFS; adjust with `--solver`, `--max_depth`, and `--max_nodes` if needed.

## Environment Options

- `--env-mode`: `random`, `v0`, `v1` (milestone layouts)
- `--height`, `--width`: Grid dimensions (default 8x8)
- `--num-robots`: Number of robots (default 2)
- `--include-noop`: Enable no-op actions for "thinking"
- `--obs-mode`: `image` or `symbolic` observations
- `--ensure-solvable`: Only generate solvable puzzles (slower)

## Training Options

- `--timesteps`: Total training steps
- `--n-envs`: Number of parallel environments
- `--lr`: Learning rate (default 3e-4)
- `--n-steps`: Rollout length (default 128)
- `--batch-size`: Minibatch size (default 256)

## Logging and Checkpoints

- `--log-dir`: TensorBoard log directory
- `--save-path`: Model save path
- `--save-freq`: Checkpoint frequency (timesteps)
- `--eval-freq`: Evaluation frequency (timesteps)

## Testing

Run the test suite:

```bash
pytest tests/ -v
```

## Visualisation

Two offline renderers are available:

- Matplotlib-style raster renderer: `env/visuals/mpl_renderer.py`
- Plotly renderer (PNG export via kaleido): `env/visuals/plotly_renderer.py`

Use `initial_visualisation_cells.py` (with `#%%` cells) to:
- Load the bank curriculum from `puzzle_bank` and `curriculum_config_example2.json`
- Load a checkpointed PPO model
- Record and preview episodes at a few curriculum levels with both renderers

## Architecture Details

### SmallCNN
- 3x3 conv layers with padding=1 (safe for small grids)
- Global average pooling
- Suitable for 4x4+ grids

### ConvLSTM (DRC-lite)
- Convolutional LSTM layers for spatial planning
- Configurable layers and repeats per timestep
- Designed for recurrent planning behavior

### Milestone Environments
- **v0**: 4x4 grid, single RIGHT move to goal
- **v1**: 5x5 grids, one move in each of four directions

```

- I read `WorkInProgress.md`, extracted the implemented changes and reviews, and created concise docs and README updates. Now I’ll summarize what I added.

### What I added/updated
- Added `docs/ProgressNotes/CurriculumBank_Implementation.md` documenting:
  - Bank-first curriculum redesign, manifest v1.1 histograms, `layout_blob` replay, stratified sampling, and the band-first controller.
  - Key code entry points and CLI quickstart.
  - Migration, schema notes, behavioral changes, and future follow-ups.

- Added `docs/ProgressNotes/CurriculumBank_Review.md` consolidating Review #1 and #2:
  - Strengths, risks (manifest fidelity, coverage double-counting, depth override strategy, sampler memory), decisions, and actionable recommendations (verify tool, controller accounting, CLI separation, sampler metrics, blob versioning, tests).

- Updated `docs/README.md`:
  - Included new `CurriculumBank_Implementation.md` and `CurriculumBank_Review.md` in the ProgressNotes index.

- Updated top-level `README.md`:
  - Emphasized bank-first workflow and added controller usage section.
  - Fixed minor docs list placeholder and clarified that bank training disables online fallback by default.

All tasks are checked off.

- Summary:
  - Created implementation and review notes for the curriculum bank redesign.
  - Updated `docs/README.md` and project `README.md` to reflect the new bank-first pipeline and controller usage.

