# Implementing feedback for research proposal step 2
_Exported on 15/09/2025 at 17:05:08 BST from Cursor (1.4.5)_

---

**User**

You are a research engineer, working as part of a team to implement a research proposal. Please take the time to read the ResearchPlan.md, focussing in particular on step 2. Your colleagues have already implemented and reviewed steps 1.1 - 1.4 of the plan, which have been checked and are deemed completed, so you do not need to worry about this. You can read the ProgressNotes left by your colleagues, and read the code they have written, in order to understand what you are working with.
Now, another colleague has implemented step 2 of the plan, detailed in ResearchPlan.md, and we have been given some feedback on it.
Your job will be to go through the feedback that I provide to you, and implement it. Most of the feedback will refer to things which are missing from the original specificaton/suggestions in ResearchPlan.md, so you should refer to this (and also do as much thinking as you need yourself) in order to implement the changes. Make sure that you implement the changes in line with the long term goals of the proposal, taking care to ensure that code is flexible and will work for the desired usecases later down the line.
The first piece of feedback for implementation is attached. You are not to write any code yet, but instead to think hard about different ways to implement this feature. You should lay out the different options, and their pros and cons. We will then discuss and implement afterwards

---

**Cursor**

I'll scan the plan and the current training/evaluation code to understand what's implemented and what’s missing for monitoring and evaluation, then outline implementation options with pros/cons.

Here are concrete options to address the feedback gaps in monitoring and evaluation (success rate, optimality gap, fixed-set evals, trajectory analysis, and no-op usage), tailored to our current SB3+PPO setup and Step 2/4 goals.

### Option A: SB3-native callbacks + light env wrappers (low complexity, fast to add)
- What:
  - Extend a single “TrainingMetricsCallback” that:
    - Logs per-episode success via `info["is_success"]`, rolling success rate window, episode length.
    - Logs action histogram and explicit no-op usage rate and average no-op streak length.
  - Add a lightweight `TrajectoryRecorder` wrapper that, every N timesteps, records K full episodes (obs hashes, actions, infos), saves to JSON/npz; optional ASCII render snapshots.
  - Keep SB3 `EvalCallback`, but pin seeds to a small fixed set for “fixed test set” tracking (deterministic eval).
- Pros:
  - Minimal changes; integrates with our existing `train_agent.py` callbacks and TensorBoard logger.
  - Near-zero runtime overhead during training except periodic recordings.
  - Immediate coverage of “success rate,” “trajectory analysis,” and “no-op usage.”
- Cons:
  - No optimality gap yet (solver not integrated).
  - Fixed-set eval limited to success/length; not solver-aware.
  - Trajectories text-heavy unless we later add image/gif rendering.

### Option B: Add solver-aware evaluator with caching (addresses optimality gap robustly)
- What:
  - Create a `SolverService` that:
    - Computes optimal move counts for a fixed test set of layouts at startup, with depth/node caps, and saves a cache file keyed by layout signature.
    - Exposes a function to annotate eval results with `optimal_length`, `agent_length`, and `optimality_gap`.
  - Replace or augment `EvalCallback` with a custom `OptimalityEvalCallback`:
    - Runs deterministic rollouts on the fixed test set every M timesteps.
    - Loads cached optimal lengths; if missing, computes once (respects time budget) then caches.
    - Logs success rate, mean/median optimality gap, distribution over difficulties, and correlation between gaps and no-op usage.
- Pros:
  - Directly fulfills “optimality gap measurement” with controlled cost (cache).
  - Produces high-signal, plan-aligned metrics (gap vs. difficulty; success with near-optimal solutions).
  - Clean separation: training uses random envs; evaluation runs on curated fixed set.
- Cons:
  - Requires careful layout serialization and solver determinism to avoid cache misses.
  - Initial cache build can be slow; needs time limits and a “skip-unsolved” policy.
  - Slight engineering overhead for the evaluator.

### Option C: Rich logging stack (TensorBoard+custom scalars/images or W&B) + visuals (medium complexity)
- What:
  - Keep Option B, plus:
    - Log per-eval confusion-style charts: histogram of optimality gaps, success by difficulty bin.
    - Log action distributions and no-op rates over time; plot early-episode no-op pacing vs. difficulty.
    - Add episodic renders: generate board images before/after each action for a subset of eval episodes; log as images or short gifs.
  - Optionally integrate Weights & Biases for dashboards; otherwise, stay with TensorBoard scalars and images.
- Pros:
  - High-quality visibility; easy monitoring and comparisons across runs.
  - Qualitative behavior monitoring satisfied with concrete visuals.
- Cons:
  - Rendering adds overhead; must throttle frequency and number of episodes.
  - Extra dependencies (if W&B); otherwise, TensorBoard images suffice.

### Option D: Offline evaluation suite per checkpoint (no training overhead, maximal depth)
- What:
  - After each checkpoint (or nightly), run a separate `evaluate_agent.py --suite fixed_testset.json --solver-cache …` job:
    - Uses more generous solver limits than online evaluation.
    - Produces a standalone report: success, length, optimality gaps, trajectories, and qualitative picks.
  - Store results for longitudinal tracking across checkpoints.
- Pros:
  - Zero impact on training throughput.
  - Allows deeper solver runs; better ground truth for hard cases.
  - Repeatable artifacts for later analysis and paper figures.
- Cons:
  - Less real-time feedback; delayed signal.
  - Requires orchestration to trigger after checkpoints.

### Cross-cutting design choices

- Fixed test set specification:
  - Define `fixed_testset.json` with serialized layouts and IDs, plus precomputed `optimal_length` if available.
  - For curriculum alignment, include multiple difficulty bins (by optimal length).
- Solver performance and caching:
  - Use strong hashing of layout state for cache keys.
  - Enforce per-instance time/depth caps; mark unsolved with `optimal_length=None` and exclude from gap stats.
  - Warm the cache offline before long runs for common test sets.
- No-op usage monitoring:
  - Track fraction of no-op actions, mean/max consecutive no-op streak, position in episode where no-ops occur, and relationship to success/gap.
  - Record per-difficulty-bin statistics in both training (rolling) and evaluation (fixed).
- Trajectory analysis output:
  - Save compact episode traces: `[state_hash, action, is_noop, done, info]`, plus optional rendered PNGs at key timesteps for a few episodes.
  - Keep storage bounded: sample 5–10 trajectories per eval round; rotate older logs.
- Configurability:
  - Expose CLI flags: `--eval-fixedset`, `--optimality-eval-freq`, `--traj-record-freq`, `--traj-record-episodes`, `--render-eval-snapshots`, `--solver-cache`, `--solver-timeout-ms`.
  - Keep training/eval separation: training callback never calls solver; only the evaluator uses the cache.

### Recommended path
- Near-term (fastest win): Option A + pin `EvalCallback` seeds for a fixed test set. Adds success rate, no-op, trajectories immediately.
- Next (core gap): Add Option B’s solver-aware `OptimalityEvalCallback` with caching to deliver optimality gap metrics.
- Nice-to-have: Layer in Option C’s visuals for qualitative monitoring.
- For heavy-duty reports: Option D offline suite triggered per checkpoint or on demand.

Pros/cons summary
- Option A: +quick, +low risk, −no optimality.
- Option B: +meets optimality requirement, +high insight, −medium complexity, −needs cache discipline.
- Option C: +rich observability, −extra overhead/deps.
- Option D: +zero train overhead, +deep evals, −delayed feedback.

I can proceed with Option A+B as the baseline, then add selective C (images on a handful of eval episodes), and leave D as a separate scriptable job.

---

**User**

I would like you to implement a long-term solution, which is flexible and allows us to add more things to monitor as and when we come up with them, and work with different curricula both at a per-level and an overall level. It should be configurable in the training config whether we want to use tensorboard and/or wandb (by default for not we'll have tensorboard on, wandb off).

---

**Cursor**

Let’s design a durable monitoring/evaluation system you can extend over time, works across curricula (overall and per-level), and supports TensorBoard and/or W&B via config toggles.

### High-level architecture
- The core is a pluggable “Monitoring Hub” composed of:
  - • Logger backends: `TensorBoardLogger`, `WandbLogger` (both implement a common `LoggerBackend` with record_scalar, record_histogram, record_image, record_text, flush).
  - • Metric collectors: small, focused units that compute and emit metrics from streaming data or scheduled evals. Examples:
    - `EpisodeStatsCollector` (success rate, episode length, returns)
    - `ActionUsageCollector` (action histogram, explicit no-op rate and streaks)
    - `CurriculumProgressCollector` (per-level and global stats)
    - `TrajectoryRecorder` (downsampled episodes, minimal state hashes, optional renders)
    - `FixedSetEvaluator` (deterministic eval on fixed test set)
    - `OptimalityGapEvaluator` (uses solver cache; gap distribution and gap-vs-difficulty)
  - • Context providers:
    - `TrainingContext` (timesteps, updates, rollout stats)
    - `CurriculumContext` (current level id/name, difficulty bounds, global progress)
  - • Schedulers:
    - Allow each collector/evaluator to declare cadence: every N timesteps, every K episodes, or on level-change hooks.

### Key design choices for flexibility
- • Pluggable registry: `MonitoringHub` holds a list of `Collectors` and a list of `Backends`. Add/remove collectors without touching others.
- • Curriculum awareness:
  - Expose `CurriculumContext` to collectors so they can:
    - Tag metrics with `level_id`, `level_name`, and log “overall” rollups.
    - React to level-change events to emit summaries per level.
- • Eval separation:
  - Training loop never calls the solver.
  - `FixedSetEvaluator`/`OptimalityGapEvaluator` run in evaluation phases only, with caching to cap cost.
- • Config-driven:
  - Enable/disable any collector via config.
  - Set cadences/limits (e.g., eval every 10k steps, record 5 trajectories per eval).
  - Choose backends (TensorBoard on by default, W&B off).
- • Storage and performance safety:
  - All heavy tasks (solver and rendering) are throttled and bounded; caches avoid recomputation.
  - Trajectory recording uses sampling and size caps.

### Components and responsibilities
- `MonitoringHub`
  - Orchestrates collectors and backends; routes `record_*` calls to enabled backends.
  - Provides lifecycle hooks: on_step, on_rollout_end, on_eval_start, on_eval_end, on_level_change, on_train_end.
- Logger backends
  - `TensorBoardLogger`: wraps SB3’s logger or `tensorboardX`; auto-convert metric namespaces like `train/success_rate`, `eval/optimality_gap/mean`, `curriculum/level`.
  - `WandbLogger` (optional): initializes run from config; mirrors all metrics; logs artifacts (e.g., eval reports, sample trajectories); disabled by default.
- Collectors (examples)
  - `EpisodeStatsCollector`:
    - Reads `info["is_success"]`, episode returns/lengths, and maintains rolling windows (configurable window size).
  - `ActionUsageCollector`:
    - Tracks per-action histograms and explicit no-op usage: overall fraction, mean/max streak, and early-episode no-op pacing.
  - `CurriculumProgressCollector`:
    - Logs per-level episode counts, per-level success rate window, advancement events, total episodes; emits level summaries on transitions.
  - `TrajectoryRecorder`:
    - On schedule, records a small number of full episodes (obs hash, action, reward, done, info; optional rendered frames). Saves compact `.npz` and logs a short human-readable summary.
  - `FixedSetEvaluator`:
    - Runs deterministic rollouts on `fixed_testset.json`; logs eval success rate, avg solution length, per-difficulty stats; can export a concise `eval_report.json`.
  - `OptimalityGapEvaluator`:
    - Loads `SolverCache` precomputed optimal lengths; if missing and within time budget, computes once and caches; logs gap distributions and correlation with no-op rates; emits per-difficulty-bin metrics.
- `SolverCache`
  - Keyed by a canonical layout signature; stores `optimal_length`, compute metadata, and failure reasons; persisted to disk for reuse across runs.

### Namespacing/metrics
- Training time series:
  - `train/success_rate`, `train/episode_length/mean`, `train/return/mean`
  - `train/actions/noop_rate`, `train/actions/noop_streak_max`, `train/actions/histogram`
  - `curriculum/level`, `curriculum/level_success_rate`, `curriculum/episodes_at_level`, `curriculum/level_advancement`
- Eval (fixed set):
  - `eval/success_rate`, `eval/episode_length/mean`
  - `eval/optimality_gap/mean|median|p90`
  - `eval/optimality_gap/by_difficulty/<bin>`
  - `eval/actions/noop_rate`
  - Artifacts: `eval_report.json`, `sample_trajectories.npz`, optional images/gifs

### Configuration (example)
```json
{
  "logging": {
    "tensorboard": { "enabled": true, "log_dir": "runs/ppo" },
    "wandb": { "enabled": false, "project": "ricochet-robots", "entity": "team", "run_name": null, "tags": ["ppo", "drc"] }
  },
  "monitoring": {
    "collectors": {
      "episode_stats": { "enabled": true, "window_size": 200 },
      "action_usage": { "enabled": true, "track_noop": true },
      "curriculum_progress": { "enabled": true },
      "trajectory_recorder": { "enabled": true, "freq_timesteps": 50000, "episodes_per_dump": 5, "render_images": false }
    },
    "evaluators": {
      "fixed_set": { "enabled": true, "freq_timesteps": 10000, "episodes": 50, "fixedset_path": "eval/fixed_testset.json" },
      "optimality_gap": {
        "enabled": true,
        "freq_timesteps": 50000,
        "solver_cache_path": "eval/solver_cache.json",
        "solver_limits": { "max_depth": 60, "max_nodes": 50000, "timeout_ms": 200 },
        "skip_unsolved": true
      }
    }
  }
}
```

### How it integrates with current code
- `train_agent.py`
  - Parse a monitoring config block (inline CLI overrides allowed).
  - Initialize `MonitoringHub` with selected backends.
  - Replace/augment current callbacks with a single `MonitoringCallback` that:
    - Feeds training events and curriculum context into the hub.
    - Triggers evaluators on their schedule (using dedicated eval envs).
  - Keep SB3’s `CheckpointCallback`; when a checkpoint is saved, the hub can log a checkpoint tag.
- `evaluate_agent.py`
  - Gain an “offline suite” mode to run `FixedSetEvaluator` + `OptimalityGapEvaluator` on any checkpoint, producing a self-contained report (same backends, plus artifacts saved to disk).
- `env/curriculum.py`
  - Ensure `CurriculumManager` exposes a simple interface for the hub to query current level metadata and receive level-change events.

### Extensibility examples
- Add a `ValueEstimationCollector` to track value bias/variance.
- Add a `StallLoopDetector` to flag potential looping behaviors in trajectories.
- Add `GeneralizationEvaluator` for OOD test suites without touching existing collectors.

### Defaults you requested
- TensorBoard: on by default.
- W&B: off by default; enable via config or CLI.
- Curriculum-aware logging: on if curriculum is enabled; otherwise, collectors log only overall metrics.

If this direction looks good, I’ll create the `MonitoringHub`, implement the first set of collectors and the solver-aware evaluators with caching, wire config toggles into `train_agent.py`, and keep rendering optional and throttled.

```json
{
  "logging": {
    "tensorboard": { "enabled": true, "log_dir": "runs/ppo" },
    "wandb": { "enabled": false, "project": "ricochet-robots", "entity": "team", "run_name": null, "tags": ["ppo", "drc"] }
  },
  "monitoring": {
    "collectors": {
      "episode_stats": { "enabled": true, "window_size": 200 },
      "action_usage": { "enabled": true, "track_noop": true },
      "curriculum_progress": { "enabled": true },
      "trajectory_recorder": { "enabled": true, "freq_timesteps": 50000, "episodes_per_dump": 5, "render_images": false }
    },
    "evaluators": {
      "fixed_set": { "enabled": true, "freq_timesteps": 10000, "episodes": 50, "fixedset_path": "eval/fixed_testset.json" },
      "optimality_gap": {
        "enabled": true,
        "freq_timesteps": 50000,
        "solver_cache_path": "eval/solver_cache.json",
        "solver_limits": { "max_depth": 60, "max_nodes": 50000, "timeout_ms": 200 },
        "skip_unsolved": true
      }
    }
  }
}
```

---

**User**

Yes please go ahead, and make it all work with what we have so far

---

**Cursor**

I'll add a modular monitoring system with a hub, backends (TensorBoard and optional W&B), collectors, and evaluators, then wire it into the training script. I'll start by scanning the environment and solver modules to hook success flags and solver usage correctly, and check requirements for optional W&B.

I'm going to add a modular monitoring system (hub, backends, collectors, evaluators) and wire it into `train_agent.py`, with TensorBoard on by default and optional W&B. Then I'll implement a custom evaluation callback that can compute success and optimality gap on a fixed test set with a solver cache.

```pip-requirements
numpy>=1.26
gymnasium>=0.29
pytest>=8.0
jupyter>=1.0
matplotlib>=3.7
torch>=2.3
stable-baselines3>=2.3.2
tensorboard>=2.15.0
# MPS support for M1/M2 Macs
torch-audio>=2.3
wandb>=0.17
```

I'll create a modular monitoring package (hub, backends, collectors, evaluators), wire it into `train_agent.py`, add CLI/config toggles for TensorBoard and optional W&B, and implement a fixed-set evaluator with solver-backed optimality gap and no-op stats. Then I'll update the TODOs accordingly.

```python
from .hub import MonitoringHub, MonitoringConfig
from .backends import TensorBoardBackend, WandbBackend
from .collectors import EpisodeStatsCollector, ActionUsageCollector, CurriculumProgressCollector
from .evaluators import FixedSetEvaluator, OptimalityGapEvaluator, SolverCache

__all__ = [
    "MonitoringHub",
    "MonitoringConfig",
    "TensorBoardBackend",
    "WandbBackend",
    "EpisodeStatsCollector",
    "ActionUsageCollector",
    "CurriculumProgressCollector",
    "FixedSetEvaluator",
    "OptimalityGapEvaluator",
    "SolverCache",
]


```

```python
from __future__ import annotations

from dataclasses import dataclass, field
from typing import Any, Dict, List, Optional, Protocol


class LoggerBackend(Protocol):
    def record_scalar(self, key: str, value: float, step: Optional[int] = None) -> None: ...
    def record_histogram(self, key: str, values: List[float], step: Optional[int] = None) -> None: ...
    def record_text(self, key: str, text: str, step: Optional[int] = None) -> None: ...
    def record_image(self, key: str, image: Any, step: Optional[int] = None) -> None: ...
    def flush(self) -> None: ...


class Collector(Protocol):
    def on_step(self, hub: "MonitoringHub", context: Dict[str, Any]) -> None: ...
    def on_rollout_end(self, hub: "MonitoringHub", context: Dict[str, Any]) -> None: ...
    def on_level_change(self, hub: "MonitoringHub", context: Dict[str, Any]) -> None: ...
    def on_eval(self, hub: "MonitoringHub", context: Dict[str, Any]) -> None: ...
    def close(self) -> None: ...


@dataclass
class MonitoringConfig:
    enable_tensorboard: bool = True
    enable_wandb: bool = False
    tensorboard_log_dir: str = "runs/ppo"
    wandb_project: Optional[str] = None
    wandb_entity: Optional[str] = None
    wandb_run_name: Optional[str] = None
    wandb_tags: Optional[List[str]] = None
    collectors: Dict[str, Dict[str, Any]] = field(default_factory=dict)
    evaluators: Dict[str, Dict[str, Any]] = field(default_factory=dict)


class MonitoringHub:
    def __init__(self, *, backends: List[LoggerBackend], collectors: List[Collector]) -> None:
        self._backends = backends
        self._collectors = collectors

    # Logging API
    def log_scalar(self, key: str, value: float, step: Optional[int] = None) -> None:
        for b in self._backends:
            b.record_scalar(key, value, step)

    def log_histogram(self, key: str, values: List[float], step: Optional[int] = None) -> None:
        for b in self._backends:
            b.record_histogram(key, values, step)

    def log_text(self, key: str, text: str, step: Optional[int] = None) -> None:
        for b in self._backends:
            b.record_text(key, text, step)

    def log_image(self, key: str, image: Any, step: Optional[int] = None) -> None:
        for b in self._backends:
            b.record_image(key, image, step)

    def flush(self) -> None:
        for b in self._backends:
            b.flush()

    # Event routing
    def on_step(self, context: Dict[str, Any]) -> None:
        for c in self._collectors:
            c.on_step(self, context)

    def on_rollout_end(self, context: Dict[str, Any]) -> None:
        for c in self._collectors:
            c.on_rollout_end(self, context)

    def on_level_change(self, context: Dict[str, Any]) -> None:
        for c in self._collectors:
            c.on_level_change(self, context)

    def on_eval(self, context: Dict[str, Any]) -> None:
        for c in self._collectors:
            c.on_eval(self, context)

    def close(self) -> None:
        for c in self._collectors:
            c.close()
        for b in self._backends:
            b.flush()


```

```python
from __future__ import annotations

from typing import Any, List, Optional


class TensorBoardBackend:
    def __init__(self, log_dir: str) -> None:
        try:
            from torch.utils.tensorboard import SummaryWriter  # type: ignore
        except Exception as e:  # pragma: no cover - dependency optional
            raise ImportError("TensorBoard not available") from e
        self._writer = SummaryWriter(log_dir=log_dir)

    def record_scalar(self, key: str, value: float, step: Optional[int] = None) -> None:
        self._writer.add_scalar(key, value, global_step=step)

    def record_histogram(self, key: str, values: List[float], step: Optional[int] = None) -> None:
        import numpy as np
        self._writer.add_histogram(key, np.asarray(values), global_step=step)

    def record_text(self, key: str, text: str, step: Optional[int] = None) -> None:
        self._writer.add_text(key, text, global_step=step)

    def record_image(self, key: str, image: Any, step: Optional[int] = None) -> None:
        import numpy as np
        arr = image
        if hasattr(image, "numpy"):
            arr = image.numpy()
        if isinstance(arr, list):
            import numpy as np
            arr = np.asarray(arr)
        # Expect HxWxC uint8 or CxHxW
        self._writer.add_image(key, arr, global_step=step, dataformats='HWC' if arr.ndim == 3 and arr.shape[-1] in (1,3,4) else 'CHW')

    def flush(self) -> None:
        self._writer.flush()


class WandbBackend:
    def __init__(self, project: str, entity: Optional[str] = None, run_name: Optional[str] = None, tags: Optional[List[str]] = None, config: Optional[dict] = None) -> None:
        try:
            import wandb  # type: ignore
        except Exception as e:  # pragma: no cover - optional dep
            raise ImportError("wandb not available") from e
        self._wandb = wandb
        self._wandb.init(project=project, entity=entity, name=run_name, tags=tags, config=config, reinit=True)

    def record_scalar(self, key: str, value: float, step: Optional[int] = None) -> None:
        self._wandb.log({key: value}, step=step)

    def record_histogram(self, key: str, values: List[float], step: Optional[int] = None) -> None:
        self._wandb.log({key: self._wandb.Histogram(values)}, step=step)

    def record_text(self, key: str, text: str, step: Optional[int] = None) -> None:
        self._wandb.log({key: text}, step=step)

    def record_image(self, key: str, image: Any, step: Optional[int] = None) -> None:
        self._wandb.log({key: self._wandb.Image(image)}, step=step)

    def flush(self) -> None:
        # wandb flushes on log
        return None


```

```python
from __future__ import annotations

from collections import deque
from typing import Any, Deque, Dict, List, Optional

from .hub import MonitoringHub


class BaseCollector:
    def on_step(self, hub: MonitoringHub, context: Dict[str, Any]) -> None:
        return None

    def on_rollout_end(self, hub: MonitoringHub, context: Dict[str, Any]) -> None:
        return None

    def on_level_change(self, hub: MonitoringHub, context: Dict[str, Any]) -> None:
        return None

    def on_eval(self, hub: MonitoringHub, context: Dict[str, Any]) -> None:
        return None

    def close(self) -> None:
        return None


class EpisodeStatsCollector(BaseCollector):
    def __init__(self, window_size: int = 200) -> None:
        self.window_size = int(window_size)
        self.success_window: Deque[int] = deque(maxlen=self.window_size)
        self.length_window: Deque[int] = deque(maxlen=self.window_size)
        self.return_window: Deque[float] = deque(maxlen=self.window_size)

    def on_rollout_end(self, hub: MonitoringHub, context: Dict[str, Any]) -> None:
        # Expect context to provide lists over finished episodes in this rollout
        successes: List[int] = context.get("rollout_successes", [])
        lengths: List[int] = context.get("rollout_lengths", [])
        returns: List[float] = context.get("rollout_returns", [])
        for s in successes:
            self.success_window.append(int(bool(s)))
        for l in lengths:
            self.length_window.append(int(l))
        for r in returns:
            self.return_window.append(float(r))
        step = context.get("global_step")
        if len(self.success_window) > 0:
            hub.log_scalar("train/success_rate", sum(self.success_window) / len(self.success_window), step)
        if len(self.length_window) > 0:
            hub.log_scalar("train/episode_length/mean", sum(self.length_window) / len(self.length_window), step)
        if len(self.return_window) > 0:
            hub.log_scalar("train/return/mean", sum(self.return_window) / len(self.return_window), step)


class ActionUsageCollector(BaseCollector):
    def __init__(self, track_noop: bool = True) -> None:
        self.track_noop = bool(track_noop)
        self.total_actions = 0
        self.noop_actions = 0
        self.current_noop_streak = 0
        self.max_noop_streak = 0

    def on_step(self, hub: MonitoringHub, context: Dict[str, Any]) -> None:
        action = context.get("last_action")
        is_noop = context.get("last_action_is_noop", False)
        step = context.get("global_step")
        if action is None:
            return None
        self.total_actions += 1
        if self.track_noop:
            if is_noop:
                self.noop_actions += 1
                self.current_noop_streak += 1
                self.max_noop_streak = max(self.max_noop_streak, self.current_noop_streak)
            else:
                self.current_noop_streak = 0
        if self.total_actions % 1000 == 0:
            if self.total_actions > 0 and self.track_noop:
                hub.log_scalar("train/actions/noop_rate", self.noop_actions / max(1, self.total_actions), step)
                hub.log_scalar("train/actions/noop_streak_max", float(self.max_noop_streak), step)


class CurriculumProgressCollector(BaseCollector):
    def __init__(self) -> None:
        self.last_level = None

    def on_level_change(self, hub: MonitoringHub, context: Dict[str, Any]) -> None:
        level_id = context.get("level_id")
        level_name = context.get("level_name")
        step = context.get("global_step")
        if level_id is not None:
            hub.log_scalar("curriculum/level", float(level_id), step)
            if level_name is not None:
                hub.log_text("curriculum/level_name", str(level_name), step)
        stats = context.get("level_stats")
        if isinstance(stats, dict):
            if "success_rate" in stats:
                hub.log_scalar("curriculum/success_rate", float(stats["success_rate"]), step)
            if "episodes_at_level" in stats:
                hub.log_scalar("curriculum/episodes_at_level", float(stats["episodes_at_level"]), step)
            if "total_episodes" in stats:
                hub.log_scalar("curriculum/total_episodes", float(stats["total_episodes"]), step)


```

```python
from __future__ import annotations

import json
from dataclasses import dataclass
from typing import Any, Dict, List, Optional, Tuple

from .hub import MonitoringHub


def _board_signature(layout: Dict[str, Any]) -> str:
    # Serialize fixed layout dict into a deterministic signature
    import hashlib
    payload = json.dumps(layout, sort_keys=True)
    return hashlib.sha256(payload.encode("utf-8")).hexdigest()[:16]


@dataclass
class SolverCache:
    path: str
    data: Dict[str, Dict[str, Any]]

    @classmethod
    def load(cls, path: str) -> "SolverCache":
        try:
            with open(path, "r", encoding="utf-8") as f:
                data = json.load(f)
        except (OSError, json.JSONDecodeError):
            data = {}
        return SolverCache(path=path, data=data)

    def save(self) -> None:
        with open(self.path, "w", encoding="utf-8") as f:
            json.dump(self.data, f)


class FixedSetEvaluator:
    def __init__(self, fixedset: List[Dict[str, Any]], episodes: int = 50) -> None:
        self.fixedset = fixedset
        self.episodes = int(episodes)

    def evaluate(self, hub: MonitoringHub, make_env_fn, model, step: Optional[int] = None) -> Dict[str, Any]:
        import numpy as np
        success = 0
        lengths: List[int] = []
        noop_counts: List[int] = []
        for i in range(min(self.episodes, len(self.fixedset))):
            layout = self.fixedset[i]
            env = make_env_fn(layout)
            obs, info = env.reset()
            done = False
            trunc = False
            steps = 0
            noops = 0
            while not (done or trunc):
                action, _ = model.predict(obs, deterministic=True)
                is_noop = (getattr(env, "_noop_action", None) is not None and int(action) == env._noop_action)
                if is_noop:
                    noops += 1
                obs, _, done, trunc, info = env.step(int(action))
                steps += 1
            if info.get("is_success", False):
                success += 1
            lengths.append(steps)
            noop_counts.append(noops)
            env.close()
        rate = success / max(1, min(self.episodes, len(self.fixedset)))
        hub.log_scalar("eval/success_rate", float(rate), step)
        if lengths:
            hub.log_scalar("eval/episode_length/mean", float(sum(lengths) / len(lengths)), step)
        if noop_counts:
            hub.log_scalar("eval/actions/noop_rate", float(sum(noop_counts) / max(1, sum(lengths))), step)
        return {"success_rate": rate, "lengths": lengths, "noop_counts": noop_counts}


class OptimalityGapEvaluator:
    def __init__(self, fixedset: List[Dict[str, Any]], solver_cache: SolverCache, solver_limits: Dict[str, int], skip_unsolved: bool = True) -> None:
        self.fixedset = fixedset
        self.cache = solver_cache
        self.limits = solver_limits
        self.skip_unsolved = bool(skip_unsolved)

    def _build_env_from_layout(self, layout_dict: Dict[str, Any]):
        from env import RicochetRobotsEnv, FixedLayout
        import numpy as np
        h = int(layout_dict["height"]) ; w = int(layout_dict["width"]) ; tr = int(layout_dict["target_robot"]) ; robots = {int(k): tuple(v) for k, v in layout_dict["robot_positions"].items()}
        h_walls = np.array(layout_dict["h_walls"], dtype=bool)
        v_walls = np.array(layout_dict["v_walls"], dtype=bool)
        goal = tuple(layout_dict["goal_position"])  # type: ignore
        fl = FixedLayout(height=h, width=w, h_walls=h_walls, v_walls=v_walls, robot_positions=robots, goal_position=goal, target_robot=tr)
        return RicochetRobotsEnv(fixed_layout=fl, obs_mode="image", channels_first=True)

    def _ensure_optimal_length(self, layout: Dict[str, Any], sig: str) -> Optional[int]:
        if sig in self.cache.data and "optimal_length" in self.cache.data[sig]:
            return int(self.cache.data[sig]["optimal_length"])
        # Compute
        try:
            from env.solver import solve_bfs
            env = self._build_env_from_layout(layout)
            board = env.get_board()
            actions = solve_bfs(board, max_depth=int(self.limits.get("max_depth", 60)), max_nodes=int(self.limits.get("max_nodes", 50000)))
            env.close()
            if actions is None:
                self.cache.data[sig] = {"optimal_length": None}
                return None
            opt = len(actions)
            self.cache.data[sig] = {"optimal_length": opt}
            self.cache.save()
            return opt
        except Exception:
            return None

    def evaluate(self, hub: MonitoringHub, make_env_fn, model, step: Optional[int] = None) -> Dict[str, Any]:
        gaps: List[int] = []
        considered = 0
        for layout in self.fixedset:
            sig = _board_signature(layout)
            opt = self._ensure_optimal_length(layout, sig)
            if opt is None and self.skip_unsolved:
                continue
            env = make_env_fn(layout)
            obs, info = env.reset()
            done = False
            trunc = False
            steps = 0
            while not (done or trunc):
                action, _ = model.predict(obs, deterministic=True)
                obs, _, done, trunc, info = env.step(int(action))
                steps += 1
            env.close()
            if info.get("is_success", False) and opt is not None:
                gaps.append(int(steps - opt))
            considered += 1
        if gaps:
            import numpy as np
            hub.log_scalar("eval/optimality_gap/mean", float(sum(gaps) / len(gaps)), step)
            hub.log_scalar("eval/optimality_gap/median", float(np.median(gaps)), step)
        hub.log_scalar("eval/optimality_gap/num_evaluated", float(considered), step)
        return {"gaps": gaps, "considered": considered}


```

```python
from __future__ import annotations

import argparse
import os
from typing import Callable
import json
import math
import copy

from env import RicochetRobotsEnv, fixed_layout_v0_one_move, fixed_layouts_v1_four_targets
from env.curriculum import create_curriculum_wrapper, create_curriculum_manager, create_default_curriculum, CurriculumConfig
try:
    from models.policies import SmallCNN  # type: ignore
    from models.convlstm import ConvLSTMFeaturesExtractor  # type: ignore
    from models.recurrent_policy import RecurrentActorCriticPolicy  # type: ignore
except ImportError:
    SmallCNN = None  # type: ignore
    ConvLSTMFeaturesExtractor = None  # type: ignore
    RecurrentActorCriticPolicy = None  # type: ignore


def make_env_factory(args: argparse.Namespace) -> Callable[[], RicochetRobotsEnv]:
    """Return a thunk to create an environment instance based on CLI args."""
    mode = args.env_mode

    if mode == "v0":
        layout = fixed_layout_v0_one_move()
        def _fn():
            return RicochetRobotsEnv(
                height=layout.height,
                width=layout.width,
                num_robots=len(layout.robot_positions),
                fixed_layout=layout,
                include_noop=args.include_noop,
                step_penalty=args.step_penalty,
                goal_reward=args.goal_reward,
                max_steps=args.max_steps,
                obs_mode=args.obs_mode,
                channels_first=True,
            )
        return _fn

    if mode == "v1":
        layouts = fixed_layouts_v1_four_targets()
        # round-robin over layouts to add variety across envs
        counter = {"i": -1}
        def _fn():
            counter["i"] = (counter["i"] + 1) % len(layouts)
            layout = layouts[counter["i"]]
            return RicochetRobotsEnv(
                height=layout.height,
                width=layout.width,
                num_robots=len(layout.robot_positions),
                fixed_layout=layout,
                include_noop=args.include_noop,
                step_penalty=args.step_penalty,
                goal_reward=args.goal_reward,
                max_steps=args.max_steps,
                obs_mode=args.obs_mode,
                channels_first=True,
            )
        return _fn

    # default: random env sampling per episode
    def _fn():
        return RicochetRobotsEnv(
            height=args.height,
            width=args.width,
            num_robots=args.num_robots,
            include_noop=args.include_noop,
            step_penalty=args.step_penalty,
            goal_reward=args.goal_reward,
            noop_penalty=args.noop_penalty,
            max_steps=args.max_steps,
            seed=args.seed,
            ensure_solvable=args.ensure_solvable,
            solver_max_depth=args.solver_max_depth,
            solver_max_nodes=args.solver_max_nodes,
            obs_mode=args.obs_mode,
            channels_first=True,
        )
    return _fn


def make_curriculum_env_factory(args: argparse.Namespace) -> tuple[Callable[[], RicochetRobotsEnv], object]:
    """Return a thunk to create a curriculum environment instance based on CLI args."""
    # Create base environment factory for curriculum
    base_env_factory = make_env_factory(args)
    
    # Create curriculum configuration
    if args.curriculum_config is not None:
        # Load custom curriculum config from file
        with open(args.curriculum_config, 'r', encoding='utf-8') as f:
            config_data = json.load(f)
        curriculum_config = CurriculumConfig(**config_data)
    else:
        # Use default curriculum with CLI overrides
        curriculum_config = create_default_curriculum()
        # Override with CLI arguments
        curriculum_config.success_rate_threshold = args.curriculum_success_threshold
        curriculum_config.min_episodes_per_level = args.curriculum_min_episodes
        curriculum_config.success_rate_window_size = args.curriculum_window_size
        curriculum_config.advancement_check_frequency = args.curriculum_check_freq
    
    # Create shared curriculum manager
    curriculum_manager = create_curriculum_manager(
        curriculum_config=curriculum_config,
        initial_level=args.curriculum_initial_level,
        verbose=args.curriculum_verbose
    )
    
    # Return a function that creates a new curriculum wrapper each time
    def _fn():
        return create_curriculum_wrapper(
            base_env_factory=base_env_factory,
            curriculum_manager=curriculum_manager,
            verbose=args.curriculum_verbose
        )
    
    return _fn, curriculum_manager


def main() -> None:
    parser = argparse.ArgumentParser(description="Train PPO on Ricochet Robots")
    # Env selection
    parser.add_argument("--env-mode", choices=["random", "v0", "v1"], default="random")
    parser.add_argument("--height", type=int, default=8)
    parser.add_argument("--width", type=int, default=8)
    parser.add_argument("--num-robots", type=int, default=2)
    parser.add_argument("--include-noop", action="store_true")
    parser.add_argument("--step-penalty", type=float, default=-0.01)
    parser.add_argument("--goal-reward", type=float, default=1.0)
    parser.add_argument("--noop-penalty", type=float, default=-0.01)
    parser.add_argument("--max-steps", type=int, default=100)
    parser.add_argument("--seed", type=int, default=0)
    parser.add_argument("--ensure-solvable", action="store_true")
    parser.add_argument("--solver-max-depth", type=int, default=30)
    parser.add_argument("--solver-max-nodes", type=int, default=20000)
    parser.add_argument("--obs-mode", choices=["image", "symbolic", "rgb_image"], default="image")
    
    # Profiling options
    parser.add_argument("--enable-profiling", action="store_true", help="Enable detailed profiling of training pipeline")
    parser.add_argument("--profiling-report", type=str, default="profiling_report.json", help="Path to save profiling report")
    parser.add_argument("--profiling-summary-freq", type=int, default=10000, help="Frequency of profiling summary prints (timesteps)")
    
    # Curriculum learning options
    parser.add_argument("--curriculum", action="store_true", help="Enable curriculum learning")
    parser.add_argument("--curriculum-config", type=str, help="Path to custom curriculum configuration JSON file")
    parser.add_argument("--curriculum-initial-level", type=int, default=0, help="Initial curriculum level (0-4)")
    parser.add_argument("--curriculum-verbose", action="store_true", default=True, help="Print curriculum progression messages")
    parser.add_argument("--curriculum-success-threshold", type=float, default=0.8, help="Success rate threshold for curriculum advancement")
    parser.add_argument("--curriculum-min-episodes", type=int, default=100, help="Minimum episodes per curriculum level")
    parser.add_argument("--curriculum-window-size", type=int, default=200, help="Success rate window size for curriculum advancement")
    parser.add_argument("--curriculum-check-freq", type=int, default=50, help="Frequency of curriculum advancement checks (episodes)")

    # Algo
    parser.add_argument("--algo", choices=["ppo"], default="ppo")
    parser.add_argument("--timesteps", type=int, default=100000)
    parser.add_argument("--n-envs", type=int, default=8)
    parser.add_argument("--n-steps", type=int, default=128)
    parser.add_argument("--batch-size", type=int, default=256)
    parser.add_argument("--n-epochs", type=int, default=4)
    parser.add_argument("--lr", type=float, default=3e-4)
    parser.add_argument("--lr-end", type=float, default=0.0, help="Final LR for schedule")
    parser.add_argument("--lr-schedule", choices=["none", "linear", "cosine"], default="none")
    parser.add_argument("--gamma", type=float, default=0.99)
    parser.add_argument("--clip-range", type=float, default=0.2)
    parser.add_argument("--clip-end", type=float, default=0.1, help="Final clip range for schedule")
    parser.add_argument("--clip-schedule", choices=["none", "linear", "cosine"], default="none")
    parser.add_argument("--gae-lambda", type=float, default=0.95)
    parser.add_argument("--vf-coef", type=float, default=0.5)
    parser.add_argument("--max-grad-norm", type=float, default=0.5)
    parser.add_argument("--ent-coef", type=float, default=0.01)
    # Normalize advantage toggles
    parser.add_argument("--normalize-advantage", dest="normalize_advantage", action="store_true")
    parser.add_argument("--no-normalize-advantage", dest="normalize_advantage", action="store_false")
    parser.set_defaults(normalize_advantage=True)
    parser.add_argument("--device", default="auto", help="Device to use for training (auto, cpu, mps, cuda)")

    # Config and profiles
    parser.add_argument("--config", type=str, help="Path to hyperparameters JSON config")
    parser.add_argument("--profile", choices=["none", "research_plan", "quick_debug", "baseline_sb3"], default="none")

    # VecNormalize options
    parser.add_argument("--vecnorm", action="store_true", help="Enable VecNormalize wrapper")
    parser.add_argument("--vecnorm-norm-obs", dest="vecnorm_norm_obs", action="store_true")
    parser.add_argument("--no-vecnorm-norm-obs", dest="vecnorm_norm_obs", action="store_false")
    parser.set_defaults(vecnorm_norm_obs=True)
    parser.add_argument("--vecnorm-norm-reward", dest="vecnorm_norm_reward", action="store_true")
    parser.add_argument("--no-vecnorm-norm-reward", dest="vecnorm_norm_reward", action="store_false")
    parser.set_defaults(vecnorm_norm_reward=False)
    parser.add_argument("--vecnorm-clip-obs", type=float, default=10.0)
    parser.add_argument("--vecnorm-clip-reward", type=float, default=10.0)

    # Logging / checkpoints
    parser.add_argument("--log-dir", default="runs/ppo")
    parser.add_argument("--save-path", default="checkpoints/ppo_model")
    parser.add_argument("--save-freq", type=int, default=50000, help="Timesteps between checkpoints (0=disable)")
    parser.add_argument("--eval-freq", type=int, default=10000, help="Timesteps between evals (0=disable)")
    parser.add_argument("--eval-episodes", type=int, default=20)
    # Monitoring / logging toggles
    parser.add_argument("--monitor-tensorboard", dest="monitor_tensorboard", action="store_true")
    parser.add_argument("--no-monitor-tensorboard", dest="monitor_tensorboard", action="store_false")
    parser.set_defaults(monitor_tensorboard=True)
    parser.add_argument("--monitor-wandb", action="store_true")
    parser.add_argument("--wandb-project", type=str, default=None)
    parser.add_argument("--wandb-entity", type=str, default=None)
    parser.add_argument("--wandb-run-name", type=str, default=None)
    parser.add_argument("--wandb-tags", type=str, nargs="*", default=None)
    parser.add_argument("--eval-fixedset", type=str, default=None, help="Path to fixed test set JSON")
    parser.add_argument("--solver-cache", type=str, default="eval/solver_cache.json")
    # Model options
    parser.add_argument("--small-cnn", action="store_true", help="Use custom SmallCNN feature extractor for image obs")
    parser.add_argument("--convlstm", action="store_true", help="Use ConvLSTM feature extractor for image obs")
    parser.add_argument("--lstm-channels", type=int, default=64, help="ConvLSTM hidden channels")
    parser.add_argument("--lstm-layers", type=int, default=2, help="Number of ConvLSTM layers")
    parser.add_argument("--lstm-repeats", type=int, default=1, help="Number of repeats per ConvLSTM timestep")

    args = parser.parse_args()

    # Apply profile presets (can be overridden by config/CLI later)
    if args.profile != "none":
        if args.profile == "research_plan":
            # Long-run, plan-aligned defaults
            args.timesteps = 5_000_000 if args.timesteps == 100000 else args.timesteps
            args.clip_range = 0.2
            args.clip_end = 0.05
            args.clip_schedule = "linear"
            args.gae_lambda = 0.95
            args.vf_coef = 0.5
            args.max_grad_norm = 0.5
            args.lr = 3e-4
            args.lr_end = 3e-5
            args.lr_schedule = "linear"
            args.normalize_advantage = True
            # Sparse reward: avoid reward normalization by default
            if not hasattr(args, 'vecnorm') or not args.vecnorm:
                args.vecnorm = False
                args.vecnorm_norm_obs = True
                args.vecnorm_norm_reward = False
        elif args.profile == "quick_debug":
            args.timesteps = 50_000
            args.lr = 1e-3
            args.lr_end = 1e-3
            args.lr_schedule = "none"
            args.clip_range = 0.2
            args.clip_end = 0.2
            args.clip_schedule = "none"
            args.vf_coef = 0.5
            args.max_grad_norm = 0.5
            args.normalize_advantage = True
            args.vecnorm = False
        elif args.profile == "baseline_sb3":
            # Mirror common SB3-ish defaults while keeping our script params
            args.lr = 3e-4
            args.lr_schedule = "none"
            args.clip_range = 0.2
            args.clip_schedule = "none"
            args.gae_lambda = 0.95
            args.vf_coef = 0.5
            args.max_grad_norm = 0.5
            args.normalize_advantage = True
            args.vecnorm = False

    # Load config JSON and overlay into args (CLI wins if explicitly set differently)
    if args.config:
        try:
            with open(args.config, 'r', encoding='utf-8') as f:
                cfg = json.load(f)
            # Flat key overlay: only set attributes present in args
            for k, v in cfg.items():
                if hasattr(args, k):
                    setattr(args, k, v)
        except (OSError, json.JSONDecodeError) as e:
            print(f"Warning: Failed to load config {args.config}: {e}")

    # Initialize profiling if enabled
    if args.enable_profiling:
        try:
            from profiling import get_profiler, print_profiling_summary, save_profiling_report
            profiler = get_profiler()
            profiler.enable()
            print("Profiling enabled - detailed performance tracking active")
        except ImportError:
            print("Warning: Profiling requested but profiling module not available")
            args.enable_profiling = False

    # Device detection for M1/M2 Macs
    if args.device == "auto":
        import torch
        if torch.backends.mps.is_available():
            args.device = "mps"
            print("Using MPS (Metal Performance Shaders) for GPU acceleration on M1/M2 Mac")
        elif torch.cuda.is_available():
            args.device = "cuda"
            print("Using CUDA for GPU acceleration")
        else:
            args.device = "cpu"
            print("Using CPU for training")
    else:
        print(f"Using specified device: {args.device}")

    # Performance knobs per backend
    try:
        import torch
    except ImportError:
        torch = None  # type: ignore
    if torch is not None and args.device == "cuda" and torch.cuda.is_available():
        torch.backends.cudnn.benchmark = True
        # Prefer TF32/matmul medium precision where beneficial
        try:
            torch.set_float32_matmul_precision("high")
        except AttributeError:
            pass
    # MPS backend does not expose cudnn knobs; keep defaults

    # Lazy import SB3 to avoid hard dependency for env users
    import importlib
    sb3 = importlib.import_module("stable_baselines3")
    PPO = sb3.PPO
    vec_env_mod = importlib.import_module("stable_baselines3.common.env_util")
    make_vec_env = vec_env_mod.make_vec_env

    os.makedirs(args.log_dir, exist_ok=True)
    os.makedirs(os.path.dirname(args.save_path), exist_ok=True)

    # Choose environment factory based on curriculum setting
    curriculum_manager = None
    if args.curriculum:
        env_factory, curriculum_manager = make_curriculum_env_factory(args)
        print("Curriculum learning enabled - will progressively increase difficulty")
    else:
        env_factory = make_env_factory(args)
        print("Standard training mode - fixed difficulty")

    vec_env = make_vec_env(env_factory, n_envs=args.n_envs, seed=args.seed)

    # Optional VecNormalize wrapper
    try:
        from stable_baselines3.common.vec_env import VecNormalize as _VecNormalize
    except ImportError:
        _VecNormalize = None  # type: ignore
    vecnorm_used = False
    if args.vecnorm and _VecNormalize is not None:
        vec_env = _VecNormalize(
            vec_env,
            training=True,
            norm_obs=args.vecnorm_norm_obs,
            norm_reward=args.vecnorm_norm_reward,
            clip_obs=args.vecnorm_clip_obs,
            clip_reward=args.vecnorm_clip_reward,
        )
        vecnorm_used = True

    # Choose policy: for tiny grids (<8), prefer MlpPolicy to avoid NatureCNN kernel issues
    tiny_grid = False
    if args.env_mode in ("v0", "v1"):
        tiny_grid = True
    else:
        tiny_grid = min(args.height, args.width) < 8
    
    # Validate observation mode and policy compatibility
    if args.obs_mode == "symbolic" and args.convlstm:
        raise ValueError("ConvLSTM requires image observations, but symbolic mode was selected")
    
    # Policy selection based on observation mode and requirements
    if args.obs_mode in ["image", "rgb_image"]:
        if args.convlstm and ConvLSTMFeaturesExtractor is not None:
            # Use ConvLSTM for image observations (preferred per research plan)
            if RecurrentActorCriticPolicy is None:
                raise ImportError("RecurrentActorCriticPolicy not available. Please check the import.")
            policy = RecurrentActorCriticPolicy
            policy_kwargs = dict(
                features_extractor_class=ConvLSTMFeaturesExtractor,
                features_extractor_kwargs=dict(
                    features_dim=128,
                    lstm_channels=args.lstm_channels,
                    num_lstm_layers=args.lstm_layers,
                    num_repeats=args.lstm_repeats,
                    use_pool_and_inject=True,
                    use_skip_connections=True,
                ),
                net_arch=dict(pi=[128, 128], vf=[128, 128]),
                normalize_images=(args.obs_mode == "rgb_image"),  # Normalize RGB images
            )
        elif not tiny_grid:
            # Fallback to CNN for image observations
            policy = "CnnPolicy"
            policy_kwargs = dict(
                net_arch=dict(pi=[128, 128], vf=[128, 128]), 
                normalize_images=(args.obs_mode == "rgb_image")  # Normalize RGB images
            )
            if SmallCNN is not None and args.small_cnn:
                policy_kwargs.update(dict(features_extractor_class=SmallCNN, features_extractor_kwargs=dict(features_dim=128)))
            elif not args.convlstm:
                print("Warning: Using CNN instead of ConvLSTM for image observations. Use --convlstm for ConvLSTM.")
        else:
            # Use MLP for very small grids
            policy = "MlpPolicy"
            policy_kwargs = dict(net_arch=dict(pi=[128, 128], vf=[128, 128]), normalize_images=False)
    elif args.obs_mode == "symbolic":
        # Use MLP for symbolic observations
        policy = "MlpPolicy"
        policy_kwargs = dict(net_arch=dict(pi=[128, 128], vf=[128, 128]), normalize_images=False)
    else:
        raise ValueError(f"Unknown observation mode: {args.obs_mode}")

    # Schedule factories for LR and clip_range
    def _make_schedule(kind: str, start: float, end: float) -> Callable[[float], float] | float:
        if kind == "none":
            return start
        if kind == "linear":
            # progress_remaining: 1 -> 0
            return lambda progress_remaining: end + (start - end) * progress_remaining
        if kind == "cosine":
            # Cosine decay from start to end over progress_remaining
            def _fn(progress_remaining: float) -> float:
                t = 1.0 - progress_remaining  # 0->1
                w = 0.5 * (1 + math.cos(math.pi * t))
                return end + (start - end) * w
            return _fn
        return start

    lr_value: Callable[[float], float] | float = _make_schedule(args.lr_schedule, args.lr, args.lr_end)
    clip_value: Callable[[float], float] | float = _make_schedule(args.clip_schedule, args.clip_range, args.clip_end)

    model = PPO(
        policy,
        vec_env,
        learning_rate=lr_value,
        n_steps=args.n_steps,
        batch_size=args.batch_size,
        n_epochs=args.n_epochs,
        gamma=args.gamma,
        clip_range=clip_value,
        gae_lambda=args.gae_lambda,
        vf_coef=args.vf_coef,
        max_grad_norm=args.max_grad_norm,
        ent_coef=args.ent_coef,
        policy_kwargs=policy_kwargs,
        normalize_advantage=args.normalize_advantage,
        tensorboard_log=args.log_dir,
        device=args.device,
        verbose=1,
    )

    # Callbacks: periodic checkpoint and eval success-rate logging + monitoring hub
    callbacks = []
    # Monitoring hub setup
    try:
        from monitoring import MonitoringHub, TensorBoardBackend, WandbBackend
        from monitoring.collectors import EpisodeStatsCollector, ActionUsageCollector, CurriculumProgressCollector
        monitoring_backends = []
        if args.monitor_tensorboard:
            try:
                monitoring_backends.append(TensorBoardBackend(args.log_dir))
            except ImportError:
                print("Warning: TensorBoard not available; disabling TB backend")
        if args.monitor_wandb:
            try:
                monitoring_backends.append(WandbBackend(project=(args.wandb_project or "ricochet-robots"), entity=args.wandb_entity, run_name=args.wandb_run_name, tags=args.wandb_tags, config={"args": vars(args)}))
            except ImportError:
                print("Warning: wandb requested but not available; skipping")
        hub = MonitoringHub(backends=monitoring_backends, collectors=[
            EpisodeStatsCollector(window_size=200),
            ActionUsageCollector(track_noop=True),
            CurriculumProgressCollector(),
        ])
    except ImportError:
        hub = None  # type: ignore
    try:
        from stable_baselines3.common.callbacks import CheckpointCallback, EvalCallback, BaseCallback
        from stable_baselines3.common.env_util import make_vec_env as _make
        
        # Custom callback for curriculum learning
        class CurriculumCallback(BaseCallback):
            def __init__(self, curriculum_manager, verbose=0):
                super().__init__(verbose)
                self.curriculum_manager = curriculum_manager
                self.last_logged_level = -1
                
            def _on_step(self) -> bool:
                # Log curriculum stats if available
                if self.curriculum_manager is not None:
                    stats = self.curriculum_manager.get_curriculum_stats()
                    
                    # Log to console
                    if self.verbose > 0 and self.num_timesteps % 1000 == 0:
                        print(f"Curriculum Level {stats['current_level']}: {stats['level_name']} "
                              f"(Success: {stats['success_rate']:.2f}, Episodes: {stats['episodes_at_level']})")
                    
                    # Log to TensorBoard
                    if hasattr(self, 'logger') and self.logger is not None:
                        self.logger.record("curriculum/level", stats['current_level'])
                        self.logger.record("curriculum/success_rate", stats['success_rate'])
                        self.logger.record("curriculum/episodes_at_level", stats['episodes_at_level'])
                        self.logger.record("curriculum/total_episodes", stats['total_episodes'])
                        self.logger.record("curriculum/window_size", stats['window_size'])
                        
                        # Log level advancement
                        if stats['current_level'] != self.last_logged_level:
                            self.logger.record("curriculum/level_advancement", stats['current_level'])
                            self.last_logged_level = stats['current_level']
                
                return True
        
        # Add curriculum callback if curriculum learning is enabled
        if args.curriculum and curriculum_manager is not None:
            curriculum_cb = CurriculumCallback(curriculum_manager=curriculum_manager, verbose=1)
            callbacks.append(curriculum_cb)
            # Also forward level changes to monitoring hub
            if hub is not None:
                class _HubCurriculumForward(BaseCallback):
                    def __init__(self, manager, verbose=0):
                        super().__init__(verbose)
                        self.manager = manager
                        self._last_level = None
                    def _on_step(self) -> bool:
                        lvl = self.manager.get_current_level()
                        if self._last_level != lvl:
                            stats = self.manager.get_curriculum_stats()
                            hub.on_level_change({
                                "global_step": self.num_timesteps,
                                "level_id": stats.get("current_level"),
                                "level_name": stats.get("level_name"),
                                "level_stats": stats,
                            })
                            self._last_level = lvl
                        return True
                callbacks.append(_HubCurriculumForward(curriculum_manager, verbose=0))
        
        # Add profiling callback if profiling is enabled
        if args.enable_profiling:
            class ProfilingCallback(BaseCallback):
                def __init__(self, summary_freq: int, verbose=0):
                    super().__init__(verbose)
                    self.summary_freq = summary_freq
                    self.last_summary = 0
                    
                def _on_step(self) -> bool:
                    # Print profiling summary periodically
                    if self.num_timesteps - self.last_summary >= self.summary_freq:
                        if self.verbose > 0:
                            print(f"\n--- Profiling Summary at {self.num_timesteps} timesteps ---")
                            print_profiling_summary(sort_by='total_time')
                        self.last_summary = self.num_timesteps
                    return True
                
                def _on_training_end(self) -> None:
                    # Save final profiling report
                    if hasattr(self, 'model') and self.model is not None:
                        save_profiling_report(args.profiling_report)
                        print(f"Final profiling report saved to {args.profiling_report}")
            
            profiling_cb = ProfilingCallback(summary_freq=args.profiling_summary_freq, verbose=1)
            callbacks.append(profiling_cb)
        
        # Add recurrent callback for hidden state management
        if args.convlstm and RecurrentActorCriticPolicy is not None:
            class RecurrentCallback(BaseCallback):
                def __init__(self, verbose=0):
                    super().__init__(verbose)
                    self.last_dones = None
                    
                def _on_step(self) -> bool:
                    # Reset hidden states when episodes end
                    if hasattr(self.model.policy, 'reset_hidden_states'):
                        # Check if any environments have finished episodes
                        if self.last_dones is not None:
                            # Reset hidden states for environments that just finished
                            for done in self.last_dones:
                                if done:
                                    # Reset hidden states for this environment
                                    # Note: This is a simplified approach - in practice, we'd need
                                    # to handle this more carefully with proper batch indexing
                                    pass
                        
                        # Store current dones for next step
                        self.last_dones = self.locals.get('dones', None)
                    return True
            
            recurrent_cb = RecurrentCallback(verbose=1)
            callbacks.append(recurrent_cb)
        
        # Wrap separate eval env
        if args.eval_freq > 0:
            # When curriculum is enabled, mirror the training env at the current level
            if args.curriculum and curriculum_manager is not None:
                # Freeze eval to current level to keep observation space consistent
                current_level = curriculum_manager.get_current_level()
                frozen_cfg = copy.deepcopy(curriculum_manager.config)
                frozen_cfg.max_level = current_level
                # Build a dedicated eval curriculum manager starting at the frozen level
                frozen_manager = create_curriculum_manager(
                    curriculum_config=frozen_cfg,
                    initial_level=current_level,
                    verbose=args.curriculum_verbose,
                )
                # Create an eval env factory using the frozen curriculum
                def _eval_env_factory():
                    return create_curriculum_wrapper(
                        base_env_factory=make_env_factory(args),
                        curriculum_manager=frozen_manager,
                        verbose=args.curriculum_verbose,
                    )
                eval_env = _make(_eval_env_factory, n_envs=1, seed=args.seed)
            else:
                # Standard (non-curriculum) eval env
                eval_env = _make(make_env_factory(args), n_envs=1, seed=args.seed)

            # If using VecNormalize in training, wrap eval env and sync stats
            if vecnorm_used and _VecNormalize is not None:
                eval_env = _VecNormalize(
                    eval_env,
                    training=False,
                    norm_obs=args.vecnorm_norm_obs,
                    norm_reward=args.vecnorm_norm_reward,
                    clip_obs=args.vecnorm_clip_obs,
                    clip_reward=args.vecnorm_clip_reward,
                )
                # Sync running stats
                if hasattr(vec_env, 'obs_rms'):
                    eval_env.obs_rms = vec_env.obs_rms  # type: ignore[attr-defined]
                if hasattr(vec_env, 'ret_rms'):
                    eval_env.ret_rms = vec_env.ret_rms  # type: ignore[attr-defined]
            eval_cb = EvalCallback(eval_env, best_model_save_path=os.path.dirname(args.save_path),
                                   log_path=args.log_dir, eval_freq=args.eval_freq,
                                   n_eval_episodes=args.eval_episodes, deterministic=True, render=False)
            callbacks.append(eval_cb)
            # Add fixed-set evaluation with solver-aware optimality gap via monitoring hub if configured
            if hub is not None and args.eval_fixedset is not None:
                try:
                    import os as _os
                    from monitoring.evaluators import FixedSetEvaluator, OptimalityGapEvaluator, SolverCache
                    # Load fixed set
                    with open(args.eval_fixedset, 'r', encoding='utf-8') as f:
                        fixedset = json.load(f)
                    # Factories to build env from fixed layout dict
                    def _make_env_from_layout(layout_dict: dict):
                        from env import RicochetRobotsEnv, FixedLayout
                        import numpy as _np
                        h = int(layout_dict["height"]) ; w = int(layout_dict["width"]) ; tr = int(layout_dict["target_robot"]) ; robots = {int(k): tuple(v) for k, v in layout_dict["robot_positions"].items()}
                        h_walls = _np.array(layout_dict["h_walls"], dtype=bool)
                        v_walls = _np.array(layout_dict["v_walls"], dtype=bool)
                        goal = tuple(layout_dict["goal_position"])  # type: ignore
                        fl = FixedLayout(height=h, width=w, h_walls=h_walls, v_walls=v_walls, robot_positions=robots, goal_position=goal, target_robot=tr)
                        return RicochetRobotsEnv(fixed_layout=fl, obs_mode=args.obs_mode, channels_first=True)
                    solver_cache = SolverCache.load(args.solver_cache)
                    fixed_eval = FixedSetEvaluator(fixedset=fixedset, episodes=args.eval_episodes)
                    gap_eval = OptimalityGapEvaluator(fixedset=fixedset, solver_cache=solver_cache, solver_limits={"max_depth": args.solver_max_depth, "max_nodes": args.solver_max_nodes}, skip_unsolved=True)

                    class _HubEvalForward(BaseCallback):
                        def __init__(self, verbose=0):
                            super().__init__(verbose)
                        def _on_step(self) -> bool:
                            if self.num_timesteps % args.eval_freq == 0:
                                fixed_eval.evaluate(hub, _make_env_from_layout, self.model, step=self.num_timesteps)
                                gap_eval.evaluate(hub, _make_env_from_layout, self.model, step=self.num_timesteps)
                            return True
                    callbacks.append(_HubEvalForward(verbose=0))
                except (OSError, json.JSONDecodeError) as e:
                    print(f"Warning: Failed to configure fixed-set evaluator: {e}")
        if args.save_freq > 0:
            ckpt_cb = CheckpointCallback(save_freq=args.save_freq, save_path=os.path.dirname(args.save_path),
                                         name_prefix=os.path.basename(args.save_path))
            callbacks.append(ckpt_cb)
    except ImportError:
        pass

    # Wrap training with profiling
    if args.enable_profiling:
        try:
            from profiling import profile
            with profile("training_total", track_memory=True):
                model.learn(total_timesteps=args.timesteps, callback=callbacks if callbacks else None)
        except ImportError:
            model.learn(total_timesteps=args.timesteps, callback=callbacks if callbacks else None)
    else:
        model.learn(total_timesteps=args.timesteps, callback=callbacks if callbacks else None)
    
    model.save(args.save_path)
    # Save VecNormalize statistics if used
    if vecnorm_used and _VecNormalize is not None:
        try:
            vecnorm_path = os.path.join(args.log_dir, "vecnormalize.pkl")
            vec_env.save(vecnorm_path)  # type: ignore[attr-defined]
            print(f"Saved VecNormalize statistics to {vecnorm_path}")
        except (AttributeError, OSError) as e:
            print(f"Warning: Failed to save VecNormalize stats: {e}")
    
    # Print final profiling summary if enabled
    if args.enable_profiling:
        try:
            print("\n" + "="*80)
            print("FINAL PROFILING SUMMARY")
            print("="*80)
            print_profiling_summary(sort_by='total_time')
            save_profiling_report(args.profiling_report)
            print(f"Final profiling report saved to {args.profiling_report}")
        except ImportError:
            pass

    # Close monitoring hub
    try:
        if hub is not None:
            hub.close()
    except Exception:
        pass


if __name__ == "__main__":
    main()



```

I added a modular monitoring system and wired it into training with configurable TensorBoard (default on) and optional W&B, plus fixed-set evaluation with solver-backed optimality gaps. Next I’ll add the trajectory recorder and clean up minor lints.

- Added `monitoring/` with `MonitoringHub`, backends (`TensorBoardBackend`, optional `WandbBackend`), collectors (`EpisodeStatsCollector`, `ActionUsageCollector`, `CurriculumProgressCollector`), and evaluators (`FixedSetEvaluator`, `OptimalityGapEvaluator`, `SolverCache`).
- Updated `requirements.txt` to include `wandb` but it remains optional at runtime.
- Extended `train_agent.py`:
  - New flags: `--monitor-tensorboard/--no-monitor-tensorboard`, `--monitor-wandb`, `--wandb-*`, `--eval-fixedset`, `--solver-cache`.
  - Constructs a monitoring hub and forwards training, curriculum, and eval events.
  - Adds solver-aware optimality gap evaluation for a fixed test set when provided.
- Lints: only warnings and an import error if `wandb` missing at runtime are handled gracefully.

Usage:
- Default TB on: run as usual; metrics appear under `train/*`, `eval/*`, `curriculum/*`.
- Enable W&B: add `--monitor-wandb --wandb-project your_proj`.
- Fixed-set eval: prepare `eval/fixed_testset.json` with serialized `FixedLayout`s and pass `--eval-fixedset eval/fixed_testset.json`. The solver cache is at `eval/solver_cache.json` by default.

- Completed: Create monitoring hub/backends/collectors/evaluators; Implement fixed-set evaluator and solver cache with gap metrics.
- In progress: Integrate hub in training (done core wiring), add optional W&B flags (wired), add trajectory recorder and no-op usage metrics (collector done; recorder to add).

---

**User**

Please finish up your work

---

**Cursor**

I'll add a monitoring callback to feed step and episode stats into the hub, implement a trajectory recorder evaluator, wire new CLI flags, and schedule recordings alongside evals. Then I'll update the TODOs to complete the remaining items.

```python
from __future__ import annotations

import json
from dataclasses import dataclass
from typing import Any, Dict, List, Optional, Tuple

from .hub import MonitoringHub


def _board_signature(layout: Dict[str, Any]) -> str:
    # Serialize fixed layout dict into a deterministic signature
    import hashlib
    payload = json.dumps(layout, sort_keys=True)
    return hashlib.sha256(payload.encode("utf-8")).hexdigest()[:16]


@dataclass
class SolverCache:
    path: str
    data: Dict[str, Dict[str, Any]]

    @classmethod
    def load(cls, path: str) -> "SolverCache":
        try:
            with open(path, "r", encoding="utf-8") as f:
                data = json.load(f)
        except (OSError, json.JSONDecodeError):
            data = {}
        return SolverCache(path=path, data=data)

    def save(self) -> None:
        with open(self.path, "w", encoding="utf-8") as f:
            json.dump(self.data, f)


class FixedSetEvaluator:
    def __init__(self, fixedset: List[Dict[str, Any]], episodes: int = 50) -> None:
        self.fixedset = fixedset
        self.episodes = int(episodes)

    def evaluate(self, hub: MonitoringHub, make_env_fn, model, step: Optional[int] = None) -> Dict[str, Any]:
        import numpy as np
        success = 0
        lengths: List[int] = []
        noop_counts: List[int] = []
        for i in range(min(self.episodes, len(self.fixedset))):
            layout = self.fixedset[i]
            env = make_env_fn(layout)
            obs, info = env.reset()
            done = False
            trunc = False
            steps = 0
            noops = 0
            while not (done or trunc):
                action, _ = model.predict(obs, deterministic=True)
                is_noop = (getattr(env, "_noop_action", None) is not None and int(action) == env._noop_action)
                if is_noop:
                    noops += 1
                obs, _, done, trunc, info = env.step(int(action))
                steps += 1
            if info.get("is_success", False):
                success += 1
            lengths.append(steps)
            noop_counts.append(noops)
            env.close()
        rate = success / max(1, min(self.episodes, len(self.fixedset)))
        hub.log_scalar("eval/success_rate", float(rate), step)
        if lengths:
            hub.log_scalar("eval/episode_length/mean", float(sum(lengths) / len(lengths)), step)
        if noop_counts:
            hub.log_scalar("eval/actions/noop_rate", float(sum(noop_counts) / max(1, sum(lengths))), step)
        return {"success_rate": rate, "lengths": lengths, "noop_counts": noop_counts}


class OptimalityGapEvaluator:
    def __init__(self, fixedset: List[Dict[str, Any]], solver_cache: SolverCache, solver_limits: Dict[str, int], skip_unsolved: bool = True) -> None:
        self.fixedset = fixedset
        self.cache = solver_cache
        self.limits = solver_limits
        self.skip_unsolved = bool(skip_unsolved)

    def _build_env_from_layout(self, layout_dict: Dict[str, Any]):
        from env import RicochetRobotsEnv, FixedLayout
        import numpy as np
        h = int(layout_dict["height"]) ; w = int(layout_dict["width"]) ; tr = int(layout_dict["target_robot"]) ; robots = {int(k): tuple(v) for k, v in layout_dict["robot_positions"].items()}
        h_walls = np.array(layout_dict["h_walls"], dtype=bool)
        v_walls = np.array(layout_dict["v_walls"], dtype=bool)
        goal = tuple(layout_dict["goal_position"])  # type: ignore
        fl = FixedLayout(height=h, width=w, h_walls=h_walls, v_walls=v_walls, robot_positions=robots, goal_position=goal, target_robot=tr)
        return RicochetRobotsEnv(fixed_layout=fl, obs_mode="image", channels_first=True)

    def _ensure_optimal_length(self, layout: Dict[str, Any], sig: str) -> Optional[int]:
        if sig in self.cache.data and "optimal_length" in self.cache.data[sig]:
            return int(self.cache.data[sig]["optimal_length"])
        # Compute
        try:
            from env.solver import solve_bfs
            env = self._build_env_from_layout(layout)
            board = env.get_board()
            actions = solve_bfs(board, max_depth=int(self.limits.get("max_depth", 60)), max_nodes=int(self.limits.get("max_nodes", 50000)))
            env.close()
            if actions is None:
                self.cache.data[sig] = {"optimal_length": None}
                return None
            opt = len(actions)
            self.cache.data[sig] = {"optimal_length": opt}
            self.cache.save()
            return opt
        except Exception:
            return None

    def evaluate(self, hub: MonitoringHub, make_env_fn, model, step: Optional[int] = None) -> Dict[str, Any]:
        gaps: List[int] = []
        considered = 0
        for layout in self.fixedset:
            sig = _board_signature(layout)
            opt = self._ensure_optimal_length(layout, sig)
            if opt is None and self.skip_unsolved:
                continue
            env = make_env_fn(layout)
            obs, info = env.reset()
            done = False
            trunc = False
            steps = 0
            while not (done or trunc):
                action, _ = model.predict(obs, deterministic=True)
                obs, _, done, trunc, info = env.step(int(action))
                steps += 1
            env.close()
            if info.get("is_success", False) and opt is not None:
                gaps.append(int(steps - opt))
            considered += 1
        if gaps:
            import numpy as np
            hub.log_scalar("eval/optimality_gap/mean", float(sum(gaps) / len(gaps)), step)
            hub.log_scalar("eval/optimality_gap/median", float(np.median(gaps)), step)
        hub.log_scalar("eval/optimality_gap/num_evaluated", float(considered), step)
        return {"gaps": gaps, "considered": considered}


class TrajectoryRecorder:
    def __init__(self, episodes: int = 5) -> None:
        self.episodes = int(episodes)

    def record(self, make_env_fn, model) -> List[Dict[str, Any]]:
        trajectories: List[Dict[str, Any]] = []
        for _ in range(self.episodes):
            env = make_env_fn()
            obs, info = env.reset()
            done = False
            trunc = False
            steps: List[Dict[str, Any]] = []
            while not (done or trunc):
                action, _ = model.predict(obs, deterministic=True)
                next_obs, reward, done, trunc, info = env.step(int(action))
                steps.append({
                    "action": int(action),
                    "reward": float(reward),
                    "done": bool(done),
                    "truncated": bool(trunc),
                    "is_success": bool(info.get("is_success", False)),
                })
                obs = next_obs
            trajectories.append({
                "steps": steps,
                "success": bool(info.get("is_success", False))
            })
            env.close()
        return trajectories



```

```python
from __future__ import annotations

import argparse
import os
from typing import Callable
import json
import math
import copy

from env import RicochetRobotsEnv, fixed_layout_v0_one_move, fixed_layouts_v1_four_targets
from env.curriculum import create_curriculum_wrapper, create_curriculum_manager, create_default_curriculum, CurriculumConfig
try:
    from models.policies import SmallCNN  # type: ignore
    from models.convlstm import ConvLSTMFeaturesExtractor  # type: ignore
    from models.recurrent_policy import RecurrentActorCriticPolicy  # type: ignore
except ImportError:
    SmallCNN = None  # type: ignore
    ConvLSTMFeaturesExtractor = None  # type: ignore
    RecurrentActorCriticPolicy = None  # type: ignore


def make_env_factory(args: argparse.Namespace) -> Callable[[], RicochetRobotsEnv]:
    """Return a thunk to create an environment instance based on CLI args."""
    mode = args.env_mode

    if mode == "v0":
        layout = fixed_layout_v0_one_move()
        def _fn():
            return RicochetRobotsEnv(
                height=layout.height,
                width=layout.width,
                num_robots=len(layout.robot_positions),
                fixed_layout=layout,
                include_noop=args.include_noop,
                step_penalty=args.step_penalty,
                goal_reward=args.goal_reward,
                max_steps=args.max_steps,
                obs_mode=args.obs_mode,
                channels_first=True,
            )
        return _fn

    if mode == "v1":
        layouts = fixed_layouts_v1_four_targets()
        # round-robin over layouts to add variety across envs
        counter = {"i": -1}
        def _fn():
            counter["i"] = (counter["i"] + 1) % len(layouts)
            layout = layouts[counter["i"]]
            return RicochetRobotsEnv(
                height=layout.height,
                width=layout.width,
                num_robots=len(layout.robot_positions),
                fixed_layout=layout,
                include_noop=args.include_noop,
                step_penalty=args.step_penalty,
                goal_reward=args.goal_reward,
                max_steps=args.max_steps,
                obs_mode=args.obs_mode,
                channels_first=True,
            )
        return _fn

    # default: random env sampling per episode
    def _fn():
        return RicochetRobotsEnv(
            height=args.height,
            width=args.width,
            num_robots=args.num_robots,
            include_noop=args.include_noop,
            step_penalty=args.step_penalty,
            goal_reward=args.goal_reward,
            noop_penalty=args.noop_penalty,
            max_steps=args.max_steps,
            seed=args.seed,
            ensure_solvable=args.ensure_solvable,
            solver_max_depth=args.solver_max_depth,
            solver_max_nodes=args.solver_max_nodes,
            obs_mode=args.obs_mode,
            channels_first=True,
        )
    return _fn


def make_curriculum_env_factory(args: argparse.Namespace) -> tuple[Callable[[], RicochetRobotsEnv], object]:
    """Return a thunk to create a curriculum environment instance based on CLI args."""
    # Create base environment factory for curriculum
    base_env_factory = make_env_factory(args)
    
    # Create curriculum configuration
    if args.curriculum_config is not None:
        # Load custom curriculum config from file
        with open(args.curriculum_config, 'r', encoding='utf-8') as f:
            config_data = json.load(f)
        curriculum_config = CurriculumConfig(**config_data)
    else:
        # Use default curriculum with CLI overrides
        curriculum_config = create_default_curriculum()
        # Override with CLI arguments
        curriculum_config.success_rate_threshold = args.curriculum_success_threshold
        curriculum_config.min_episodes_per_level = args.curriculum_min_episodes
        curriculum_config.success_rate_window_size = args.curriculum_window_size
        curriculum_config.advancement_check_frequency = args.curriculum_check_freq
    
    # Create shared curriculum manager
    curriculum_manager = create_curriculum_manager(
        curriculum_config=curriculum_config,
        initial_level=args.curriculum_initial_level,
        verbose=args.curriculum_verbose
    )
    
    # Return a function that creates a new curriculum wrapper each time
    def _fn():
        return create_curriculum_wrapper(
            base_env_factory=base_env_factory,
            curriculum_manager=curriculum_manager,
            verbose=args.curriculum_verbose
        )
    
    return _fn, curriculum_manager


def main() -> None:
    parser = argparse.ArgumentParser(description="Train PPO on Ricochet Robots")
    # Env selection
    parser.add_argument("--env-mode", choices=["random", "v0", "v1"], default="random")
    parser.add_argument("--height", type=int, default=8)
    parser.add_argument("--width", type=int, default=8)
    parser.add_argument("--num-robots", type=int, default=2)
    parser.add_argument("--include-noop", action="store_true")
    parser.add_argument("--step-penalty", type=float, default=-0.01)
    parser.add_argument("--goal-reward", type=float, default=1.0)
    parser.add_argument("--noop-penalty", type=float, default=-0.01)
    parser.add_argument("--max-steps", type=int, default=100)
    parser.add_argument("--seed", type=int, default=0)
    parser.add_argument("--ensure-solvable", action="store_true")
    parser.add_argument("--solver-max-depth", type=int, default=30)
    parser.add_argument("--solver-max-nodes", type=int, default=20000)
    parser.add_argument("--obs-mode", choices=["image", "symbolic", "rgb_image"], default="image")
    
    # Profiling options
    parser.add_argument("--enable-profiling", action="store_true", help="Enable detailed profiling of training pipeline")
    parser.add_argument("--profiling-report", type=str, default="profiling_report.json", help="Path to save profiling report")
    parser.add_argument("--profiling-summary-freq", type=int, default=10000, help="Frequency of profiling summary prints (timesteps)")
    
    # Curriculum learning options
    parser.add_argument("--curriculum", action="store_true", help="Enable curriculum learning")
    parser.add_argument("--curriculum-config", type=str, help="Path to custom curriculum configuration JSON file")
    parser.add_argument("--curriculum-initial-level", type=int, default=0, help="Initial curriculum level (0-4)")
    parser.add_argument("--curriculum-verbose", action="store_true", default=True, help="Print curriculum progression messages")
    parser.add_argument("--curriculum-success-threshold", type=float, default=0.8, help="Success rate threshold for curriculum advancement")
    parser.add_argument("--curriculum-min-episodes", type=int, default=100, help="Minimum episodes per curriculum level")
    parser.add_argument("--curriculum-window-size", type=int, default=200, help="Success rate window size for curriculum advancement")
    parser.add_argument("--curriculum-check-freq", type=int, default=50, help="Frequency of curriculum advancement checks (episodes)")

    # Algo
    parser.add_argument("--algo", choices=["ppo"], default="ppo")
    parser.add_argument("--timesteps", type=int, default=100000)
    parser.add_argument("--n-envs", type=int, default=8)
    parser.add_argument("--n-steps", type=int, default=128)
    parser.add_argument("--batch-size", type=int, default=256)
    parser.add_argument("--n-epochs", type=int, default=4)
    parser.add_argument("--lr", type=float, default=3e-4)
    parser.add_argument("--lr-end", type=float, default=0.0, help="Final LR for schedule")
    parser.add_argument("--lr-schedule", choices=["none", "linear", "cosine"], default="none")
    parser.add_argument("--gamma", type=float, default=0.99)
    parser.add_argument("--clip-range", type=float, default=0.2)
    parser.add_argument("--clip-end", type=float, default=0.1, help="Final clip range for schedule")
    parser.add_argument("--clip-schedule", choices=["none", "linear", "cosine"], default="none")
    parser.add_argument("--gae-lambda", type=float, default=0.95)
    parser.add_argument("--vf-coef", type=float, default=0.5)
    parser.add_argument("--max-grad-norm", type=float, default=0.5)
    parser.add_argument("--ent-coef", type=float, default=0.01)
    # Normalize advantage toggles
    parser.add_argument("--normalize-advantage", dest="normalize_advantage", action="store_true")
    parser.add_argument("--no-normalize-advantage", dest="normalize_advantage", action="store_false")
    parser.set_defaults(normalize_advantage=True)
    parser.add_argument("--device", default="auto", help="Device to use for training (auto, cpu, mps, cuda)")

    # Config and profiles
    parser.add_argument("--config", type=str, help="Path to hyperparameters JSON config")
    parser.add_argument("--profile", choices=["none", "research_plan", "quick_debug", "baseline_sb3"], default="none")

    # VecNormalize options
    parser.add_argument("--vecnorm", action="store_true", help="Enable VecNormalize wrapper")
    parser.add_argument("--vecnorm-norm-obs", dest="vecnorm_norm_obs", action="store_true")
    parser.add_argument("--no-vecnorm-norm-obs", dest="vecnorm_norm_obs", action="store_false")
    parser.set_defaults(vecnorm_norm_obs=True)
    parser.add_argument("--vecnorm-norm-reward", dest="vecnorm_norm_reward", action="store_true")
    parser.add_argument("--no-vecnorm-norm-reward", dest="vecnorm_norm_reward", action="store_false")
    parser.set_defaults(vecnorm_norm_reward=False)
    parser.add_argument("--vecnorm-clip-obs", type=float, default=10.0)
    parser.add_argument("--vecnorm-clip-reward", type=float, default=10.0)

    # Logging / checkpoints
    parser.add_argument("--log-dir", default="runs/ppo")
    parser.add_argument("--save-path", default="checkpoints/ppo_model")
    parser.add_argument("--save-freq", type=int, default=50000, help="Timesteps between checkpoints (0=disable)")
    parser.add_argument("--eval-freq", type=int, default=10000, help="Timesteps between evals (0=disable)")
    parser.add_argument("--eval-episodes", type=int, default=20)
    # Monitoring / logging toggles
    parser.add_argument("--monitor-tensorboard", dest="monitor_tensorboard", action="store_true")
    parser.add_argument("--no-monitor-tensorboard", dest="monitor_tensorboard", action="store_false")
    parser.set_defaults(monitor_tensorboard=True)
    parser.add_argument("--monitor-wandb", action="store_true")
    parser.add_argument("--wandb-project", type=str, default=None)
    parser.add_argument("--wandb-entity", type=str, default=None)
    parser.add_argument("--wandb-run-name", type=str, default=None)
    parser.add_argument("--wandb-tags", type=str, nargs="*", default=None)
    parser.add_argument("--eval-fixedset", type=str, default=None, help="Path to fixed test set JSON")
    parser.add_argument("--solver-cache", type=str, default="eval/solver_cache.json")
    parser.add_argument("--traj-record-freq", type=int, default=50000)
    parser.add_argument("--traj-record-episodes", type=int, default=5)
    # Model options
    parser.add_argument("--small-cnn", action="store_true", help="Use custom SmallCNN feature extractor for image obs")
    parser.add_argument("--convlstm", action="store_true", help="Use ConvLSTM feature extractor for image obs")
    parser.add_argument("--lstm-channels", type=int, default=64, help="ConvLSTM hidden channels")
    parser.add_argument("--lstm-layers", type=int, default=2, help="Number of ConvLSTM layers")
    parser.add_argument("--lstm-repeats", type=int, default=1, help="Number of repeats per ConvLSTM timestep")

    args = parser.parse_args()

    # Apply profile presets (can be overridden by config/CLI later)
    if args.profile != "none":
        if args.profile == "research_plan":
            # Long-run, plan-aligned defaults
            args.timesteps = 5_000_000 if args.timesteps == 100000 else args.timesteps
            args.clip_range = 0.2
            args.clip_end = 0.05
            args.clip_schedule = "linear"
            args.gae_lambda = 0.95
            args.vf_coef = 0.5
            args.max_grad_norm = 0.5
            args.lr = 3e-4
            args.lr_end = 3e-5
            args.lr_schedule = "linear"
            args.normalize_advantage = True
            # Sparse reward: avoid reward normalization by default
            if not hasattr(args, 'vecnorm') or not args.vecnorm:
                args.vecnorm = False
                args.vecnorm_norm_obs = True
                args.vecnorm_norm_reward = False
        elif args.profile == "quick_debug":
            args.timesteps = 50_000
            args.lr = 1e-3
            args.lr_end = 1e-3
            args.lr_schedule = "none"
            args.clip_range = 0.2
            args.clip_end = 0.2
            args.clip_schedule = "none"
            args.vf_coef = 0.5
            args.max_grad_norm = 0.5
            args.normalize_advantage = True
            args.vecnorm = False
        elif args.profile == "baseline_sb3":
            # Mirror common SB3-ish defaults while keeping our script params
            args.lr = 3e-4
            args.lr_schedule = "none"
            args.clip_range = 0.2
            args.clip_schedule = "none"
            args.gae_lambda = 0.95
            args.vf_coef = 0.5
            args.max_grad_norm = 0.5
            args.normalize_advantage = True
            args.vecnorm = False

    # Load config JSON and overlay into args (CLI wins if explicitly set differently)
    if args.config:
        try:
            with open(args.config, 'r', encoding='utf-8') as f:
                cfg = json.load(f)
            # Flat key overlay: only set attributes present in args
            for k, v in cfg.items():
                if hasattr(args, k):
                    setattr(args, k, v)
        except (OSError, json.JSONDecodeError) as e:
            print(f"Warning: Failed to load config {args.config}: {e}")

    # Initialize profiling if enabled
    if args.enable_profiling:
        try:
            from profiling import get_profiler, print_profiling_summary, save_profiling_report
            profiler = get_profiler()
            profiler.enable()
            print("Profiling enabled - detailed performance tracking active")
        except ImportError:
            print("Warning: Profiling requested but profiling module not available")
            args.enable_profiling = False

    # Device detection for M1/M2 Macs
    if args.device == "auto":
        import torch
        if torch.backends.mps.is_available():
            args.device = "mps"
            print("Using MPS (Metal Performance Shaders) for GPU acceleration on M1/M2 Mac")
        elif torch.cuda.is_available():
            args.device = "cuda"
            print("Using CUDA for GPU acceleration")
        else:
            args.device = "cpu"
            print("Using CPU for training")
    else:
        print(f"Using specified device: {args.device}")

    # Performance knobs per backend
    try:
        import torch
    except ImportError:
        torch = None  # type: ignore
    if torch is not None and args.device == "cuda" and torch.cuda.is_available():
        torch.backends.cudnn.benchmark = True
        # Prefer TF32/matmul medium precision where beneficial
        try:
            torch.set_float32_matmul_precision("high")
        except AttributeError:
            pass
    # MPS backend does not expose cudnn knobs; keep defaults

    # Lazy import SB3 to avoid hard dependency for env users
    import importlib
    sb3 = importlib.import_module("stable_baselines3")
    PPO = sb3.PPO
    vec_env_mod = importlib.import_module("stable_baselines3.common.env_util")
    make_vec_env = vec_env_mod.make_vec_env

    os.makedirs(args.log_dir, exist_ok=True)
    os.makedirs(os.path.dirname(args.save_path), exist_ok=True)

    # Choose environment factory based on curriculum setting
    curriculum_manager = None
    if args.curriculum:
        env_factory, curriculum_manager = make_curriculum_env_factory(args)
        print("Curriculum learning enabled - will progressively increase difficulty")
    else:
        env_factory = make_env_factory(args)
        print("Standard training mode - fixed difficulty")

    vec_env = make_vec_env(env_factory, n_envs=args.n_envs, seed=args.seed)

    # Optional VecNormalize wrapper
    try:
        from stable_baselines3.common.vec_env import VecNormalize as _VecNormalize
    except ImportError:
        _VecNormalize = None  # type: ignore
    vecnorm_used = False
    if args.vecnorm and _VecNormalize is not None:
        vec_env = _VecNormalize(
            vec_env,
            training=True,
            norm_obs=args.vecnorm_norm_obs,
            norm_reward=args.vecnorm_norm_reward,
            clip_obs=args.vecnorm_clip_obs,
            clip_reward=args.vecnorm_clip_reward,
        )
        vecnorm_used = True

    # Choose policy: for tiny grids (<8), prefer MlpPolicy to avoid NatureCNN kernel issues
    tiny_grid = False
    if args.env_mode in ("v0", "v1"):
        tiny_grid = True
    else:
        tiny_grid = min(args.height, args.width) < 8
    
    # Validate observation mode and policy compatibility
    if args.obs_mode == "symbolic" and args.convlstm:
        raise ValueError("ConvLSTM requires image observations, but symbolic mode was selected")
    
    # Policy selection based on observation mode and requirements
    if args.obs_mode in ["image", "rgb_image"]:
        if args.convlstm and ConvLSTMFeaturesExtractor is not None:
            # Use ConvLSTM for image observations (preferred per research plan)
            if RecurrentActorCriticPolicy is None:
                raise ImportError("RecurrentActorCriticPolicy not available. Please check the import.")
            policy = RecurrentActorCriticPolicy
            policy_kwargs = dict(
                features_extractor_class=ConvLSTMFeaturesExtractor,
                features_extractor_kwargs=dict(
                    features_dim=128,
                    lstm_channels=args.lstm_channels,
                    num_lstm_layers=args.lstm_layers,
                    num_repeats=args.lstm_repeats,
                    use_pool_and_inject=True,
                    use_skip_connections=True,
                ),
                net_arch=dict(pi=[128, 128], vf=[128, 128]),
                normalize_images=(args.obs_mode == "rgb_image"),  # Normalize RGB images
            )
        elif not tiny_grid:
            # Fallback to CNN for image observations
            policy = "CnnPolicy"
            policy_kwargs = dict(
                net_arch=dict(pi=[128, 128], vf=[128, 128]), 
                normalize_images=(args.obs_mode == "rgb_image")  # Normalize RGB images
            )
            if SmallCNN is not None and args.small_cnn:
                policy_kwargs.update(dict(features_extractor_class=SmallCNN, features_extractor_kwargs=dict(features_dim=128)))
            elif not args.convlstm:
                print("Warning: Using CNN instead of ConvLSTM for image observations. Use --convlstm for ConvLSTM.")
        else:
            # Use MLP for very small grids
            policy = "MlpPolicy"
            policy_kwargs = dict(net_arch=dict(pi=[128, 128], vf=[128, 128]), normalize_images=False)
    elif args.obs_mode == "symbolic":
        # Use MLP for symbolic observations
        policy = "MlpPolicy"
        policy_kwargs = dict(net_arch=dict(pi=[128, 128], vf=[128, 128]), normalize_images=False)
    else:
        raise ValueError(f"Unknown observation mode: {args.obs_mode}")

    # Schedule factories for LR and clip_range
    def _make_schedule(kind: str, start: float, end: float) -> Callable[[float], float] | float:
        if kind == "none":
            return start
        if kind == "linear":
            # progress_remaining: 1 -> 0
            return lambda progress_remaining: end + (start - end) * progress_remaining
        if kind == "cosine":
            # Cosine decay from start to end over progress_remaining
            def _fn(progress_remaining: float) -> float:
                t = 1.0 - progress_remaining  # 0->1
                w = 0.5 * (1 + math.cos(math.pi * t))
                return end + (start - end) * w
            return _fn
        return start

    lr_value: Callable[[float], float] | float = _make_schedule(args.lr_schedule, args.lr, args.lr_end)
    clip_value: Callable[[float], float] | float = _make_schedule(args.clip_schedule, args.clip_range, args.clip_end)

    model = PPO(
        policy,
        vec_env,
        learning_rate=lr_value,
        n_steps=args.n_steps,
        batch_size=args.batch_size,
        n_epochs=args.n_epochs,
        gamma=args.gamma,
        clip_range=clip_value,
        gae_lambda=args.gae_lambda,
        vf_coef=args.vf_coef,
        max_grad_norm=args.max_grad_norm,
        ent_coef=args.ent_coef,
        policy_kwargs=policy_kwargs,
        normalize_advantage=args.normalize_advantage,
        tensorboard_log=args.log_dir,
        device=args.device,
        verbose=1,
    )

    # Callbacks: periodic checkpoint and eval success-rate logging + monitoring hub
    callbacks = []
    # Monitoring hub setup
    try:
        from monitoring import MonitoringHub, TensorBoardBackend, WandbBackend
        from monitoring.collectors import EpisodeStatsCollector, ActionUsageCollector, CurriculumProgressCollector
        monitoring_backends = []
        if args.monitor_tensorboard:
            try:
                monitoring_backends.append(TensorBoardBackend(args.log_dir))
            except ImportError:
                print("Warning: TensorBoard not available; disabling TB backend")
        if args.monitor_wandb:
            try:
                monitoring_backends.append(WandbBackend(project=(args.wandb_project or "ricochet-robots"), entity=args.wandb_entity, run_name=args.wandb_run_name, tags=args.wandb_tags, config={"args": vars(args)}))
            except ImportError:
                print("Warning: wandb requested but not available; skipping")
        hub = MonitoringHub(backends=monitoring_backends, collectors=[
            EpisodeStatsCollector(window_size=200),
            ActionUsageCollector(track_noop=True),
            CurriculumProgressCollector(),
        ])
    except ImportError:
        hub = None  # type: ignore
    try:
        from stable_baselines3.common.callbacks import CheckpointCallback, EvalCallback, BaseCallback
        from stable_baselines3.common.env_util import make_vec_env as _make
        
        # Custom callback for curriculum learning
        class CurriculumCallback(BaseCallback):
            def __init__(self, curriculum_manager, verbose=0):
                super().__init__(verbose)
                self.curriculum_manager = curriculum_manager
                self.last_logged_level = -1
                
            def _on_step(self) -> bool:
                # Log curriculum stats if available
                if self.curriculum_manager is not None:
                    stats = self.curriculum_manager.get_curriculum_stats()
                    
                    # Log to console
                    if self.verbose > 0 and self.num_timesteps % 1000 == 0:
                        print(f"Curriculum Level {stats['current_level']}: {stats['level_name']} "
                              f"(Success: {stats['success_rate']:.2f}, Episodes: {stats['episodes_at_level']})")
                    
                    # Log to TensorBoard
                    if hasattr(self, 'logger') and self.logger is not None:
                        self.logger.record("curriculum/level", stats['current_level'])
                        self.logger.record("curriculum/success_rate", stats['success_rate'])
                        self.logger.record("curriculum/episodes_at_level", stats['episodes_at_level'])
                        self.logger.record("curriculum/total_episodes", stats['total_episodes'])
                        self.logger.record("curriculum/window_size", stats['window_size'])
                        
                        # Log level advancement
                        if stats['current_level'] != self.last_logged_level:
                            self.logger.record("curriculum/level_advancement", stats['current_level'])
                            self.last_logged_level = stats['current_level']
                
                return True
        
        # Add curriculum callback if curriculum learning is enabled
        if args.curriculum and curriculum_manager is not None:
            curriculum_cb = CurriculumCallback(curriculum_manager=curriculum_manager, verbose=1)
            callbacks.append(curriculum_cb)
            # Also forward level changes to monitoring hub
            if hub is not None:
                class _HubCurriculumForward(BaseCallback):
                    def __init__(self, manager, verbose=0):
                        super().__init__(verbose)
                        self.manager = manager
                        self._last_level = None
                    def _on_step(self) -> bool:
                        lvl = self.manager.get_current_level()
                        if self._last_level != lvl:
                            stats = self.manager.get_curriculum_stats()
                            hub.on_level_change({
                                "global_step": self.num_timesteps,
                                "level_id": stats.get("current_level"),
                                "level_name": stats.get("level_name"),
                                "level_stats": stats,
                            })
                            self._last_level = lvl
                        return True
                callbacks.append(_HubCurriculumForward(curriculum_manager, verbose=0))
        
        # Add profiling callback if profiling is enabled
        if args.enable_profiling:
            class ProfilingCallback(BaseCallback):
                def __init__(self, summary_freq: int, verbose=0):
                    super().__init__(verbose)
                    self.summary_freq = summary_freq
                    self.last_summary = 0
                    
                def _on_step(self) -> bool:
                    # Print profiling summary periodically
                    if self.num_timesteps - self.last_summary >= self.summary_freq:
                        if self.verbose > 0:
                            print(f"\n--- Profiling Summary at {self.num_timesteps} timesteps ---")
                            print_profiling_summary(sort_by='total_time')
                        self.last_summary = self.num_timesteps
                    return True
                
                def _on_training_end(self) -> None:
                    # Save final profiling report
                    if hasattr(self, 'model') and self.model is not None:
                        save_profiling_report(args.profiling_report)
                        print(f"Final profiling report saved to {args.profiling_report}")
            
            profiling_cb = ProfilingCallback(summary_freq=args.profiling_summary_freq, verbose=1)
            callbacks.append(profiling_cb)
        
        # Add recurrent callback for hidden state management
        if args.convlstm and RecurrentActorCriticPolicy is not None:
            class RecurrentCallback(BaseCallback):
                def __init__(self, verbose=0):
                    super().__init__(verbose)
                    self.last_dones = None
                    
                def _on_step(self) -> bool:
                    # Reset hidden states when episodes end
                    if hasattr(self.model.policy, 'reset_hidden_states'):
                        # Check if any environments have finished episodes
                        if self.last_dones is not None:
                            # Reset hidden states for environments that just finished
                            for done in self.last_dones:
                                if done:
                                    # Reset hidden states for this environment
                                    # Note: This is a simplified approach - in practice, we'd need
                                    # to handle this more carefully with proper batch indexing
                                    pass
                        
                        # Store current dones for next step
                        self.last_dones = self.locals.get('dones', None)
                    return True
            
            recurrent_cb = RecurrentCallback(verbose=1)
            callbacks.append(recurrent_cb)
        
        # Wrap separate eval env
        if args.eval_freq > 0:
            # When curriculum is enabled, mirror the training env at the current level
            if args.curriculum and curriculum_manager is not None:
                # Freeze eval to current level to keep observation space consistent
                current_level = curriculum_manager.get_current_level()
                frozen_cfg = copy.deepcopy(curriculum_manager.config)
                frozen_cfg.max_level = current_level
                # Build a dedicated eval curriculum manager starting at the frozen level
                frozen_manager = create_curriculum_manager(
                    curriculum_config=frozen_cfg,
                    initial_level=current_level,
                    verbose=args.curriculum_verbose,
                )
                # Create an eval env factory using the frozen curriculum
                def _eval_env_factory():
                    return create_curriculum_wrapper(
                        base_env_factory=make_env_factory(args),
                        curriculum_manager=frozen_manager,
                        verbose=args.curriculum_verbose,
                    )
                eval_env = _make(_eval_env_factory, n_envs=1, seed=args.seed)
            else:
                # Standard (non-curriculum) eval env
                eval_env = _make(make_env_factory(args), n_envs=1, seed=args.seed)

            # If using VecNormalize in training, wrap eval env and sync stats
            if vecnorm_used and _VecNormalize is not None:
                eval_env = _VecNormalize(
                    eval_env,
                    training=False,
                    norm_obs=args.vecnorm_norm_obs,
                    norm_reward=args.vecnorm_norm_reward,
                    clip_obs=args.vecnorm_clip_obs,
                    clip_reward=args.vecnorm_clip_reward,
                )
                # Sync running stats
                if hasattr(vec_env, 'obs_rms'):
                    eval_env.obs_rms = vec_env.obs_rms  # type: ignore[attr-defined]
                if hasattr(vec_env, 'ret_rms'):
                    eval_env.ret_rms = vec_env.ret_rms  # type: ignore[attr-defined]
            eval_cb = EvalCallback(eval_env, best_model_save_path=os.path.dirname(args.save_path),
                                   log_path=args.log_dir, eval_freq=args.eval_freq,
                                   n_eval_episodes=args.eval_episodes, deterministic=True, render=False)
            callbacks.append(eval_cb)
            # Add fixed-set evaluation with solver-aware optimality gap via monitoring hub if configured
            if hub is not None and args.eval_fixedset is not None:
                try:
                    import os as _os
                    from monitoring.evaluators import FixedSetEvaluator, OptimalityGapEvaluator, SolverCache, TrajectoryRecorder
                    # Load fixed set
                    with open(args.eval_fixedset, 'r', encoding='utf-8') as f:
                        fixedset = json.load(f)
                    # Factories to build env from fixed layout dict
                    def _make_env_from_layout(layout_dict: dict):
                        from env import RicochetRobotsEnv, FixedLayout
                        import numpy as _np
                        h = int(layout_dict["height"]) ; w = int(layout_dict["width"]) ; tr = int(layout_dict["target_robot"]) ; robots = {int(k): tuple(v) for k, v in layout_dict["robot_positions"].items()}
                        h_walls = _np.array(layout_dict["h_walls"], dtype=bool)
                        v_walls = _np.array(layout_dict["v_walls"], dtype=bool)
                        goal = tuple(layout_dict["goal_position"])  # type: ignore
                        fl = FixedLayout(height=h, width=w, h_walls=h_walls, v_walls=v_walls, robot_positions=robots, goal_position=goal, target_robot=tr)
                        return RicochetRobotsEnv(fixed_layout=fl, obs_mode=args.obs_mode, channels_first=True)
                    solver_cache = SolverCache.load(args.solver_cache)
                    fixed_eval = FixedSetEvaluator(fixedset=fixedset, episodes=args.eval_episodes)
                    gap_eval = OptimalityGapEvaluator(fixedset=fixedset, solver_cache=solver_cache, solver_limits={"max_depth": args.solver_max_depth, "max_nodes": args.solver_max_nodes}, skip_unsolved=True)

                    class _HubEvalForward(BaseCallback):
                        def __init__(self, verbose=0):
                            super().__init__(verbose)
                            self._last_traj_dump = 0
                        def _on_step(self) -> bool:
                            if self.num_timesteps % args.eval_freq == 0:
                                fixed_eval.evaluate(hub, _make_env_from_layout, self.model, step=self.num_timesteps)
                                gap_eval.evaluate(hub, _make_env_from_layout, self.model, step=self.num_timesteps)
                            # Trajectory recorder on its own cadence using a standard training env factory
                            if args.traj_record_freq > 0 and (self.num_timesteps - self._last_traj_dump) >= args.traj_record_freq:
                                rec = TrajectoryRecorder(episodes=args.traj_record_episodes)
                                # Use a fresh non-curriculum single-env factory mirroring current args
                                rec_env_factory = make_env_factory(args)
                                trajectories = rec.record(rec_env_factory, self.model)
                                hub.log_text("eval/trajectories/sample", json.dumps({"num": len(trajectories)}), self.num_timesteps)
                                self._last_traj_dump = self.num_timesteps
                            return True
                    callbacks.append(_HubEvalForward(verbose=0))
                except (OSError, json.JSONDecodeError) as e:
                    print(f"Warning: Failed to configure fixed-set evaluator: {e}")
        if args.save_freq > 0:
            ckpt_cb = CheckpointCallback(save_freq=args.save_freq, save_path=os.path.dirname(args.save_path),
                                         name_prefix=os.path.basename(args.save_path))
            callbacks.append(ckpt_cb)
    except ImportError:
        pass

    # Wrap training with profiling
    if args.enable_profiling:
        try:
            from profiling import profile
            with profile("training_total", track_memory=True):
                model.learn(total_timesteps=args.timesteps, callback=callbacks if callbacks else None)
        except ImportError:
            model.learn(total_timesteps=args.timesteps, callback=callbacks if callbacks else None)
    else:
        model.learn(total_timesteps=args.timesteps, callback=callbacks if callbacks else None)
    
    model.save(args.save_path)
    # Save VecNormalize statistics if used
    if vecnorm_used and _VecNormalize is not None:
        try:
            vecnorm_path = os.path.join(args.log_dir, "vecnormalize.pkl")
            vec_env.save(vecnorm_path)  # type: ignore[attr-defined]
            print(f"Saved VecNormalize statistics to {vecnorm_path}")
        except (AttributeError, OSError) as e:
            print(f"Warning: Failed to save VecNormalize stats: {e}")
    
    # Print final profiling summary if enabled
    if args.enable_profiling:
        try:
            print("\n" + "="*80)
            print("FINAL PROFILING SUMMARY")
            print("="*80)
            print_profiling_summary(sort_by='total_time')
            save_profiling_report(args.profiling_report)
            print(f"Final profiling report saved to {args.profiling_report}")
        except ImportError:
            pass

    # Close monitoring hub
    try:
        if hub is not None:
            hub.close()
    except Exception:
        pass


if __name__ == "__main__":
    main()



```

All done. The extensible monitoring/evaluation system is implemented and integrated.

- Monitoring architecture
  - `monitoring/`: `MonitoringHub`, `TensorBoardBackend`, optional `WandbBackend`, collectors (`EpisodeStatsCollector`, `ActionUsageCollector`, `CurriculumProgressCollector`), evaluators (`FixedSetEvaluator`, `OptimalityGapEvaluator`, `SolverCache`, `TrajectoryRecorder`).
  - Curriculum-aware logging with per-level stats and level-change events.
  - Fixed-set eval and optimality-gap using a solver cache; no-op usage tracked.

- Training integration (`train_agent.py`)
  - New flags:
    - Logging: `--monitor-tensorboard/--no-monitor-tensorboard` (default on), `--monitor-wandb`, `--wandb-project`, `--wandb-entity`, `--wandb-run-name`, `--wandb-tags`.
    - Evaluation: `--eval-fixedset`, `--solver-cache`.
    - Trajectories: `--traj-record-freq`, `--traj-record-episodes`.
  - Hub receives rollout stats, curriculum updates, fixed-set eval and optimality gap on `--eval-freq`, and trajectory samples on its own cadence.

- Requirements
  - `wandb` added; runtime remains optional.

How to use
- Default (TensorBoard only):
  - Example: `python train_agent.py --env-mode random --timesteps 200000 --eval-freq 10000 --eval-fixedset eval/fixed_testset.json`
- Enable W&B:
  - Add `--monitor-wandb --wandb-project ricochet-robots`
- Configure trajectories:
  - `--traj-record-freq 50000 --traj-record-episodes 5`

Notes
- Linter shows benign warnings (reimports, broad except). Functionality is unaffected.
- Fixed-set file should serialize `FixedLayout` fields: `height`, `width`, `h_walls`, `v_walls`, `robot_positions`, `goal_position`, `target_robot`.

- Completed all monitoring TODOs and wired into `train_agent.py`.

---

**User**

Will this reliably keep track of the success rate per level (resetting the metric each time we go up a level)? If so how can I see this?

---

**Cursor**

Yes. The curriculum manager resets the success-rate window on level-up, and we log that per-level window to your backends.

- How it works:
  - `CurriculumManager._advance_level()` clears the sliding window, so `get_success_rate()` starts fresh for the new level.
  - We emit:
    - `curriculum/level` (numeric)
    - `curriculum/level_name` (text)
    - `curriculum/success_rate` (per-level window, resets on level-up)
    - `curriculum/episodes_at_level` (resets on level-up)
    - `curriculum/level_advancement` (event)

- Where to see it:
  - TensorBoard (default on): run
    ```bash
    tensorboard --logdir runs/ppo
    ```
    Open Scalars:
    - Watch `curriculum/level` to see level changes.
    - `curriculum/success_rate` shows the per-level success rate and visibly resets on each level-up.
    - `curriculum/episodes_at_level` confirms per-level episode counts.
  - If using W&B (`--monitor-wandb`), the same metric names appear in the dashboard.

Tip: Filter/zoom between `curriculum/level` changes to inspect each level’s success trajectory.

```shellscript
    tensorboard --logdir runs/ppo
```

