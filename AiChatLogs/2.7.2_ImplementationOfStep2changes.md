# Implementing feedback for research proposal step 2
_Exported on 15/09/2025 at 17:37:44 BST from Cursor (1.4.5)_

---

**User**

You are a research engineer, working as part of a team to implement a research proposal. Please take the time to read the ResearchPlan.md, focussing in particular on step 2. Your colleagues have already implemented and reviewed steps 1.1 - 1.4 of the plan, which have been checked and are deemed completed, so you do not need to worry about this. You can read the ProgressNotes left by your colleagues, and read the code they have written, in order to understand what you are working with.

Now, another colleague has implemented step 2 of the plan, detailed in ResearchPlan.md, and we have been given some feedback on it.
Your job will be to go through the feedback that I provide to you, and implement it. Most of the feedback will refer to things which are missing from the original specificaton/suggestions in ResearchPlan.md, so you should refer to this (and also do as much thinking as you need yourself) in order to implement the changes. Make sure that you implement the changes in line with the long term goals of the proposal, taking care to ensure that code is flexible and will work for the desired usecases later down the line.

The feedback for implementation is attached. We will go through and address in order. Please go ahead and make whatever changes are necessary to address the first piece of feedback, stopping to ask questions whenever you need to.

---

**Cursor**

I'll scan the training and policy code to see how PPO and the recurrent policy are wired today, so I can switch to a correct sb3-contrib RecurrentPPO integration and identify all touch points to update.

I'm going to read the training script and recurrent policy files to see how PPO is currently integrated and what needs to change to use a correct recurrent PPO from sb3-contrib.

I'll update the training and evaluation scripts to integrate sb3-contrib's RecurrentPPO correctly, disable the incorrect custom recurrent policy path, and add the dependency. Then I'll run a quick lint check for the modified files.

```pip-requirements
numpy>=1.26
gymnasium>=0.29
pytest>=8.0
jupyter>=1.0
matplotlib>=3.7
torch>=2.3
stable-baselines3>=2.3.2
sb3-contrib>=2.3.0
tensorboard>=2.15.0
# MPS support for M1/M2 Macs
torch-audio>=2.3
wandb>=0.17
```

I'll create a small task list for this change, then update requirements, adjust `train_agent.py` to use sb3-contrib's RecurrentPPO when recurrent is requested, remove the placeholder recurrent callback, and update `evaluate_agent.py` to support loading and running RecurrentPPO.

I'll update the training script to import and use RecurrentPPO from sb3-contrib, add recurrent policy kwargs, and remove the placeholder recurrent callback. Then I'll adjust evaluation to optionally import RecurrentPPO and load models accordingly.

I'll update the training script to switch to sb3-contrib RecurrentPPO when recurrent is requested, remove the placeholder recurrent callback, and update evaluation to be able to load RecurrentPPO and handle LSTM states during rollout. I'll make the edits now.

```python
from __future__ import annotations

import argparse
import os
from typing import Callable
import json
import math
import copy

from env import RicochetRobotsEnv, fixed_layout_v0_one_move, fixed_layouts_v1_four_targets
from env.curriculum import create_curriculum_wrapper, create_curriculum_manager, create_default_curriculum, CurriculumConfig
try:
    from models.policies import SmallCNN  # type: ignore
    from models.convlstm import ConvLSTMFeaturesExtractor  # type: ignore
    from models.recurrent_policy import RecurrentActorCriticPolicy  # type: ignore
except ImportError:
    SmallCNN = None  # type: ignore
    ConvLSTMFeaturesExtractor = None  # type: ignore
    RecurrentActorCriticPolicy = None  # type: ignore


def make_env_factory(args: argparse.Namespace) -> Callable[[], RicochetRobotsEnv]:
    """Return a thunk to create an environment instance based on CLI args."""
    mode = args.env_mode

    if mode == "v0":
        layout = fixed_layout_v0_one_move()
        def _fn():
            return RicochetRobotsEnv(
                height=layout.height,
                width=layout.width,
                num_robots=len(layout.robot_positions),
                fixed_layout=layout,
                include_noop=args.include_noop,
                step_penalty=args.step_penalty,
                goal_reward=args.goal_reward,
                max_steps=args.max_steps,
                obs_mode=args.obs_mode,
                channels_first=True,
            )
        return _fn

    if mode == "v1":
        layouts = fixed_layouts_v1_four_targets()
        # round-robin over layouts to add variety across envs
        counter = {"i": -1}
        def _fn():
            counter["i"] = (counter["i"] + 1) % len(layouts)
            layout = layouts[counter["i"]]
            return RicochetRobotsEnv(
                height=layout.height,
                width=layout.width,
                num_robots=len(layout.robot_positions),
                fixed_layout=layout,
                include_noop=args.include_noop,
                step_penalty=args.step_penalty,
                goal_reward=args.goal_reward,
                max_steps=args.max_steps,
                obs_mode=args.obs_mode,
                channels_first=True,
            )
        return _fn

    # default: random env sampling per episode
    def _fn():
        return RicochetRobotsEnv(
            height=args.height,
            width=args.width,
            num_robots=args.num_robots,
            include_noop=args.include_noop,
            step_penalty=args.step_penalty,
            goal_reward=args.goal_reward,
            noop_penalty=args.noop_penalty,
            max_steps=args.max_steps,
            seed=args.seed,
            ensure_solvable=args.ensure_solvable,
            solver_max_depth=args.solver_max_depth,
            solver_max_nodes=args.solver_max_nodes,
            obs_mode=args.obs_mode,
            channels_first=True,
        )
    return _fn


def make_curriculum_env_factory(args: argparse.Namespace) -> tuple[Callable[[], RicochetRobotsEnv], object]:
    """Return a thunk to create a curriculum environment instance based on CLI args."""
    # Create base environment factory for curriculum
    base_env_factory = make_env_factory(args)
    
    # Create curriculum configuration
    if args.curriculum_config is not None:
        # Load custom curriculum config from file
        with open(args.curriculum_config, 'r', encoding='utf-8') as f:
            config_data = json.load(f)
        curriculum_config = CurriculumConfig(**config_data)
    else:
        # Use default curriculum with CLI overrides
        curriculum_config = create_default_curriculum()
        # Override with CLI arguments
        curriculum_config.success_rate_threshold = args.curriculum_success_threshold
        curriculum_config.min_episodes_per_level = args.curriculum_min_episodes
        curriculum_config.success_rate_window_size = args.curriculum_window_size
        curriculum_config.advancement_check_frequency = args.curriculum_check_freq
    
    # Create shared curriculum manager
    curriculum_manager = create_curriculum_manager(
        curriculum_config=curriculum_config,
        initial_level=args.curriculum_initial_level,
        verbose=args.curriculum_verbose
    )
    
    # Return a function that creates a new curriculum wrapper each time
    def _fn():
        return create_curriculum_wrapper(
            base_env_factory=base_env_factory,
            curriculum_manager=curriculum_manager,
            verbose=args.curriculum_verbose
        )
    
    return _fn, curriculum_manager


def main() -> None:
    parser = argparse.ArgumentParser(description="Train PPO on Ricochet Robots")
    # Env selection
    parser.add_argument("--env-mode", choices=["random", "v0", "v1"], default="random")
    parser.add_argument("--height", type=int, default=8)
    parser.add_argument("--width", type=int, default=8)
    parser.add_argument("--num-robots", type=int, default=2)
    parser.add_argument("--include-noop", action="store_true")
    parser.add_argument("--step-penalty", type=float, default=-0.01)
    parser.add_argument("--goal-reward", type=float, default=1.0)
    parser.add_argument("--noop-penalty", type=float, default=-0.01)
    parser.add_argument("--max-steps", type=int, default=100)
    parser.add_argument("--seed", type=int, default=0)
    parser.add_argument("--ensure-solvable", action="store_true")
    parser.add_argument("--solver-max-depth", type=int, default=30)
    parser.add_argument("--solver-max-nodes", type=int, default=20000)
    parser.add_argument("--obs-mode", choices=["image", "symbolic", "rgb_image"], default="image")
    
    # Profiling options
    parser.add_argument("--enable-profiling", action="store_true", help="Enable detailed profiling of training pipeline")
    parser.add_argument("--profiling-report", type=str, default="profiling_report.json", help="Path to save profiling report")
    parser.add_argument("--profiling-summary-freq", type=int, default=10000, help="Frequency of profiling summary prints (timesteps)")
    
    # Curriculum learning options
    parser.add_argument("--curriculum", action="store_true", help="Enable curriculum learning")
    parser.add_argument("--curriculum-config", type=str, help="Path to custom curriculum configuration JSON file")
    parser.add_argument("--curriculum-initial-level", type=int, default=0, help="Initial curriculum level (0-4)")
    parser.add_argument("--curriculum-verbose", action="store_true", default=True, help="Print curriculum progression messages")
    parser.add_argument("--curriculum-success-threshold", type=float, default=0.8, help="Success rate threshold for curriculum advancement")
    parser.add_argument("--curriculum-min-episodes", type=int, default=100, help="Minimum episodes per curriculum level")
    parser.add_argument("--curriculum-window-size", type=int, default=200, help="Success rate window size for curriculum advancement")
    parser.add_argument("--curriculum-check-freq", type=int, default=50, help="Frequency of curriculum advancement checks (episodes)")

    # Algo
    parser.add_argument("--algo", choices=["ppo"], default="ppo")
    parser.add_argument("--timesteps", type=int, default=100000)
    parser.add_argument("--n-envs", type=int, default=8)
    parser.add_argument("--n-steps", type=int, default=128)
    parser.add_argument("--batch-size", type=int, default=256)
    parser.add_argument("--n-epochs", type=int, default=4)
    parser.add_argument("--lr", type=float, default=3e-4)
    parser.add_argument("--lr-end", type=float, default=0.0, help="Final LR for schedule")
    parser.add_argument("--lr-schedule", choices=["none", "linear", "cosine"], default="none")
    parser.add_argument("--gamma", type=float, default=0.99)
    parser.add_argument("--clip-range", type=float, default=0.2)
    parser.add_argument("--clip-end", type=float, default=0.1, help="Final clip range for schedule")
    parser.add_argument("--clip-schedule", choices=["none", "linear", "cosine"], default="none")
    parser.add_argument("--gae-lambda", type=float, default=0.95)
    parser.add_argument("--vf-coef", type=float, default=0.5)
    parser.add_argument("--max-grad-norm", type=float, default=0.5)
    parser.add_argument("--ent-coef", type=float, default=0.01)
    # Normalize advantage toggles
    parser.add_argument("--normalize-advantage", dest="normalize_advantage", action="store_true")
    parser.add_argument("--no-normalize-advantage", dest="normalize_advantage", action="store_false")
    parser.set_defaults(normalize_advantage=True)
    parser.add_argument("--device", default="auto", help="Device to use for training (auto, cpu, mps, cuda)")

    # Config and profiles
    parser.add_argument("--config", type=str, help="Path to hyperparameters JSON config")
    parser.add_argument("--profile", choices=["none", "research_plan", "quick_debug", "baseline_sb3"], default="none")

    # VecNormalize options
    parser.add_argument("--vecnorm", action="store_true", help="Enable VecNormalize wrapper")
    parser.add_argument("--vecnorm-norm-obs", dest="vecnorm_norm_obs", action="store_true")
    parser.add_argument("--no-vecnorm-norm-obs", dest="vecnorm_norm_obs", action="store_false")
    parser.set_defaults(vecnorm_norm_obs=True)
    parser.add_argument("--vecnorm-norm-reward", dest="vecnorm_norm_reward", action="store_true")
    parser.add_argument("--no-vecnorm-norm-reward", dest="vecnorm_norm_reward", action="store_false")
    parser.set_defaults(vecnorm_norm_reward=False)
    parser.add_argument("--vecnorm-clip-obs", type=float, default=10.0)
    parser.add_argument("--vecnorm-clip-reward", type=float, default=10.0)

    # Logging / checkpoints
    parser.add_argument("--log-dir", default="runs/ppo")
    parser.add_argument("--save-path", default="checkpoints/ppo_model")
    parser.add_argument("--save-freq", type=int, default=50000, help="Timesteps between checkpoints (0=disable)")
    parser.add_argument("--eval-freq", type=int, default=10000, help="Timesteps between evals (0=disable)")
    parser.add_argument("--eval-episodes", type=int, default=20)
    # Monitoring / logging toggles
    parser.add_argument("--monitor-tensorboard", dest="monitor_tensorboard", action="store_true")
    parser.add_argument("--no-monitor-tensorboard", dest="monitor_tensorboard", action="store_false")
    parser.set_defaults(monitor_tensorboard=True)
    parser.add_argument("--monitor-wandb", action="store_true")
    parser.add_argument("--wandb-project", type=str, default=None)
    parser.add_argument("--wandb-entity", type=str, default=None)
    parser.add_argument("--wandb-run-name", type=str, default=None)
    parser.add_argument("--wandb-tags", type=str, nargs="*", default=None)
    parser.add_argument("--eval-fixedset", type=str, default=None, help="Path to fixed test set JSON")
    parser.add_argument("--solver-cache", type=str, default="eval/solver_cache.json")
    parser.add_argument("--traj-record-freq", type=int, default=50000)
    parser.add_argument("--traj-record-episodes", type=int, default=5)
    # Model options
    parser.add_argument("--small-cnn", action="store_true", help="Use custom SmallCNN feature extractor for image obs")
    parser.add_argument("--convlstm", action="store_true", help="Use ConvLSTM feature extractor for image obs")
    parser.add_argument("--lstm-channels", type=int, default=64, help="ConvLSTM hidden channels")
    parser.add_argument("--lstm-layers", type=int, default=2, help="Number of ConvLSTM layers")
    parser.add_argument("--lstm-repeats", type=int, default=1, help="Number of repeats per ConvLSTM timestep")

    args = parser.parse_args()

    # Apply profile presets (can be overridden by config/CLI later)
    if args.profile != "none":
        if args.profile == "research_plan":
            # Long-run, plan-aligned defaults
            args.timesteps = 5_000_000 if args.timesteps == 100000 else args.timesteps
            args.clip_range = 0.2
            args.clip_end = 0.05
            args.clip_schedule = "linear"
            args.gae_lambda = 0.95
            args.vf_coef = 0.5
            args.max_grad_norm = 0.5
            args.lr = 3e-4
            args.lr_end = 3e-5
            args.lr_schedule = "linear"
            args.normalize_advantage = True
            # Sparse reward: avoid reward normalization by default
            if not hasattr(args, 'vecnorm') or not args.vecnorm:
                args.vecnorm = False
                args.vecnorm_norm_obs = True
                args.vecnorm_norm_reward = False
        elif args.profile == "quick_debug":
            args.timesteps = 50_000
            args.lr = 1e-3
            args.lr_end = 1e-3
            args.lr_schedule = "none"
            args.clip_range = 0.2
            args.clip_end = 0.2
            args.clip_schedule = "none"
            args.vf_coef = 0.5
            args.max_grad_norm = 0.5
            args.normalize_advantage = True
            args.vecnorm = False
        elif args.profile == "baseline_sb3":
            # Mirror common SB3-ish defaults while keeping our script params
            args.lr = 3e-4
            args.lr_schedule = "none"
            args.clip_range = 0.2
            args.clip_schedule = "none"
            args.gae_lambda = 0.95
            args.vf_coef = 0.5
            args.max_grad_norm = 0.5
            args.normalize_advantage = True
            args.vecnorm = False

    # Load config JSON and overlay into args (CLI wins if explicitly set differently)
    if args.config:
        try:
            with open(args.config, 'r', encoding='utf-8') as f:
                cfg = json.load(f)
            # Flat key overlay: only set attributes present in args
            for k, v in cfg.items():
                if hasattr(args, k):
                    setattr(args, k, v)
        except (OSError, json.JSONDecodeError) as e:
            print(f"Warning: Failed to load config {args.config}: {e}")

    # Initialize profiling if enabled
    if args.enable_profiling:
        try:
            from profiling import get_profiler, print_profiling_summary, save_profiling_report
            profiler = get_profiler()
            profiler.enable()
            print("Profiling enabled - detailed performance tracking active")
        except ImportError:
            print("Warning: Profiling requested but profiling module not available")
            args.enable_profiling = False

    # Device detection for M1/M2 Macs
    if args.device == "auto":
        import torch
        if torch.backends.mps.is_available():
            args.device = "mps"
            print("Using MPS (Metal Performance Shaders) for GPU acceleration on M1/M2 Mac")
        elif torch.cuda.is_available():
            args.device = "cuda"
            print("Using CUDA for GPU acceleration")
        else:
            args.device = "cpu"
            print("Using CPU for training")
    else:
        print(f"Using specified device: {args.device}")

    # Performance knobs per backend
    try:
        import torch
    except ImportError:
        torch = None  # type: ignore
    if torch is not None and args.device == "cuda" and torch.cuda.is_available():
        torch.backends.cudnn.benchmark = True
        # Prefer TF32/matmul medium precision where beneficial
        try:
            torch.set_float32_matmul_precision("high")
        except AttributeError:
            pass
    # MPS backend does not expose cudnn knobs; keep defaults

    # Lazy import SB3 to avoid hard dependency for env users
    import importlib
    sb3 = importlib.import_module("stable_baselines3")
    PPO = sb3.PPO
    # Try to import sb3-contrib RecurrentPPO
    try:
        sb3c = importlib.import_module("sb3_contrib")
        RecurrentPPO = sb3c.RecurrentPPO  # type: ignore[attr-defined]
    except Exception:
        RecurrentPPO = None  # type: ignore
    vec_env_mod = importlib.import_module("stable_baselines3.common.env_util")
    make_vec_env = vec_env_mod.make_vec_env

    os.makedirs(args.log_dir, exist_ok=True)
    os.makedirs(os.path.dirname(args.save_path), exist_ok=True)

    # Choose environment factory based on curriculum setting
    curriculum_manager = None
    if args.curriculum:
        env_factory, curriculum_manager = make_curriculum_env_factory(args)
        print("Curriculum learning enabled - will progressively increase difficulty")
    else:
        env_factory = make_env_factory(args)
        print("Standard training mode - fixed difficulty")

    vec_env = make_vec_env(env_factory, n_envs=args.n_envs, seed=args.seed)

    # Optional VecNormalize wrapper
    try:
        from stable_baselines3.common.vec_env import VecNormalize as _VecNormalize
    except ImportError:
        _VecNormalize = None  # type: ignore
    vecnorm_used = False
    if args.vecnorm and _VecNormalize is not None:
        vec_env = _VecNormalize(
            vec_env,
            training=True,
            norm_obs=args.vecnorm_norm_obs,
            norm_reward=args.vecnorm_norm_reward,
            clip_obs=args.vecnorm_clip_obs,
            clip_reward=args.vecnorm_clip_reward,
        )
        vecnorm_used = True

    # Choose policy: for tiny grids (<8), prefer MlpPolicy to avoid NatureCNN kernel issues
    tiny_grid = False
    if args.env_mode in ("v0", "v1"):
        tiny_grid = True
    else:
        tiny_grid = min(args.height, args.width) < 8
    
    # Validate observation mode and policy compatibility
    if args.obs_mode == "symbolic" and args.convlstm:
        raise ValueError("ConvLSTM requires image observations, but symbolic mode was selected")
    
    # Policy selection based on observation mode and requirements
    if args.obs_mode in ["image", "rgb_image"]:
        if args.convlstm:
            # Until a compliant recurrent policy wired to ConvLSTM features is implemented,
            # train with sb3-contrib RecurrentPPO using its supported LSTM policies.
            # This avoids incorrect training of recurrent models with vanilla PPO.
            if RecurrentPPO is None:
                raise ImportError("sb3-contrib is required for recurrent PPO. Please install sb3-contrib.")
            policy = "CnnLstmPolicy"
            policy_kwargs = dict(
                net_arch=dict(pi=[128, 128], vf=[128, 128]),
                normalize_images=(args.obs_mode == "rgb_image")
            )
        elif not tiny_grid:
            # Fallback to CNN for image observations
            policy = "CnnPolicy"
            policy_kwargs = dict(
                net_arch=dict(pi=[128, 128], vf=[128, 128]), 
                normalize_images=(args.obs_mode == "rgb_image")  # Normalize RGB images
            )
            if SmallCNN is not None and args.small_cnn:
                policy_kwargs.update(dict(features_extractor_class=SmallCNN, features_extractor_kwargs=dict(features_dim=128)))
            elif not args.convlstm:
                print("Warning: Using CNN instead of ConvLSTM for image observations. Use --convlstm for ConvLSTM.")
        else:
            # Use MLP for very small grids
            policy = "MlpPolicy"
            policy_kwargs = dict(net_arch=dict(pi=[128, 128], vf=[128, 128]), normalize_images=False)
    elif args.obs_mode == "symbolic":
        # Use MLP for symbolic observations
        policy = "MlpPolicy"
        policy_kwargs = dict(net_arch=dict(pi=[128, 128], vf=[128, 128]), normalize_images=False)
    else:
        raise ValueError(f"Unknown observation mode: {args.obs_mode}")

    # Schedule factories for LR and clip_range
    def _make_schedule(kind: str, start: float, end: float) -> Callable[[float], float] | float:
        if kind == "none":
            return start
        if kind == "linear":
            # progress_remaining: 1 -> 0
            return lambda progress_remaining: end + (start - end) * progress_remaining
        if kind == "cosine":
            # Cosine decay from start to end over progress_remaining
            def _fn(progress_remaining: float) -> float:
                t = 1.0 - progress_remaining  # 0->1
                w = 0.5 * (1 + math.cos(math.pi * t))
                return end + (start - end) * w
            return _fn
        return start

    lr_value: Callable[[float], float] | float = _make_schedule(args.lr_schedule, args.lr, args.lr_end)
    clip_value: Callable[[float], float] | float = _make_schedule(args.clip_schedule, args.clip_range, args.clip_end)

    # Initialize algorithm
    if args.convlstm:
        # Use sb3-contrib RecurrentPPO for recurrent training
        model = RecurrentPPO(  # type: ignore[operator]
            policy,
            vec_env,
            learning_rate=lr_value,
            n_steps=args.n_steps,
            batch_size=args.batch_size,
            n_epochs=args.n_epochs,
            gamma=args.gamma,
            clip_range=clip_value,
            gae_lambda=args.gae_lambda,
            vf_coef=args.vf_coef,
            max_grad_norm=args.max_grad_norm,
            ent_coef=args.ent_coef,
            policy_kwargs=policy_kwargs,
            normalize_advantage=args.normalize_advantage,
            tensorboard_log=args.log_dir,
            device=args.device,
            verbose=1,
        )
    else:
        model = PPO(
            policy,
            vec_env,
            learning_rate=lr_value,
            n_steps=args.n_steps,
            batch_size=args.batch_size,
            n_epochs=args.n_epochs,
            gamma=args.gamma,
            clip_range=clip_value,
            gae_lambda=args.gae_lambda,
            vf_coef=args.vf_coef,
            max_grad_norm=args.max_grad_norm,
            ent_coef=args.ent_coef,
            policy_kwargs=policy_kwargs,
            normalize_advantage=args.normalize_advantage,
            tensorboard_log=args.log_dir,
            device=args.device,
            verbose=1,
        )

    # Callbacks: periodic checkpoint and eval success-rate logging + monitoring hub
    callbacks = []
    # Monitoring hub setup
    try:
        from monitoring import MonitoringHub, TensorBoardBackend, WandbBackend
        from monitoring.collectors import EpisodeStatsCollector, ActionUsageCollector, CurriculumProgressCollector
        monitoring_backends = []
        if args.monitor_tensorboard:
            try:
                monitoring_backends.append(TensorBoardBackend(args.log_dir))
            except ImportError:
                print("Warning: TensorBoard not available; disabling TB backend")
        if args.monitor_wandb:
            try:
                monitoring_backends.append(WandbBackend(project=(args.wandb_project or "ricochet-robots"), entity=args.wandb_entity, run_name=args.wandb_run_name, tags=args.wandb_tags, config={"args": vars(args)}))
            except ImportError:
                print("Warning: wandb requested but not available; skipping")
        hub = MonitoringHub(backends=monitoring_backends, collectors=[
            EpisodeStatsCollector(window_size=200),
            ActionUsageCollector(track_noop=True),
            CurriculumProgressCollector(),
        ])
    except ImportError:
        hub = None  # type: ignore
    try:
        from stable_baselines3.common.callbacks import CheckpointCallback, EvalCallback, BaseCallback
        from stable_baselines3.common.env_util import make_vec_env as _make
        
        # Custom callback for curriculum learning
        class CurriculumCallback(BaseCallback):
            def __init__(self, curriculum_manager, verbose=0):
                super().__init__(verbose)
                self.curriculum_manager = curriculum_manager
                self.last_logged_level = -1
                
            def _on_step(self) -> bool:
                # Log curriculum stats if available
                if self.curriculum_manager is not None:
                    stats = self.curriculum_manager.get_curriculum_stats()
                    
                    # Log to console
                    if self.verbose > 0 and self.num_timesteps % 1000 == 0:
                        print(f"Curriculum Level {stats['current_level']}: {stats['level_name']} "
                              f"(Success: {stats['success_rate']:.2f}, Episodes: {stats['episodes_at_level']})")
                    
                    # Log to TensorBoard
                    if hasattr(self, 'logger') and self.logger is not None:
                        self.logger.record("curriculum/level", stats['current_level'])
                        self.logger.record("curriculum/success_rate", stats['success_rate'])
                        self.logger.record("curriculum/episodes_at_level", stats['episodes_at_level'])
                        self.logger.record("curriculum/total_episodes", stats['total_episodes'])
                        self.logger.record("curriculum/window_size", stats['window_size'])
                        
                        # Log level advancement
                        if stats['current_level'] != self.last_logged_level:
                            self.logger.record("curriculum/level_advancement", stats['current_level'])
                            self.last_logged_level = stats['current_level']
                
                return True
        
        # Add curriculum callback if curriculum learning is enabled
        if args.curriculum and curriculum_manager is not None:
            curriculum_cb = CurriculumCallback(curriculum_manager=curriculum_manager, verbose=1)
            callbacks.append(curriculum_cb)
            # Also forward level changes to monitoring hub
            if hub is not None:
                class _HubCurriculumForward(BaseCallback):
                    def __init__(self, manager, verbose=0):
                        super().__init__(verbose)
                        self.manager = manager
                        self._last_level = None
                    def _on_step(self) -> bool:
                        lvl = self.manager.get_current_level()
                        if self._last_level != lvl:
                            stats = self.manager.get_curriculum_stats()
                            hub.on_level_change({
                                "global_step": self.num_timesteps,
                                "level_id": stats.get("current_level"),
                                "level_name": stats.get("level_name"),
                                "level_stats": stats,
                            })
                            self._last_level = lvl
                        return True
                callbacks.append(_HubCurriculumForward(curriculum_manager, verbose=0))
        
        # Add profiling callback if profiling is enabled
        if args.enable_profiling:
            class ProfilingCallback(BaseCallback):
                def __init__(self, summary_freq: int, verbose=0):
                    super().__init__(verbose)
                    self.summary_freq = summary_freq
                    self.last_summary = 0
                    
                def _on_step(self) -> bool:
                    # Print profiling summary periodically
                    if self.num_timesteps - self.last_summary >= self.summary_freq:
                        if self.verbose > 0:
                            print(f"\n--- Profiling Summary at {self.num_timesteps} timesteps ---")
                            print_profiling_summary(sort_by='total_time')
                        self.last_summary = self.num_timesteps
                    return True
                
                def _on_training_end(self) -> None:
                    # Save final profiling report
                    if hasattr(self, 'model') and self.model is not None:
                        save_profiling_report(args.profiling_report)
                        print(f"Final profiling report saved to {args.profiling_report}")
            
            profiling_cb = ProfilingCallback(summary_freq=args.profiling_summary_freq, verbose=1)
            callbacks.append(profiling_cb)
        
        # Remove ad-hoc recurrent callback; sb3-contrib handles sequence masks internally
        
        # Wrap separate eval env
        if args.eval_freq > 0:
            # When curriculum is enabled, mirror the training env at the current level
            if args.curriculum and curriculum_manager is not None:
                # Freeze eval to current level to keep observation space consistent
                current_level = curriculum_manager.get_current_level()
                frozen_cfg = copy.deepcopy(curriculum_manager.config)
                frozen_cfg.max_level = current_level
                # Build a dedicated eval curriculum manager starting at the frozen level
                frozen_manager = create_curriculum_manager(
                    curriculum_config=frozen_cfg,
                    initial_level=current_level,
                    verbose=args.curriculum_verbose,
                )
                # Create an eval env factory using the frozen curriculum
                def _eval_env_factory():
                    return create_curriculum_wrapper(
                        base_env_factory=make_env_factory(args),
                        curriculum_manager=frozen_manager,
                        verbose=args.curriculum_verbose,
                    )
                eval_env = _make(_eval_env_factory, n_envs=1, seed=args.seed)
            else:
                # Standard (non-curriculum) eval env
                eval_env = _make(make_env_factory(args), n_envs=1, seed=args.seed)

            # If using VecNormalize in training, wrap eval env and sync stats
            if vecnorm_used and _VecNormalize is not None:
                eval_env = _VecNormalize(
                    eval_env,
                    training=False,
                    norm_obs=args.vecnorm_norm_obs,
                    norm_reward=args.vecnorm_norm_reward,
                    clip_obs=args.vecnorm_clip_obs,
                    clip_reward=args.vecnorm_clip_reward,
                )
                # Sync running stats
                if hasattr(vec_env, 'obs_rms'):
                    eval_env.obs_rms = vec_env.obs_rms  # type: ignore[attr-defined]
                if hasattr(vec_env, 'ret_rms'):
                    eval_env.ret_rms = vec_env.ret_rms  # type: ignore[attr-defined]
            eval_cb = EvalCallback(eval_env, best_model_save_path=os.path.dirname(args.save_path),
                                   log_path=args.log_dir, eval_freq=args.eval_freq,
                                   n_eval_episodes=args.eval_episodes, deterministic=True, render=False)
            callbacks.append(eval_cb)
            # Add fixed-set evaluation with solver-aware optimality gap via monitoring hub if configured
            if hub is not None and args.eval_fixedset is not None:
                try:
                    import os as _os
                    from monitoring.evaluators import FixedSetEvaluator, OptimalityGapEvaluator, SolverCache, TrajectoryRecorder
                    # Load fixed set
                    with open(args.eval_fixedset, 'r', encoding='utf-8') as f:
                        fixedset = json.load(f)
                    # Factories to build env from fixed layout dict
                    def _make_env_from_layout(layout_dict: dict):
                        from env import RicochetRobotsEnv, FixedLayout
                        import numpy as _np
                        h = int(layout_dict["height"]) ; w = int(layout_dict["width"]) ; tr = int(layout_dict["target_robot"]) ; robots = {int(k): tuple(v) for k, v in layout_dict["robot_positions"].items()}
                        h_walls = _np.array(layout_dict["h_walls"], dtype=bool)
                        v_walls = _np.array(layout_dict["v_walls"], dtype=bool)
                        goal = tuple(layout_dict["goal_position"])  # type: ignore
                        fl = FixedLayout(height=h, width=w, h_walls=h_walls, v_walls=v_walls, robot_positions=robots, goal_position=goal, target_robot=tr)
                        return RicochetRobotsEnv(fixed_layout=fl, obs_mode=args.obs_mode, channels_first=True)
                    solver_cache = SolverCache.load(args.solver_cache)
                    fixed_eval = FixedSetEvaluator(fixedset=fixedset, episodes=args.eval_episodes)
                    gap_eval = OptimalityGapEvaluator(fixedset=fixedset, solver_cache=solver_cache, solver_limits={"max_depth": args.solver_max_depth, "max_nodes": args.solver_max_nodes}, skip_unsolved=True)

                    class _HubEvalForward(BaseCallback):
                        def __init__(self, verbose=0):
                            super().__init__(verbose)
                            self._last_traj_dump = 0
                        def _on_step(self) -> bool:
                            if self.num_timesteps % args.eval_freq == 0:
                                fixed_eval.evaluate(hub, _make_env_from_layout, self.model, step=self.num_timesteps)
                                gap_eval.evaluate(hub, _make_env_from_layout, self.model, step=self.num_timesteps)
                            # Trajectory recorder on its own cadence using a standard training env factory
                            if args.traj_record_freq > 0 and (self.num_timesteps - self._last_traj_dump) >= args.traj_record_freq:
                                rec = TrajectoryRecorder(episodes=args.traj_record_episodes)
                                # Use a fresh non-curriculum single-env factory mirroring current args
                                rec_env_factory = make_env_factory(args)
                                trajectories = rec.record(rec_env_factory, self.model)
                                hub.log_text("eval/trajectories/sample", json.dumps({"num": len(trajectories)}), self.num_timesteps)
                                self._last_traj_dump = self.num_timesteps
                            return True
                    callbacks.append(_HubEvalForward(verbose=0))
                except (OSError, json.JSONDecodeError) as e:
                    print(f"Warning: Failed to configure fixed-set evaluator: {e}")
        if args.save_freq > 0:
            ckpt_cb = CheckpointCallback(save_freq=args.save_freq, save_path=os.path.dirname(args.save_path),
                                         name_prefix=os.path.basename(args.save_path))
            callbacks.append(ckpt_cb)
    except ImportError:
        pass

    # Wrap training with profiling
    if args.enable_profiling:
        try:
            from profiling import profile
            with profile("training_total", track_memory=True):
                model.learn(total_timesteps=args.timesteps, callback=callbacks if callbacks else None)
        except ImportError:
            model.learn(total_timesteps=args.timesteps, callback=callbacks if callbacks else None)
    else:
        model.learn(total_timesteps=args.timesteps, callback=callbacks if callbacks else None)
    
    model.save(args.save_path)
    # Save VecNormalize statistics if used
    if vecnorm_used and _VecNormalize is not None:
        try:
            vecnorm_path = os.path.join(args.log_dir, "vecnormalize.pkl")
            vec_env.save(vecnorm_path)  # type: ignore[attr-defined]
            print(f"Saved VecNormalize statistics to {vecnorm_path}")
        except (AttributeError, OSError) as e:
            print(f"Warning: Failed to save VecNormalize stats: {e}")
    
    # Print final profiling summary if enabled
    if args.enable_profiling:
        try:
            print("\n" + "="*80)
            print("FINAL PROFILING SUMMARY")
            print("="*80)
            print_profiling_summary(sort_by='total_time')
            save_profiling_report(args.profiling_report)
            print(f"Final profiling report saved to {args.profiling_report}")
        except ImportError:
            pass

    # Close monitoring hub
    try:
        if hub is not None:
            hub.close()
    except Exception:
        pass


if __name__ == "__main__":
    main()



```

```python
from __future__ import annotations

import argparse

from env import RicochetRobotsEnv, fixed_layout_v0_one_move, fixed_layouts_v1_four_targets


def make_single_env(args: argparse.Namespace) -> RicochetRobotsEnv:
    if args.env_mode == "v0":
        layout = fixed_layout_v0_one_move()
        return RicochetRobotsEnv(fixed_layout=layout, obs_mode=args.obs_mode, channels_first=True)
    if args.env_mode == "v1":
        layouts = fixed_layouts_v1_four_targets()
        return RicochetRobotsEnv(fixed_layout=layouts[0], obs_mode=args.obs_mode, channels_first=True)
    return RicochetRobotsEnv(height=args.height, width=args.width, num_robots=args.num_robots, obs_mode=args.obs_mode, channels_first=True)


def main() -> None:
    parser = argparse.ArgumentParser(description="Evaluate a trained PPO agent")
    parser.add_argument("--model-path", required=True)
    parser.add_argument("--env-mode", choices=["random", "v0", "v1"], default="random")
    parser.add_argument("--obs-mode", choices=["image", "symbolic"], default="image")
    parser.add_argument("--recurrent", action="store_true", help="Load sb3-contrib RecurrentPPO model")
    parser.add_argument("--episodes", type=int, default=50)
    parser.add_argument("--height", type=int, default=8)
    parser.add_argument("--width", type=int, default=8)
    parser.add_argument("--num-robots", type=int, default=2)
    args = parser.parse_args()

    import importlib
    PPO = importlib.import_module("stable_baselines3").PPO  # type: ignore[attr-defined]
    # Try to import RecurrentPPO for recurrent models
    try:
        RecurrentPPO = importlib.import_module("sb3_contrib").RecurrentPPO  # type: ignore[attr-defined]
    except Exception:
        RecurrentPPO = None  # type: ignore

    env = make_single_env(args)
    policy_type = "CnnPolicy" if args.obs_mode == "image" else "MlpPolicy"
    if args.recurrent:
        if RecurrentPPO is None:
            raise ImportError("sb3-contrib is required to load a RecurrentPPO model.")
        model = RecurrentPPO.load(args.model_path)  # type: ignore[assignment]
    else:
        model = PPO.load(args.model_path)

    success = 0
    lengths = []
    for _ in range(args.episodes):
        obs, info = env.reset()
        done = False
        trunc = False
        steps = 0
        lstm_state = None
        episode_starts = True
        while not (done or trunc):
            if args.recurrent:
                action, lstm_state = model.predict(obs, state=lstm_state, episode_start=episode_starts, deterministic=True)  # type: ignore[arg-type]
            else:
                action, _ = model.predict(obs, deterministic=True)
            obs, _, done, trunc, info = env.step(int(action))
            episode_starts = done or trunc
            steps += 1
        if info.get("is_success", False):
            success += 1
        lengths.append(steps)

    rate = success / args.episodes
    avg_len = sum(lengths) / max(1, len(lengths))
    print(f"Success rate: {rate:.3f} ({success}/{args.episodes}), avg length: {avg_len:.2f}")


if __name__ == "__main__":
    main()



```

- I’ve switched the training to use sb3-contrib’s RecurrentPPO when `--convlstm` is requested, and removed the ad-hoc recurrent callback.
- I updated the evaluation script to load and roll out RecurrentPPO models with LSTM state and `episode_start` handling. I also added `sb3-contrib` to `requirements.txt`.

Summary
- Edited `train_agent.py` to import `sb3-contrib` RecurrentPPO, choose it when `--convlstm` is set, and initialize with `CnnLstmPolicy`. Removed the placeholder `RecurrentCallback`.
- Left the custom `models/recurrent_policy.py` unused for training for now; we can revisit when implementing a compliant policy with a sequence-aware buffer.
- Edited `evaluate_agent.py` to add `--recurrent` flag, load `RecurrentPPO` if set, and manage `lstm_state` and `episode_start` per step.
- Updated `requirements.txt` to include `sb3-contrib>=2.3.0`.

Next, I can fix the action distribution bug and API compliance in `models/recurrent_policy.py` or continue with the next feedback item.

---

**User**

Yes, please go ahead and continue implementing the next piece of feedback, 'Action distribution bug in the recurrent policy'

---

**Cursor**

I'll fix the CategoricalDistribution construction in `models/recurrent_policy.py` and ensure action dtypes are correct when computing log-probs. Then I'll run a quick lint check on the file.

```python
from __future__ import annotations

import torch
import torch.nn as nn
from typing import Dict, List, Optional, Tuple, Any
from stable_baselines3.common.policies import BasePolicy
from stable_baselines3.common.torch_layers import BaseFeaturesExtractor
from stable_baselines3.common.distributions import Distribution, CategoricalDistribution
from stable_baselines3.common.utils import get_device
from gymnasium.spaces import Box, Discrete


class RecurrentActorCriticPolicy(BasePolicy):
    """
    Custom recurrent policy that properly handles hidden states across time steps.
    
    This policy maintains hidden states internally and provides them to the feature extractor
    on each forward pass, enabling true recurrent behavior for planning tasks.
    """
    
    def __init__(
        self,
        observation_space: Box,
        action_space: Discrete,
        lr_schedule: callable,
        features_extractor_class: type[BaseFeaturesExtractor],
        features_extractor_kwargs: Optional[Dict[str, Any]] = None,
        net_arch: Optional[List[int]] = None,
        activation_fn: type[nn.Module] = nn.ReLU,
        ortho_init: bool = True,
        use_sde: bool = False,
        log_std_init: float = 0.0,
        full_std: bool = True,
        use_expln: bool = False,
        squash_output: bool = False,
        features_dim: int = 128,
        normalize_images: bool = True,
        initial_std: float = 1.0,
        **kwargs
    ):
        super().__init__(
            observation_space,
            action_space,
            lr_schedule,
            **kwargs
        )
        
        # Set essential attributes
        self.features_extractor_class = features_extractor_class
        self.features_extractor_kwargs = features_extractor_kwargs or {}
        self.net_arch = net_arch
        self.activation_fn = activation_fn
        self.features_dim = features_dim
        self.ortho_init = ortho_init
        self.normalize_images = normalize_images
        # Store unused-but-standard args to avoid lint warnings and for future use
        self._use_sde = use_sde
        self._log_std_init = log_std_init
        self._full_std = full_std
        self._use_expln = use_expln
        self._initial_std = initial_std
        
        # Initialize hidden states storage
        self._hidden_states: Optional[List[Tuple[torch.Tensor, torch.Tensor]]] = None
        self._batch_size: Optional[int] = None
        self._device = get_device("auto")
        
        # Create feature extractor first
        self.features_extractor = self.features_extractor_class(
            self.observation_space, 
            **self.features_extractor_kwargs
        )
        
        # Get the actual features_dim from the feature extractor
        self.features_dim = self.features_extractor._features_dim
        
        # Build the action and value networks
        self._build_networks()
        
        # Initialize optimizer (required by SB3)
        self.optimizer = torch.optim.Adam(self.parameters(), lr=lr_schedule(1))
        
    def _preprocess_obs(self, obs: torch.Tensor) -> torch.Tensor:
        """Convert observations to float in [0,1] when normalize_images is enabled."""
        if self.normalize_images:
            if obs.dtype == torch.uint8:
                obs = obs.float().div_(255.0)
            else:
                # Ensure float32 dtype for convs and autocast stability
                obs = obs.float()
        return obs

    def _build_networks(self):
        """Build the actor and critic networks."""
        # Extract network architecture
        pi_arch = self.net_arch.get('pi', [128, 128])
        vf_arch = self.net_arch.get('vf', [128, 128])
        
        # Action network (policy head) - simple 2-layer MLP
        self.action_net = nn.Sequential(
            nn.Linear(self.features_dim, pi_arch[0]),
            self.activation_fn(),
            nn.Linear(pi_arch[0], pi_arch[1]),
            self.activation_fn(),
            nn.Linear(pi_arch[1], self.action_space.n)
        )
        
        # Value network (critic head) - simple 2-layer MLP
        self.value_net = nn.Sequential(
            nn.Linear(self.features_dim, vf_arch[0]),
            self.activation_fn(),
            nn.Linear(vf_arch[0], vf_arch[1]),
            self.activation_fn(),
            nn.Linear(vf_arch[1], 1)
        )
        
        # Initialize weights
        if self.ortho_init:
            self._init_weights()
    
    def _init_weights(self):
        """Initialize network weights using orthogonal initialization."""
        for module in self.modules():
            if isinstance(module, nn.Linear):
                nn.init.orthogonal_(module.weight, gain=1.0)
                if module.bias is not None:
                    nn.init.constant_(module.bias, 0.0)
    
    def _init_hidden_states(self, _batch_size: int, _device: torch.device) -> List[Tuple[torch.Tensor, torch.Tensor]]:
        """Initialize hidden states for the recurrent feature extractor."""
        if not hasattr(self.features_extractor, 'convlstm'):
            return []
        
        # We need to know the spatial dimensions, but we don't have them yet
        # Return None and let the feature extractor handle initialization
        return None
    
    def _get_hidden_states(self, batch_size: int) -> List[Tuple[torch.Tensor, torch.Tensor]]:
        """Get or initialize hidden states for the current batch."""
        if self._hidden_states is None or self._batch_size != batch_size:
            self._hidden_states = self._init_hidden_states(batch_size, self._device)
            self._batch_size = batch_size
        
        return self._hidden_states
    
    def reset_hidden_states(self, batch_size: Optional[int] = None):
        """Reset hidden states to zero."""
        if batch_size is not None:
            self._batch_size = batch_size
        self._hidden_states = None
    
    def forward(
        self, 
        obs: torch.Tensor, 
        deterministic: bool = False
    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
        """
        Forward pass through the recurrent policy.
        
        Args:
            obs: Observation tensor (B, C, H, W) or (B, features_dim)
            deterministic: Whether to use deterministic action selection
            
        Returns:
            action: Selected action
            value: State value estimate
            log_prob: Log probability of the selected action
        """
        # Import profiling here to avoid circular imports
        try:
            from profiling import profile
        except ImportError:
            # Fallback if profiling not available
            def profile(_name, _track_memory=True):
                from contextlib import nullcontext
                return nullcontext()
        
        with profile("recurrent_policy_forward", track_memory=True):
            obs = self._preprocess_obs(obs)
            batch_size = obs.shape[0]
            
            # Get current hidden states
            with profile("recurrent_policy_get_hidden_states", track_memory=True):
                hidden_states = self._get_hidden_states(batch_size)
            
            # Extract features using the recurrent feature extractor
            if hasattr(self.features_extractor, 'convlstm'):
                # Recurrent feature extractor - pass hidden states
                features, new_hidden_states = self.features_extractor(obs, hidden_states)
                # Update stored hidden states (detach to prevent gradient issues)
                self._hidden_states = [(h.detach(), c.detach()) for h, c in new_hidden_states]
            else:
                # Non-recurrent feature extractor
                features = self.features_extractor(obs)
            
            # Compute action logits and value
            with profile("recurrent_policy_action_value", track_memory=True):
                action_logits = self.action_net(features)
                value = self.value_net(features).squeeze(-1)
            
            # Create action distribution
            with profile("recurrent_policy_action_dist", track_memory=True):
                action_dist = self._get_action_dist_from_latent(action_logits)
            
            # Sample action
            with profile("recurrent_policy_action_sampling", track_memory=True):
                if deterministic:
                    action = action_dist.mode()
                else:
                    action = action_dist.sample()
                
                # Compute log probability
                log_prob = action_dist.log_prob(action)
            
            return action, value, log_prob
    
    def _get_action_dist_from_latent(self, latent_pi: torch.Tensor) -> Distribution:
        """Create a categorical action distribution from policy logits."""
        dist = CategoricalDistribution(self.action_space.n)
        dist.proba_distribution(action_logits=latent_pi)
        return dist
    
    def evaluate_actions(
        self, 
        obs: torch.Tensor, 
        actions: torch.Tensor
    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
        """
        Evaluate actions for training.
        
        Args:
            obs: Observation tensor
            actions: Action tensor
            
        Returns:
            value: State value estimate
            log_prob: Log probability of actions
            entropy: Entropy of action distribution
        """
        # Import profiling here to avoid circular imports
        try:
            from profiling import profile
        except ImportError:
            # Fallback if profiling not available
            def profile(_name, _track_memory=True):
                from contextlib import nullcontext
                return nullcontext()
        
        with profile("recurrent_policy_evaluate_actions", track_memory=True):
            obs = self._preprocess_obs(obs)
            batch_size = obs.shape[0]
            
            # Get current hidden states
            hidden_states = self._get_hidden_states(batch_size)
            
            # Extract features
            if hasattr(self.features_extractor, 'convlstm'):
                features, new_hidden_states = self.features_extractor(obs, hidden_states)
                # Update stored hidden states (detach to prevent gradient issues)
                self._hidden_states = [(h.detach(), c.detach()) for h, c in new_hidden_states]
            else:
                features = self.features_extractor(obs)
            
            # Compute action logits and value
            action_logits = self.action_net(features)
            value = self.value_net(features).squeeze(-1)
            
            # Create action distribution
            action_dist = self._get_action_dist_from_latent(action_logits)
            
            # Compute log probability and entropy
            if actions.dtype != torch.long:
                actions = actions.long()
            log_prob = action_dist.log_prob(actions)
            entropy = action_dist.entropy()
            
            return value, log_prob, entropy
    
    def get_hidden_states(self) -> Optional[List[Tuple[torch.Tensor, torch.Tensor]]]:
        """Get current hidden states for analysis."""
        return self._hidden_states
    
    def set_hidden_states(self, hidden_states: List[Tuple[torch.Tensor, torch.Tensor]]):
        """Set hidden states (useful for analysis and interventions)."""
        self._hidden_states = hidden_states
        if hidden_states:
            self._batch_size = hidden_states[0][0].shape[0]
    
    def _predict(self, observation: torch.Tensor, deterministic: bool = False) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        Predict action and value for a single observation.
        
        Args:
            observation: Single observation tensor
            deterministic: Whether to use deterministic action selection
            
        Returns:
            action: Selected action
            value: State value estimate
        """
        # Add batch dimension if needed
        if observation.dim() == 3:  # (C, H, W)
            observation = observation.unsqueeze(0)  # (1, C, H, W)
        
        observation = self._preprocess_obs(observation)
        action, value, _ = self.forward(observation, deterministic)
        
        # Remove batch dimension for single observation
        return action.squeeze(0), value.squeeze(0)
    
    def predict_values(self, obs: torch.Tensor) -> torch.Tensor:
        """
        Predict values for observations (used by SB3).
        
        Args:
            obs: Observation tensor
            
        Returns:
            values: State value estimates
        """
        # Import profiling here to avoid circular imports
        try:
            from profiling import profile
        except ImportError:
            # Fallback if profiling not available
            def profile(_name, _track_memory=True):
                from contextlib import nullcontext
                return nullcontext()
        
        with profile("recurrent_policy_predict_values", track_memory=True):
            obs = self._preprocess_obs(obs)
            batch_size = obs.shape[0]
            
            # Get current hidden states
            hidden_states = self._get_hidden_states(batch_size)
            
            # Extract features using the recurrent feature extractor
            if hasattr(self.features_extractor, 'convlstm'):
                # Recurrent feature extractor - pass hidden states
                features, new_hidden_states = self.features_extractor(obs, hidden_states)
                # Update stored hidden states (detach to prevent gradient issues)
                self._hidden_states = [(h.detach(), c.detach()) for h, c in new_hidden_states]
            else:
                # Non-recurrent feature extractor
                features = self.features_extractor(obs)
            
            # Compute value
            value = self.value_net(features).squeeze(-1)
            
            return value

```

Fixed the distribution construction in `models/recurrent_policy.py`:
- Use `CategoricalDistribution(self.action_space.n)` and call `proba_distribution(action_logits=latent_pi)`.
- Ensure `actions` are `torch.long` before `log_prob`.

This addresses the action distribution bug.

---

**User**

Okay, next piece of feedbacK!

---

**Cursor**

I'll add `get_distribution` to `models/recurrent_policy.py` to align with SB3's policy API and adjust usage to be consistent.

```python
from __future__ import annotations

import torch
import torch.nn as nn
from typing import Dict, List, Optional, Tuple, Any
from stable_baselines3.common.policies import BasePolicy
from stable_baselines3.common.torch_layers import BaseFeaturesExtractor
from stable_baselines3.common.distributions import Distribution, CategoricalDistribution
from stable_baselines3.common.utils import get_device
from gymnasium.spaces import Box, Discrete


class RecurrentActorCriticPolicy(BasePolicy):
    """
    Custom recurrent policy that properly handles hidden states across time steps.
    
    This policy maintains hidden states internally and provides them to the feature extractor
    on each forward pass, enabling true recurrent behavior for planning tasks.
    """
    
    def __init__(
        self,
        observation_space: Box,
        action_space: Discrete,
        lr_schedule: callable,
        features_extractor_class: type[BaseFeaturesExtractor],
        features_extractor_kwargs: Optional[Dict[str, Any]] = None,
        net_arch: Optional[List[int]] = None,
        activation_fn: type[nn.Module] = nn.ReLU,
        ortho_init: bool = True,
        use_sde: bool = False,
        log_std_init: float = 0.0,
        full_std: bool = True,
        use_expln: bool = False,
        squash_output: bool = False,
        features_dim: int = 128,
        normalize_images: bool = True,
        initial_std: float = 1.0,
        **kwargs
    ):
        super().__init__(
            observation_space,
            action_space,
            lr_schedule,
            **kwargs
        )
        
        # Set essential attributes
        self.features_extractor_class = features_extractor_class
        self.features_extractor_kwargs = features_extractor_kwargs or {}
        self.net_arch = net_arch
        self.activation_fn = activation_fn
        self.features_dim = features_dim
        self.ortho_init = ortho_init
        self.normalize_images = normalize_images
        # Store unused-but-standard args to avoid lint warnings and for future use
        self._use_sde = use_sde
        self._log_std_init = log_std_init
        self._full_std = full_std
        self._use_expln = use_expln
        self._initial_std = initial_std
        
        # Initialize hidden states storage
        self._hidden_states: Optional[List[Tuple[torch.Tensor, torch.Tensor]]] = None
        self._batch_size: Optional[int] = None
        self._device = get_device("auto")
        
        # Create feature extractor first
        self.features_extractor = self.features_extractor_class(
            self.observation_space, 
            **self.features_extractor_kwargs
        )
        
        # Get the actual features_dim from the feature extractor
        self.features_dim = self.features_extractor._features_dim
        
        # Build the action and value networks
        self._build_networks()
        
        # Initialize optimizer (required by SB3)
        self.optimizer = torch.optim.Adam(self.parameters(), lr=lr_schedule(1))
        
    def _preprocess_obs(self, obs: torch.Tensor) -> torch.Tensor:
        """Convert observations to float in [0,1] when normalize_images is enabled."""
        if self.normalize_images:
            if obs.dtype == torch.uint8:
                obs = obs.float().div_(255.0)
            else:
                # Ensure float32 dtype for convs and autocast stability
                obs = obs.float()
        return obs

    def _build_networks(self):
        """Build the actor and critic networks."""
        # Extract network architecture
        pi_arch = self.net_arch.get('pi', [128, 128])
        vf_arch = self.net_arch.get('vf', [128, 128])
        
        # Action network (policy head) - simple 2-layer MLP
        self.action_net = nn.Sequential(
            nn.Linear(self.features_dim, pi_arch[0]),
            self.activation_fn(),
            nn.Linear(pi_arch[0], pi_arch[1]),
            self.activation_fn(),
            nn.Linear(pi_arch[1], self.action_space.n)
        )
        
        # Value network (critic head) - simple 2-layer MLP
        self.value_net = nn.Sequential(
            nn.Linear(self.features_dim, vf_arch[0]),
            self.activation_fn(),
            nn.Linear(vf_arch[0], vf_arch[1]),
            self.activation_fn(),
            nn.Linear(vf_arch[1], 1)
        )
        
        # Initialize weights
        if self.ortho_init:
            self._init_weights()
    
    def _init_weights(self):
        """Initialize network weights using orthogonal initialization."""
        for module in self.modules():
            if isinstance(module, nn.Linear):
                nn.init.orthogonal_(module.weight, gain=1.0)
                if module.bias is not None:
                    nn.init.constant_(module.bias, 0.0)
    
    def _init_hidden_states(self, _batch_size: int, _device: torch.device) -> List[Tuple[torch.Tensor, torch.Tensor]]:
        """Initialize hidden states for the recurrent feature extractor."""
        if not hasattr(self.features_extractor, 'convlstm'):
            return []
        
        # We need to know the spatial dimensions, but we don't have them yet
        # Return None and let the feature extractor handle initialization
        return None
    
    def _get_hidden_states(self, batch_size: int) -> List[Tuple[torch.Tensor, torch.Tensor]]:
        """Get or initialize hidden states for the current batch."""
        if self._hidden_states is None or self._batch_size != batch_size:
            self._hidden_states = self._init_hidden_states(batch_size, self._device)
            self._batch_size = batch_size
        
        return self._hidden_states
    
    def reset_hidden_states(self, batch_size: Optional[int] = None):
        """Reset hidden states to zero."""
        if batch_size is not None:
            self._batch_size = batch_size
        self._hidden_states = None
    
    def forward(
        self, 
        obs: torch.Tensor, 
        deterministic: bool = False
    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
        """
        Forward pass through the recurrent policy.
        
        Args:
            obs: Observation tensor (B, C, H, W) or (B, features_dim)
            deterministic: Whether to use deterministic action selection
            
        Returns:
            action: Selected action
            value: State value estimate
            log_prob: Log probability of the selected action
        """
        # Import profiling here to avoid circular imports
        try:
            from profiling import profile
        except ImportError:
            # Fallback if profiling not available
            def profile(_name, _track_memory=True):
                from contextlib import nullcontext
                return nullcontext()
        
        with profile("recurrent_policy_forward", track_memory=True):
            obs = self._preprocess_obs(obs)
            batch_size = obs.shape[0]
            
            # Get current hidden states
            with profile("recurrent_policy_get_hidden_states", track_memory=True):
                hidden_states = self._get_hidden_states(batch_size)
            
            # Extract features using the recurrent feature extractor
            if hasattr(self.features_extractor, 'convlstm'):
                # Recurrent feature extractor - pass hidden states
                features, new_hidden_states = self.features_extractor(obs, hidden_states)
                # Update stored hidden states (detach to prevent gradient issues)
                self._hidden_states = [(h.detach(), c.detach()) for h, c in new_hidden_states]
            else:
                # Non-recurrent feature extractor
                features = self.features_extractor(obs)
            
            # Compute action logits and value
            with profile("recurrent_policy_action_value", track_memory=True):
                action_logits = self.action_net(features)
                value = self.value_net(features).squeeze(-1)
            
            # Create action distribution
            with profile("recurrent_policy_action_dist", track_memory=True):
                action_dist = self._get_action_dist_from_latent(action_logits)
            
            # Sample action
            with profile("recurrent_policy_action_sampling", track_memory=True):
                if deterministic:
                    action = action_dist.mode()
                else:
                    action = action_dist.sample()
                
                # Compute log probability
                log_prob = action_dist.log_prob(action)
            
            return action, value, log_prob

    def get_distribution(self, obs: torch.Tensor) -> Distribution:
        """SB3-compatible API: return the action distribution given observations."""
        obs = self._preprocess_obs(obs)
        batch_size = obs.shape[0]

        hidden_states = self._get_hidden_states(batch_size)

        if hasattr(self.features_extractor, 'convlstm'):
            features, new_hidden_states = self.features_extractor(obs, hidden_states)
            self._hidden_states = [(h.detach(), c.detach()) for h, c in new_hidden_states]
        else:
            features = self.features_extractor(obs)

        action_logits = self.action_net(features)
        return self._get_action_dist_from_latent(action_logits)
    
    def _get_action_dist_from_latent(self, latent_pi: torch.Tensor) -> Distribution:
        """Create a categorical action distribution from policy logits."""
        dist = CategoricalDistribution(self.action_space.n)
        dist.proba_distribution(action_logits=latent_pi)
        return dist
    
    def evaluate_actions(
        self, 
        obs: torch.Tensor, 
        actions: torch.Tensor
    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
        """
        Evaluate actions for training.
        
        Args:
            obs: Observation tensor
            actions: Action tensor
            
        Returns:
            value: State value estimate
            log_prob: Log probability of actions
            entropy: Entropy of action distribution
        """
        # Import profiling here to avoid circular imports
        try:
            from profiling import profile
        except ImportError:
            # Fallback if profiling not available
            def profile(_name, _track_memory=True):
                from contextlib import nullcontext
                return nullcontext()
        
        with profile("recurrent_policy_evaluate_actions", track_memory=True):
            obs = self._preprocess_obs(obs)
            batch_size = obs.shape[0]
            
            # Get current hidden states
            hidden_states = self._get_hidden_states(batch_size)
            
            # Extract features
            if hasattr(self.features_extractor, 'convlstm'):
                features, new_hidden_states = self.features_extractor(obs, hidden_states)
                # Update stored hidden states (detach to prevent gradient issues)
                self._hidden_states = [(h.detach(), c.detach()) for h, c in new_hidden_states]
            else:
                features = self.features_extractor(obs)
            
            # Compute action logits and value
            action_logits = self.action_net(features)
            value = self.value_net(features).squeeze(-1)
            
            # Create action distribution
            action_dist = self._get_action_dist_from_latent(action_logits)
            
            # Compute log probability and entropy
            if actions.dtype != torch.long:
                actions = actions.long()
            log_prob = action_dist.log_prob(actions)
            entropy = action_dist.entropy()
            
            return value, log_prob, entropy
    
    def get_hidden_states(self) -> Optional[List[Tuple[torch.Tensor, torch.Tensor]]]:
        """Get current hidden states for analysis."""
        return self._hidden_states
    
    def set_hidden_states(self, hidden_states: List[Tuple[torch.Tensor, torch.Tensor]]):
        """Set hidden states (useful for analysis and interventions)."""
        self._hidden_states = hidden_states
        if hidden_states:
            self._batch_size = hidden_states[0][0].shape[0]
    
    def _predict(self, observation: torch.Tensor, deterministic: bool = False) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        Predict action and value for a single observation.
        
        Args:
            observation: Single observation tensor
            deterministic: Whether to use deterministic action selection
            
        Returns:
            action: Selected action
            value: State value estimate
        """
        # Add batch dimension if needed
        if observation.dim() == 3:  # (C, H, W)
            observation = observation.unsqueeze(0)  # (1, C, H, W)
        
        observation = self._preprocess_obs(observation)
        action, value, _ = self.forward(observation, deterministic)
        
        # Remove batch dimension for single observation
        return action.squeeze(0), value.squeeze(0)
    
    def predict_values(self, obs: torch.Tensor) -> torch.Tensor:
        """
        Predict values for observations (used by SB3).
        
        Args:
            obs: Observation tensor
            
        Returns:
            values: State value estimates
        """
        # Import profiling here to avoid circular imports
        try:
            from profiling import profile
        except ImportError:
            # Fallback if profiling not available
            def profile(_name, _track_memory=True):
                from contextlib import nullcontext
                return nullcontext()
        
        with profile("recurrent_policy_predict_values", track_memory=True):
            obs = self._preprocess_obs(obs)
            batch_size = obs.shape[0]
            
            # Get current hidden states
            hidden_states = self._get_hidden_states(batch_size)
            
            # Extract features using the recurrent feature extractor
            if hasattr(self.features_extractor, 'convlstm'):
                # Recurrent feature extractor - pass hidden states
                features, new_hidden_states = self.features_extractor(obs, hidden_states)
                # Update stored hidden states (detach to prevent gradient issues)
                self._hidden_states = [(h.detach(), c.detach()) for h, c in new_hidden_states]
            else:
                # Non-recurrent feature extractor
                features = self.features_extractor(obs)
            
            # Compute value
            value = self.value_net(features).squeeze(-1)
            
            return value

```

- I added `get_distribution(obs)` to `models/recurrent_policy.py` to better align with SB3’s policy API. It runs the extractor, updates hidden state, computes logits, and returns a `CategoricalDistribution`.

Do you want me to proceed to the next feedback item (hidden state lifecycle and env vectorization)?

---

**User**

Good work - now lets move on to 'Hidden state lifecycle and env vectorization'

---

**Cursor**

```python
from __future__ import annotations

import torch
import torch.nn as nn
from typing import Dict, List, Optional, Tuple, Any
from stable_baselines3.common.policies import BasePolicy
from stable_baselines3.common.torch_layers import BaseFeaturesExtractor
from stable_baselines3.common.distributions import Distribution, CategoricalDistribution
from stable_baselines3.common.utils import get_device
from gymnasium.spaces import Box, Discrete


class RecurrentActorCriticPolicy(BasePolicy):
    """
    Custom recurrent policy that properly handles hidden states across time steps.
    
    This policy maintains hidden states internally and provides them to the feature extractor
    on each forward pass, enabling true recurrent behavior for planning tasks.
    """
    
    def __init__(
        self,
        observation_space: Box,
        action_space: Discrete,
        lr_schedule: callable,
        features_extractor_class: type[BaseFeaturesExtractor],
        features_extractor_kwargs: Optional[Dict[str, Any]] = None,
        net_arch: Optional[List[int]] = None,
        activation_fn: type[nn.Module] = nn.ReLU,
        ortho_init: bool = True,
        use_sde: bool = False,
        log_std_init: float = 0.0,
        full_std: bool = True,
        use_expln: bool = False,
        squash_output: bool = False,
        features_dim: int = 128,
        normalize_images: bool = True,
        initial_std: float = 1.0,
        **kwargs
    ):
        super().__init__(
            observation_space,
            action_space,
            lr_schedule,
            **kwargs
        )
        
        # Set essential attributes
        self.features_extractor_class = features_extractor_class
        self.features_extractor_kwargs = features_extractor_kwargs or {}
        self.net_arch = net_arch
        self.activation_fn = activation_fn
        self.features_dim = features_dim
        self.ortho_init = ortho_init
        self.normalize_images = normalize_images
        # Store unused-but-standard args to avoid lint warnings and for future use
        self._use_sde = use_sde
        self._log_std_init = log_std_init
        self._full_std = full_std
        self._use_expln = use_expln
        self._initial_std = initial_std
        
        # Initialize hidden states storage
        self._hidden_states: Optional[List[Tuple[torch.Tensor, torch.Tensor]]] = None
        self._batch_size: Optional[int] = None
        self._device = get_device("auto")
        # Pending episode_starts mask to reset hidden states per env on next forward
        self._pending_episode_starts: Optional[torch.Tensor] = None
        
        # Create feature extractor first
        self.features_extractor = self.features_extractor_class(
            self.observation_space, 
            **self.features_extractor_kwargs
        )
        
        # Get the actual features_dim from the feature extractor
        self.features_dim = self.features_extractor._features_dim
        
        # Build the action and value networks
        self._build_networks()
        
        # Initialize optimizer (required by SB3)
        self.optimizer = torch.optim.Adam(self.parameters(), lr=lr_schedule(1))
        
    def _preprocess_obs(self, obs: torch.Tensor) -> torch.Tensor:
        """Convert observations to float in [0,1] when normalize_images is enabled."""
        if self.normalize_images:
            if obs.dtype == torch.uint8:
                obs = obs.float().div_(255.0)
            else:
                # Ensure float32 dtype for convs and autocast stability
                obs = obs.float()
        return obs

    def _build_networks(self):
        """Build the actor and critic networks."""
        # Extract network architecture
        pi_arch = self.net_arch.get('pi', [128, 128])
        vf_arch = self.net_arch.get('vf', [128, 128])
        
        # Action network (policy head) - simple 2-layer MLP
        self.action_net = nn.Sequential(
            nn.Linear(self.features_dim, pi_arch[0]),
            self.activation_fn(),
            nn.Linear(pi_arch[0], pi_arch[1]),
            self.activation_fn(),
            nn.Linear(pi_arch[1], self.action_space.n)
        )
        
        # Value network (critic head) - simple 2-layer MLP
        self.value_net = nn.Sequential(
            nn.Linear(self.features_dim, vf_arch[0]),
            self.activation_fn(),
            nn.Linear(vf_arch[0], vf_arch[1]),
            self.activation_fn(),
            nn.Linear(vf_arch[1], 1)
        )
        
        # Initialize weights
        if self.ortho_init:
            self._init_weights()
    
    def _init_weights(self):
        """Initialize network weights using orthogonal initialization."""
        for module in self.modules():
            if isinstance(module, nn.Linear):
                nn.init.orthogonal_(module.weight, gain=1.0)
                if module.bias is not None:
                    nn.init.constant_(module.bias, 0.0)
    
    def _init_hidden_states(self, _batch_size: int, _device: torch.device) -> List[Tuple[torch.Tensor, torch.Tensor]]:
        """Initialize hidden states for the recurrent feature extractor."""
        if not hasattr(self.features_extractor, 'convlstm'):
            return []
        
        # We need to know the spatial dimensions, but we don't have them yet
        # Return None and let the feature extractor handle initialization
        return None
    
    def _get_hidden_states(self, batch_size: int) -> List[Tuple[torch.Tensor, torch.Tensor]]:
        """Get or initialize hidden states for the current batch."""
        if self._hidden_states is None or self._batch_size != batch_size:
            self._hidden_states = self._init_hidden_states(batch_size, self._device)
            self._batch_size = batch_size
        
        return self._hidden_states
    
    def reset_hidden_states(self, batch_size: Optional[int] = None):
        """Reset hidden states to zero."""
        if batch_size is not None:
            self._batch_size = batch_size
        self._hidden_states = None

    def set_episode_starts(self, episode_starts: torch.Tensor | List[bool]):
        """Queue per-env resets to apply on the next forward/eval call.

        episode_starts: shape [batch] boolean mask or list; True means env just reset.
        """
        if isinstance(episode_starts, list):
            episode_starts = torch.tensor(episode_starts, dtype=torch.bool, device=self._device)
        elif not isinstance(episode_starts, torch.Tensor):
            raise TypeError("episode_starts must be a Tensor or list of bools")
        self._pending_episode_starts = episode_starts.to(self._device).bool()

    def _apply_pending_episode_starts(self, hidden_states: Optional[List[Tuple[torch.Tensor, torch.Tensor]]]) -> Optional[List[Tuple[torch.Tensor, torch.Tensor]]]:
        """Zero hidden states for batch indices where episode_starts is True."""
        if hidden_states is None or self._pending_episode_starts is None:
            return hidden_states
        mask = self._pending_episode_starts
        # Clear pending mask after applying once
        self._pending_episode_starts = None
        try:
            # Each (h, c) has shape [B, C, H, W]; mask is [B]
            for i, (h, c) in enumerate(hidden_states):
                if mask.numel() != h.shape[0]:
                    # Batch mismatch; skip selective reset
                    continue
                if mask.any():
                    h[mask] = 0.0
                    c[mask] = 0.0
                    hidden_states[i] = (h, c)
        except Exception:
            # Be conservative: if something goes wrong, drop all states
            return None
        return hidden_states
    
    def forward(
        self, 
        obs: torch.Tensor, 
        deterministic: bool = False
    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
        """
        Forward pass through the recurrent policy.
        
        Args:
            obs: Observation tensor (B, C, H, W) or (B, features_dim)
            deterministic: Whether to use deterministic action selection
            
        Returns:
            action: Selected action
            value: State value estimate
            log_prob: Log probability of the selected action
        """
        # Import profiling here to avoid circular imports
        try:
            from profiling import profile
        except ImportError:
            # Fallback if profiling not available
            def profile(_name, _track_memory=True):
                from contextlib import nullcontext
                return nullcontext()
        
        with profile("recurrent_policy_forward", track_memory=True):
            obs = self._preprocess_obs(obs)
            batch_size = obs.shape[0]
            
            # Get current hidden states
            with profile("recurrent_policy_get_hidden_states", track_memory=True):
                hidden_states = self._get_hidden_states(batch_size)
                hidden_states = self._apply_pending_episode_starts(hidden_states)
            
            # Extract features using the recurrent feature extractor
            if hasattr(self.features_extractor, 'convlstm'):
                # Recurrent feature extractor - pass hidden states
                features, new_hidden_states = self.features_extractor(obs, hidden_states)
                # Update stored hidden states (detach to prevent gradient issues)
                self._hidden_states = [(h.detach(), c.detach()) for h, c in new_hidden_states]
            else:
                # Non-recurrent feature extractor
                features = self.features_extractor(obs)
            
            # Compute action logits and value
            with profile("recurrent_policy_action_value", track_memory=True):
                action_logits = self.action_net(features)
                value = self.value_net(features).squeeze(-1)
            
            # Create action distribution
            with profile("recurrent_policy_action_dist", track_memory=True):
                action_dist = self._get_action_dist_from_latent(action_logits)
            
            # Sample action
            with profile("recurrent_policy_action_sampling", track_memory=True):
                if deterministic:
                    action = action_dist.mode()
                else:
                    action = action_dist.sample()
                
                # Compute log probability
                log_prob = action_dist.log_prob(action)
            
            return action, value, log_prob

    def get_distribution(self, obs: torch.Tensor) -> Distribution:
        """SB3-compatible API: return the action distribution given observations."""
        obs = self._preprocess_obs(obs)
        batch_size = obs.shape[0]

        hidden_states = self._get_hidden_states(batch_size)

        if hasattr(self.features_extractor, 'convlstm'):
            features, new_hidden_states = self.features_extractor(obs, hidden_states)
            self._hidden_states = [(h.detach(), c.detach()) for h, c in new_hidden_states]
        else:
            features = self.features_extractor(obs)

        action_logits = self.action_net(features)
        return self._get_action_dist_from_latent(action_logits)
    
    def _get_action_dist_from_latent(self, latent_pi: torch.Tensor) -> Distribution:
        """Create a categorical action distribution from policy logits."""
        dist = CategoricalDistribution(self.action_space.n)
        dist.proba_distribution(action_logits=latent_pi)
        return dist
    
    def evaluate_actions(
        self, 
        obs: torch.Tensor, 
        actions: torch.Tensor
    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
        """
        Evaluate actions for training.
        
        Args:
            obs: Observation tensor
            actions: Action tensor
            
        Returns:
            value: State value estimate
            log_prob: Log probability of actions
            entropy: Entropy of action distribution
        """
        # Import profiling here to avoid circular imports
        try:
            from profiling import profile
        except ImportError:
            # Fallback if profiling not available
            def profile(_name, _track_memory=True):
                from contextlib import nullcontext
                return nullcontext()
        
        with profile("recurrent_policy_evaluate_actions", track_memory=True):
            obs = self._preprocess_obs(obs)
            batch_size = obs.shape[0]
            
            # Get current hidden states
            hidden_states = self._get_hidden_states(batch_size)
            hidden_states = self._apply_pending_episode_starts(hidden_states)
            
            # Extract features
            if hasattr(self.features_extractor, 'convlstm'):
                features, new_hidden_states = self.features_extractor(obs, hidden_states)
                # Update stored hidden states (detach to prevent gradient issues)
                self._hidden_states = [(h.detach(), c.detach()) for h, c in new_hidden_states]
            else:
                features = self.features_extractor(obs)
            
            # Compute action logits and value
            action_logits = self.action_net(features)
            value = self.value_net(features).squeeze(-1)
            
            # Create action distribution
            action_dist = self._get_action_dist_from_latent(action_logits)
            
            # Compute log probability and entropy
            if actions.dtype != torch.long:
                actions = actions.long()
            log_prob = action_dist.log_prob(actions)
            entropy = action_dist.entropy()
            
            return value, log_prob, entropy
    
    def get_hidden_states(self) -> Optional[List[Tuple[torch.Tensor, torch.Tensor]]]:
        """Get current hidden states for analysis."""
        return self._hidden_states
    
    def set_hidden_states(self, hidden_states: List[Tuple[torch.Tensor, torch.Tensor]]):
        """Set hidden states (useful for analysis and interventions)."""
        self._hidden_states = hidden_states
        if hidden_states:
            self._batch_size = hidden_states[0][0].shape[0]
    
    def _predict(self, observation: torch.Tensor, deterministic: bool = False) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        Predict action and value for a single observation.
        
        Args:
            observation: Single observation tensor
            deterministic: Whether to use deterministic action selection
            
        Returns:
            action: Selected action
            value: State value estimate
        """
        # Add batch dimension if needed
        if observation.dim() == 3:  # (C, H, W)
            observation = observation.unsqueeze(0)  # (1, C, H, W)
        
        observation = self._preprocess_obs(observation)
        action, value, _ = self.forward(observation, deterministic)
        
        # Remove batch dimension for single observation
        return action.squeeze(0), value.squeeze(0)
    
    def predict_values(self, obs: torch.Tensor) -> torch.Tensor:
        """
        Predict values for observations (used by SB3).
        
        Args:
            obs: Observation tensor
            
        Returns:
            values: State value estimates
        """
        # Import profiling here to avoid circular imports
        try:
            from profiling import profile
        except ImportError:
            # Fallback if profiling not available
            def profile(_name, _track_memory=True):
                from contextlib import nullcontext
                return nullcontext()
        
        with profile("recurrent_policy_predict_values", track_memory=True):
            obs = self._preprocess_obs(obs)
            batch_size = obs.shape[0]
            
            # Get current hidden states
            hidden_states = self._get_hidden_states(batch_size)
            hidden_states = self._apply_pending_episode_starts(hidden_states)
            
            # Extract features using the recurrent feature extractor
            if hasattr(self.features_extractor, 'convlstm'):
                # Recurrent feature extractor - pass hidden states
                features, new_hidden_states = self.features_extractor(obs, hidden_states)
                # Update stored hidden states (detach to prevent gradient issues)
                self._hidden_states = [(h.detach(), c.detach()) for h, c in new_hidden_states]
            else:
                # Non-recurrent feature extractor
                features = self.features_extractor(obs)
            
            # Compute value
            value = self.value_net(features).squeeze(-1)
            
            return value

```

```python
from __future__ import annotations

import torch
import torch.nn as nn
from typing import Dict, List, Optional, Tuple, Any
from stable_baselines3.common.policies import BasePolicy
from stable_baselines3.common.torch_layers import BaseFeaturesExtractor
from stable_baselines3.common.distributions import Distribution, CategoricalDistribution
from stable_baselines3.common.utils import get_device
from gymnasium.spaces import Box, Discrete


class RecurrentActorCriticPolicy(BasePolicy):
    """
    Custom recurrent policy that properly handles hidden states across time steps.
    
    This policy maintains hidden states internally and provides them to the feature extractor
    on each forward pass, enabling true recurrent behavior for planning tasks.
    """
    
    def __init__(
        self,
        observation_space: Box,
        action_space: Discrete,
        lr_schedule: callable,
        features_extractor_class: type[BaseFeaturesExtractor],
        features_extractor_kwargs: Optional[Dict[str, Any]] = None,
        net_arch: Optional[List[int]] = None,
        activation_fn: type[nn.Module] = nn.ReLU,
        ortho_init: bool = True,
        use_sde: bool = False,
        log_std_init: float = 0.0,
        full_std: bool = True,
        use_expln: bool = False,
        squash_output: bool = False,
        features_dim: int = 128,
        normalize_images: bool = True,
        initial_std: float = 1.0,
        **kwargs
    ):
        super().__init__(
            observation_space,
            action_space,
            lr_schedule,
            **kwargs
        )
        
        # Set essential attributes
        self.features_extractor_class = features_extractor_class
        self.features_extractor_kwargs = features_extractor_kwargs or {}
        self.net_arch = net_arch
        self.activation_fn = activation_fn
        self.features_dim = features_dim
        self.ortho_init = ortho_init
        self.normalize_images = normalize_images
        # Store unused-but-standard args to avoid lint warnings and for future use
        self._use_sde = use_sde
        self._log_std_init = log_std_init
        self._full_std = full_std
        self._use_expln = use_expln
        self._initial_std = initial_std
        
        # Initialize hidden states storage
        self._hidden_states: Optional[List[Tuple[torch.Tensor, torch.Tensor]]] = None
        self._batch_size: Optional[int] = None
        self._device = get_device("auto")
        # Pending episode_starts mask to reset hidden states per env on next forward
        self._pending_episode_starts: Optional[torch.Tensor] = None
        
        # Create feature extractor first
        self.features_extractor = self.features_extractor_class(
            self.observation_space, 
            **self.features_extractor_kwargs
        )
        
        # Get the actual features_dim from the feature extractor
        self.features_dim = self.features_extractor._features_dim
        
        # Build the action and value networks
        self._build_networks()
        
        # Initialize optimizer (required by SB3)
        self.optimizer = torch.optim.Adam(self.parameters(), lr=lr_schedule(1))
        
    def _preprocess_obs(self, obs: torch.Tensor) -> torch.Tensor:
        """Convert observations to float in [0,1] when normalize_images is enabled."""
        if self.normalize_images:
            if obs.dtype == torch.uint8:
                obs = obs.float().div_(255.0)
            else:
                # Ensure float32 dtype for convs and autocast stability
                obs = obs.float()
        return obs

    def _build_networks(self):
        """Build the actor and critic networks."""
        # Extract network architecture
        pi_arch = self.net_arch.get('pi', [128, 128])
        vf_arch = self.net_arch.get('vf', [128, 128])
        
        # Action network (policy head) - simple 2-layer MLP
        self.action_net = nn.Sequential(
            nn.Linear(self.features_dim, pi_arch[0]),
            self.activation_fn(),
            nn.Linear(pi_arch[0], pi_arch[1]),
            self.activation_fn(),
            nn.Linear(pi_arch[1], self.action_space.n)
        )
        
        # Value network (critic head) - simple 2-layer MLP
        self.value_net = nn.Sequential(
            nn.Linear(self.features_dim, vf_arch[0]),
            self.activation_fn(),
            nn.Linear(vf_arch[0], vf_arch[1]),
            self.activation_fn(),
            nn.Linear(vf_arch[1], 1)
        )
        
        # Initialize weights
        if self.ortho_init:
            self._init_weights()
    
    def _init_weights(self):
        """Initialize network weights using orthogonal initialization."""
        for module in self.modules():
            if isinstance(module, nn.Linear):
                nn.init.orthogonal_(module.weight, gain=1.0)
                if module.bias is not None:
                    nn.init.constant_(module.bias, 0.0)
    
    def _init_hidden_states(self, _batch_size: int, _device: torch.device) -> List[Tuple[torch.Tensor, torch.Tensor]]:
        """Initialize hidden states for the recurrent feature extractor."""
        if not hasattr(self.features_extractor, 'convlstm'):
            return []
        
        # We need to know the spatial dimensions, but we don't have them yet
        # Return None and let the feature extractor handle initialization
        return None
    
    def _get_hidden_states(self, batch_size: int) -> List[Tuple[torch.Tensor, torch.Tensor]]:
        """Get or initialize hidden states for the current batch."""
        if self._hidden_states is None or self._batch_size != batch_size:
            self._hidden_states = self._init_hidden_states(batch_size, self._device)
            self._batch_size = batch_size
        
        return self._hidden_states
    
    def reset_hidden_states(self, batch_size: Optional[int] = None):
        """Reset hidden states to zero."""
        if batch_size is not None:
            self._batch_size = batch_size
        self._hidden_states = None

    def set_episode_starts(self, episode_starts: torch.Tensor | List[bool]):
        """Queue per-env resets to apply on the next forward/eval call.

        episode_starts: shape [batch] boolean mask or list; True means env just reset.
        """
        if isinstance(episode_starts, list):
            episode_starts = torch.tensor(episode_starts, dtype=torch.bool, device=self._device)
        elif not isinstance(episode_starts, torch.Tensor):
            raise TypeError("episode_starts must be a Tensor or list of bools")
        self._pending_episode_starts = episode_starts.to(self._device).bool()

    def _apply_pending_episode_starts(self, hidden_states: Optional[List[Tuple[torch.Tensor, torch.Tensor]]]) -> Optional[List[Tuple[torch.Tensor, torch.Tensor]]]:
        """Zero hidden states for batch indices where episode_starts is True."""
        if hidden_states is None or self._pending_episode_starts is None:
            return hidden_states
        mask = self._pending_episode_starts
        # Clear pending mask after applying once
        self._pending_episode_starts = None
        # Each (h, c) has shape [B, C, H, W]; mask is [B]
        for i, (h, c) in enumerate(hidden_states):
            if not isinstance(h, torch.Tensor) or not isinstance(c, torch.Tensor):
                return None
            if mask.numel() != h.shape[0]:
                # Batch mismatch; drop all states to avoid leakage
                return None
            if mask.any():
                h[mask] = 0.0
                c[mask] = 0.0
                hidden_states[i] = (h, c)
        return hidden_states
    
    def forward(
        self, 
        obs: torch.Tensor, 
        deterministic: bool = False
    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
        """
        Forward pass through the recurrent policy.
        
        Args:
            obs: Observation tensor (B, C, H, W) or (B, features_dim)
            deterministic: Whether to use deterministic action selection
            
        Returns:
            action: Selected action
            value: State value estimate
            log_prob: Log probability of the selected action
        """
        # Import profiling here to avoid circular imports
        try:
            from profiling import profile
        except ImportError:
            # Fallback if profiling not available
            def profile(_name, _track_memory=True):
                from contextlib import nullcontext
                return nullcontext()
        
        with profile("recurrent_policy_forward", track_memory=True):
            obs = self._preprocess_obs(obs)
            batch_size = obs.shape[0]
            
            # Get current hidden states
            with profile("recurrent_policy_get_hidden_states", track_memory=True):
                hidden_states = self._get_hidden_states(batch_size)
                hidden_states = self._apply_pending_episode_starts(hidden_states)
            
            # Extract features using the recurrent feature extractor
            if hasattr(self.features_extractor, 'convlstm'):
                # Recurrent feature extractor - pass hidden states
                features, new_hidden_states = self.features_extractor(obs, hidden_states)
                # Update stored hidden states (detach to prevent gradient issues)
                self._hidden_states = [(h.detach(), c.detach()) for h, c in new_hidden_states]
            else:
                # Non-recurrent feature extractor
                features = self.features_extractor(obs)
            
            # Compute action logits and value
            with profile("recurrent_policy_action_value", track_memory=True):
                action_logits = self.action_net(features)
                value = self.value_net(features).squeeze(-1)
            
            # Create action distribution
            with profile("recurrent_policy_action_dist", track_memory=True):
                action_dist = self._get_action_dist_from_latent(action_logits)
            
            # Sample action
            with profile("recurrent_policy_action_sampling", track_memory=True):
                if deterministic:
                    action = action_dist.mode()
                else:
                    action = action_dist.sample()
                
                # Compute log probability
                log_prob = action_dist.log_prob(action)
            
            return action, value, log_prob

    def get_distribution(self, obs: torch.Tensor) -> Distribution:
        """SB3-compatible API: return the action distribution given observations."""
        obs = self._preprocess_obs(obs)
        batch_size = obs.shape[0]

        hidden_states = self._get_hidden_states(batch_size)

        if hasattr(self.features_extractor, 'convlstm'):
            features, new_hidden_states = self.features_extractor(obs, hidden_states)
            self._hidden_states = [(h.detach(), c.detach()) for h, c in new_hidden_states]
        else:
            features = self.features_extractor(obs)

        action_logits = self.action_net(features)
        return self._get_action_dist_from_latent(action_logits)
    
    def _get_action_dist_from_latent(self, latent_pi: torch.Tensor) -> Distribution:
        """Create a categorical action distribution from policy logits."""
        dist = CategoricalDistribution(self.action_space.n)
        dist.proba_distribution(action_logits=latent_pi)
        return dist
    
    def evaluate_actions(
        self, 
        obs: torch.Tensor, 
        actions: torch.Tensor
    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
        """
        Evaluate actions for training.
        
        Args:
            obs: Observation tensor
            actions: Action tensor
            
        Returns:
            value: State value estimate
            log_prob: Log probability of actions
            entropy: Entropy of action distribution
        """
        # Import profiling here to avoid circular imports
        try:
            from profiling import profile
        except ImportError:
            # Fallback if profiling not available
            def profile(_name, _track_memory=True):
                from contextlib import nullcontext
                return nullcontext()
        
        with profile("recurrent_policy_evaluate_actions", track_memory=True):
            obs = self._preprocess_obs(obs)
            batch_size = obs.shape[0]
            
            # Get current hidden states
            hidden_states = self._get_hidden_states(batch_size)
            hidden_states = self._apply_pending_episode_starts(hidden_states)
            
            # Extract features
            if hasattr(self.features_extractor, 'convlstm'):
                features, new_hidden_states = self.features_extractor(obs, hidden_states)
                # Update stored hidden states (detach to prevent gradient issues)
                self._hidden_states = [(h.detach(), c.detach()) for h, c in new_hidden_states]
            else:
                features = self.features_extractor(obs)
            
            # Compute action logits and value
            action_logits = self.action_net(features)
            value = self.value_net(features).squeeze(-1)
            
            # Create action distribution
            action_dist = self._get_action_dist_from_latent(action_logits)
            
            # Compute log probability and entropy
            if actions.dtype != torch.long:
                actions = actions.long()
            log_prob = action_dist.log_prob(actions)
            entropy = action_dist.entropy()
            
            return value, log_prob, entropy
    
    def get_hidden_states(self) -> Optional[List[Tuple[torch.Tensor, torch.Tensor]]]:
        """Get current hidden states for analysis."""
        return self._hidden_states
    
    def set_hidden_states(self, hidden_states: List[Tuple[torch.Tensor, torch.Tensor]]):
        """Set hidden states (useful for analysis and interventions)."""
        self._hidden_states = hidden_states
        if hidden_states:
            self._batch_size = hidden_states[0][0].shape[0]
    
    def _predict(self, observation: torch.Tensor, deterministic: bool = False) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        Predict action and value for a single observation.
        
        Args:
            observation: Single observation tensor
            deterministic: Whether to use deterministic action selection
            
        Returns:
            action: Selected action
            value: State value estimate
        """
        # Add batch dimension if needed
        if observation.dim() == 3:  # (C, H, W)
            observation = observation.unsqueeze(0)  # (1, C, H, W)
        
        observation = self._preprocess_obs(observation)
        action, value, _ = self.forward(observation, deterministic)
        
        # Remove batch dimension for single observation
        return action.squeeze(0), value.squeeze(0)
    
    def predict_values(self, obs: torch.Tensor) -> torch.Tensor:
        """
        Predict values for observations (used by SB3).
        
        Args:
            obs: Observation tensor
            
        Returns:
            values: State value estimates
        """
        # Import profiling here to avoid circular imports
        try:
            from profiling import profile
        except ImportError:
            # Fallback if profiling not available
            def profile(_name, _track_memory=True):
                from contextlib import nullcontext
                return nullcontext()
        
        with profile("recurrent_policy_predict_values", track_memory=True):
            obs = self._preprocess_obs(obs)
            batch_size = obs.shape[0]
            
            # Get current hidden states
            hidden_states = self._get_hidden_states(batch_size)
            hidden_states = self._apply_pending_episode_starts(hidden_states)
            
            # Extract features using the recurrent feature extractor
            if hasattr(self.features_extractor, 'convlstm'):
                # Recurrent feature extractor - pass hidden states
                features, new_hidden_states = self.features_extractor(obs, hidden_states)
                # Update stored hidden states (detach to prevent gradient issues)
                self._hidden_states = [(h.detach(), c.detach()) for h, c in new_hidden_states]
            else:
                # Non-recurrent feature extractor
                features = self.features_extractor(obs)
            
            # Compute value
            value = self.value_net(features).squeeze(-1)
            
            return value

```

I’ll add per-env hidden-state resets to the recurrent policy and wire them into all forward/eval paths so dones don’t leak memory across envs.

- I added `set_episode_starts(episode_starts)` to `models/recurrent_policy.py`. Pass a boolean mask/list per env; the policy will zero hidden states for those envs on the next call.
- I apply this mask inside `forward`, `evaluate_actions`, and `predict_values`, so resets are honored across training/eval calls.
- I ensured shape checks and safe fallback: on any batch mismatch, it drops hidden state to avoid leakage.

Do you want me to also add a small evaluation helper that calls `policy.set_episode_starts(dones)` during rollouts with this custom policy, or proceed to the next feedback item?

---

**User**

Yes please add the eval helper, and then go on to the next feedback item 'Evaluation script incompatible with recurrent models'

---

**Cursor**

I'll add a small evaluation helper to handle per-episode resets for the custom recurrent policy and update the evaluation script to register the custom policy class when loading and handle both sb3-contrib RecurrentPPO and the custom recurrent policy during rollouts.

I'll create an evaluation utility module with a helper for stepping through episodes that handles LSTM state or custom policy episode_starts, then update the evaluation script to use it and to register the custom policy on load.

```python
*** Begin Patch
*** Update File: /Users/sidbaines/Documents/Code/20250911_RLNew/monitoring/evaluators.py
@@
 from __future__ import annotations
 
 import json
 from typing import Any, Callable
 
@@
 class OptimalityGapEvaluator:
@@
         return results
+
+
+# --- Evaluation helpers for recurrent policies ---
+
+def rollout_episode_with_recurrent_support(env_factory: Callable[[], Any], model: Any, deterministic: bool = True) -> dict[str, Any]:
+    """Run a single episode handling both sb3-contrib RecurrentPPO and custom recurrent policies.
+
+    Returns a dict with success flag and length.
+    """
+    env = env_factory()
+    obs, info = env.reset()
+    done = False
+    trunc = False
+    steps = 0
+
+    # sb3-contrib RecurrentPPO state tracking
+    lstm_state = None
+    episode_starts = True
+
+    # Custom policy detection
+    custom_recurrent = hasattr(getattr(model, 'policy', model), 'set_episode_starts')
+
+    while not (done or trunc):
+        if hasattr(model, 'predict'):
+            if lstm_state is not None or hasattr(model, 'predict') and 'state' in model.predict.__code__.co_varnames:  # type: ignore[attr-defined]
+                try:
+                    action_out = model.predict(obs, state=lstm_state, episode_start=episode_starts, deterministic=deterministic)  # type: ignore[call-arg]
+                except TypeError:
+                    # Fallback for models without recurrent signature
+                    action_out = model.predict(obs, deterministic=deterministic)
+                if isinstance(action_out, tuple) and len(action_out) == 2 and not isinstance(action_out[1], (tuple, list)):
+                    action, _ = action_out
+                else:
+                    action, lstm_state = action_out  # type: ignore[assignment]
+            else:
+                action, _ = model.predict(obs, deterministic=deterministic)
+        else:
+            # Assume custom policy-like API with _predict
+            policy = getattr(model, 'policy', model)
+            if custom_recurrent:
+                policy.set_episode_starts([episode_starts])
+            action, _ = policy._predict(obs, deterministic=deterministic)  # type: ignore[attr-defined]
+
+        obs, _, done, trunc, info = env.step(int(action))
+        if custom_recurrent:
+            # Inform custom policy of episode boundary on next step
+            policy = getattr(model, 'policy', model)
+            policy.set_episode_starts([done or trunc])
+        episode_starts = done or trunc
+        steps += 1
+
+    return {"is_success": info.get("is_success", False), "length": steps}
*** End Patch
```

```python
from __future__ import annotations

import json
from dataclasses import dataclass
from typing import Any, Dict, List, Optional, Tuple, Callable

from .hub import MonitoringHub


def _board_signature(layout: Dict[str, Any]) -> str:
    # Serialize fixed layout dict into a deterministic signature
    import hashlib
    payload = json.dumps(layout, sort_keys=True)
    return hashlib.sha256(payload.encode("utf-8")).hexdigest()[:16]


@dataclass
class SolverCache:
    path: str
    data: Dict[str, Dict[str, Any]]

    @classmethod
    def load(cls, path: str) -> "SolverCache":
        try:
            with open(path, "r", encoding="utf-8") as f:
                data = json.load(f)
        except (OSError, json.JSONDecodeError):
            data = {}
        return SolverCache(path=path, data=data)

    def save(self) -> None:
        with open(self.path, "w", encoding="utf-8") as f:
            json.dump(self.data, f)


class FixedSetEvaluator:
    def __init__(self, fixedset: List[Dict[str, Any]], episodes: int = 50) -> None:
        self.fixedset = fixedset
        self.episodes = int(episodes)

    def evaluate(self, hub: MonitoringHub, make_env_fn, model, step: Optional[int] = None) -> Dict[str, Any]:
        import numpy as np
        success = 0
        lengths: List[int] = []
        noop_counts: List[int] = []
        for i in range(min(self.episodes, len(self.fixedset))):
            layout = self.fixedset[i]
            env = make_env_fn(layout)
            obs, info = env.reset()
            done = False
            trunc = False
            steps = 0
            noops = 0
            while not (done or trunc):
                action, _ = model.predict(obs, deterministic=True)
                is_noop = (getattr(env, "_noop_action", None) is not None and int(action) == env._noop_action)
                if is_noop:
                    noops += 1
                obs, _, done, trunc, info = env.step(int(action))
                steps += 1
            if info.get("is_success", False):
                success += 1
            lengths.append(steps)
            noop_counts.append(noops)
            env.close()
        rate = success / max(1, min(self.episodes, len(self.fixedset)))
        hub.log_scalar("eval/success_rate", float(rate), step)
        if lengths:
            hub.log_scalar("eval/episode_length/mean", float(sum(lengths) / len(lengths)), step)
        if noop_counts:
            hub.log_scalar("eval/actions/noop_rate", float(sum(noop_counts) / max(1, sum(lengths))), step)
        return {"success_rate": rate, "lengths": lengths, "noop_counts": noop_counts}


class OptimalityGapEvaluator:
    def __init__(self, fixedset: List[Dict[str, Any]], solver_cache: SolverCache, solver_limits: Dict[str, int], skip_unsolved: bool = True) -> None:
        self.fixedset = fixedset
        self.cache = solver_cache
        self.limits = solver_limits
        self.skip_unsolved = bool(skip_unsolved)

    def _build_env_from_layout(self, layout_dict: Dict[str, Any]):
        from env import RicochetRobotsEnv, FixedLayout
        import numpy as np
        h = int(layout_dict["height"]) ; w = int(layout_dict["width"]) ; tr = int(layout_dict["target_robot"]) ; robots = {int(k): tuple(v) for k, v in layout_dict["robot_positions"].items()}
        h_walls = np.array(layout_dict["h_walls"], dtype=bool)
        v_walls = np.array(layout_dict["v_walls"], dtype=bool)
        goal = tuple(layout_dict["goal_position"])  # type: ignore
        fl = FixedLayout(height=h, width=w, h_walls=h_walls, v_walls=v_walls, robot_positions=robots, goal_position=goal, target_robot=tr)
        return RicochetRobotsEnv(fixed_layout=fl, obs_mode="image", channels_first=True)

    def _ensure_optimal_length(self, layout: Dict[str, Any], sig: str) -> Optional[int]:
        if sig in self.cache.data and "optimal_length" in self.cache.data[sig]:
            return int(self.cache.data[sig]["optimal_length"])
        # Compute
        try:
            from env.solver import solve_bfs
            env = self._build_env_from_layout(layout)
            board = env.get_board()
            actions = solve_bfs(board, max_depth=int(self.limits.get("max_depth", 60)), max_nodes=int(self.limits.get("max_nodes", 50000)))
            env.close()
            if actions is None:
                self.cache.data[sig] = {"optimal_length": None}
                return None
            opt = len(actions)
            self.cache.data[sig] = {"optimal_length": opt}
            self.cache.save()
            return opt
        except Exception:
            return None

    def evaluate(self, hub: MonitoringHub, make_env_fn, model, step: Optional[int] = None) -> Dict[str, Any]:
        gaps: List[int] = []
        considered = 0
        for layout in self.fixedset:
            sig = _board_signature(layout)
            opt = self._ensure_optimal_length(layout, sig)
            if opt is None and self.skip_unsolved:
                continue
            env = make_env_fn(layout)
            obs, info = env.reset()
            done = False
            trunc = False
            steps = 0
            while not (done or trunc):
                action, _ = model.predict(obs, deterministic=True)
                obs, _, done, trunc, info = env.step(int(action))
                steps += 1
            env.close()
            if info.get("is_success", False) and opt is not None:
                gaps.append(int(steps - opt))
            considered += 1
        if gaps:
            import numpy as np
            hub.log_scalar("eval/optimality_gap/mean", float(sum(gaps) / len(gaps)), step)
            hub.log_scalar("eval/optimality_gap/median", float(np.median(gaps)), step)
        hub.log_scalar("eval/optimality_gap/num_evaluated", float(considered), step)
        return {"gaps": gaps, "considered": considered}


class TrajectoryRecorder:
    def __init__(self, episodes: int = 5) -> None:
        self.episodes = int(episodes)

    def record(self, make_env_fn, model) -> List[Dict[str, Any]]:
        trajectories: List[Dict[str, Any]] = []
        for _ in range(self.episodes):
            env = make_env_fn()
            obs, info = env.reset()
            done = False
            trunc = False
            steps: List[Dict[str, Any]] = []
            while not (done or trunc):
                action, _ = model.predict(obs, deterministic=True)
                next_obs, reward, done, trunc, info = env.step(int(action))
                steps.append({
                    "action": int(action),
                    "reward": float(reward),
                    "done": bool(done),
                    "truncated": bool(trunc),
                    "is_success": bool(info.get("is_success", False)),
                })
                obs = next_obs
            trajectories.append({
                "steps": steps,
                "success": bool(info.get("is_success", False))
            })
            env.close()
        return trajectories



def rollout_episode_with_recurrent_support(env_factory: Callable[[], Any], model: Any, deterministic: bool = True) -> Dict[str, Any]:
    """Run a single episode handling both sb3-contrib RecurrentPPO and custom recurrent policies.

    Returns a dict with success flag and length.
    """
    env = env_factory()
    obs, info = env.reset()
    done = False
    trunc = False
    steps = 0

    # sb3-contrib RecurrentPPO state tracking
    lstm_state = None
    episode_starts = True

    # Custom policy detection
    policy_obj = getattr(model, 'policy', model)
    custom_recurrent = hasattr(policy_obj, 'set_episode_starts')

    while not (done or trunc):
        if hasattr(model, 'predict'):
            # Try recurrent signature first
            try:
                action_out = model.predict(obs, state=lstm_state, episode_start=episode_starts, deterministic=deterministic)  # type: ignore[arg-type]
                if isinstance(action_out, tuple) and len(action_out) == 2 and not isinstance(action_out[1], (tuple, list)):
                    action, _ = action_out
                else:
                    action, lstm_state = action_out  # type: ignore[assignment]
            except TypeError:
                # Fallback for feedforward models
                action, _ = model.predict(obs, deterministic=deterministic)
        else:
            # Assume custom policy-like API with _predict
            if custom_recurrent:
                policy_obj.set_episode_starts([episode_starts])
            action, _ = policy_obj._predict(obs, deterministic=deterministic)  # type: ignore[attr-defined]

        obs, _, done, trunc, info = env.step(int(action))
        if custom_recurrent:
            policy_obj.set_episode_starts([done or trunc])
        episode_starts = done or trunc
        steps += 1

    result = {"is_success": info.get("is_success", False), "length": steps}
    env.close()
    return result


```

```python
from __future__ import annotations

import argparse

from env import RicochetRobotsEnv, fixed_layout_v0_one_move, fixed_layouts_v1_four_targets


def make_single_env(args: argparse.Namespace) -> RicochetRobotsEnv:
    if args.env_mode == "v0":
        layout = fixed_layout_v0_one_move()
        return RicochetRobotsEnv(fixed_layout=layout, obs_mode=args.obs_mode, channels_first=True)
    if args.env_mode == "v1":
        layouts = fixed_layouts_v1_four_targets()
        return RicochetRobotsEnv(fixed_layout=layouts[0], obs_mode=args.obs_mode, channels_first=True)
    return RicochetRobotsEnv(height=args.height, width=args.width, num_robots=args.num_robots, obs_mode=args.obs_mode, channels_first=True)


def main() -> None:
    parser = argparse.ArgumentParser(description="Evaluate a trained PPO agent")
    parser.add_argument("--model-path", required=True)
    parser.add_argument("--env-mode", choices=["random", "v0", "v1"], default="random")
    parser.add_argument("--obs-mode", choices=["image", "symbolic"], default="image")
    parser.add_argument("--recurrent", action="store_true", help="Load sb3-contrib RecurrentPPO model or custom recurrent policy")
    parser.add_argument("--episodes", type=int, default=50)
    parser.add_argument("--height", type=int, default=8)
    parser.add_argument("--width", type=int, default=8)
    parser.add_argument("--num-robots", type=int, default=2)
    args = parser.parse_args()

    import importlib
    PPO = importlib.import_module("stable_baselines3").PPO  # type: ignore[attr-defined]
    # Try to import RecurrentPPO for recurrent models
    try:
        RecurrentPPO = importlib.import_module("sb3_contrib").RecurrentPPO  # type: ignore[attr-defined]
    except Exception:
        RecurrentPPO = None  # type: ignore
    # Try to import custom recurrent policy for registration
    try:
        from models.recurrent_policy import RecurrentActorCriticPolicy  # type: ignore
        # Registering custom policies for SB3 load if needed happens implicitly by import
        _custom_policy_available = True
    except Exception:
        RecurrentActorCriticPolicy = None  # type: ignore
        _custom_policy_available = False

    env = make_single_env(args)
    policy_type = "CnnPolicy" if args.obs_mode == "image" else "MlpPolicy"
    if args.recurrent and RecurrentPPO is not None:
        model = RecurrentPPO.load(args.model_path)  # type: ignore[assignment]
    else:
        # Attempt to load with SB3; if the checkpoint references a custom policy, the prior import makes it available
        model = PPO.load(args.model_path)

    success = 0
    lengths = []
    # Use unified helper to support both sb3-contrib and custom recurrent policy
    try:
        from monitoring.evaluators import rollout_episode_with_recurrent_support
        make_env_fn = lambda: make_single_env(args)
        for _ in range(args.episodes):
            res = rollout_episode_with_recurrent_support(make_env_fn, model, deterministic=True)
            if res.get("is_success", False):
                success += 1
            lengths.append(int(res.get("length", 0)))
    except ImportError:
        # Fallback to basic loop
        for _ in range(args.episodes):
            obs, info = env.reset()
            done = False
            trunc = False
            steps = 0
            while not (done or trunc):
                action, _ = model.predict(obs, deterministic=True)
                obs, _, done, trunc, info = env.step(int(action))
                steps += 1
            if info.get("is_success", False):
                success += 1
            lengths.append(steps)
        if info.get("is_success", False):
            success += 1
        lengths.append(steps)

    rate = success / args.episodes
    avg_len = sum(lengths) / max(1, len(lengths))
    print(f"Success rate: {rate:.3f} ({success}/{args.episodes}), avg length: {avg_len:.2f}")


if __name__ == "__main__":
    main()



```

- I added `rollout_episode_with_recurrent_support` in `monitoring/evaluators.py` to handle both sb3-contrib RecurrentPPO (state + episode_start) and the custom recurrent policy (`set_episode_starts`) during evaluation.
- I updated `evaluate_agent.py` to:
  - Import and register `RecurrentActorCriticPolicy` so SB3 can deserialize checkpoints that reference it.
  - Use the unified rollout helper, supporting both recurrent and non-recurrent models without extra flags.
  - Still supports `--recurrent` to force using RecurrentPPO when loading.

Summary
- New helper in `monitoring/evaluators.py`: `rollout_episode_with_recurrent_support`.
- `evaluate_agent.py` now loads RecurrentPPO when `--recurrent` is set, otherwise loads PPO but will still work with custom recurrent policy checkpoints.
- Evaluation now correctly resets per-env hidden states or LSTM state across episode boundaries.

